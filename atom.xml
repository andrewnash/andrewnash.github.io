<?xml version='1.0' encoding='utf-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://urara-demo.netlify.app/</id>
  <title><![CDATA[Andrew Nash Blog]]></title>
  <subtitle><![CDATA[MSc]]></subtitle>
  <icon>https://urara-demo.netlify.app/favicon.png</icon>
  <link href="https://urara-demo.netlify.app" />
  <link href="https://urara-demo.netlify.app/atom.xml" rel="self" type="application/atom+xml" />
  <updated>2023-06-14T20:56:16.375Z</updated>
  <author>
    <name><![CDATA[Andrew Nash]]></name>
  </author>
  <category term="Computer Vision" scheme="https://urara-demo.netlify.app/?tags=Computer%20Vision" />
  <category term="Transformer" scheme="https://urara-demo.netlify.app/?tags=Transformer" />
  <category term="RL" scheme="https://urara-demo.netlify.app/?tags=RL" />
  <entry>
    <title type="html"><![CDATA[Intelligent Ground Vehicle Competition]]></title>
    <link href="https://urara-demo.netlify.app/igvc" />
    <id>https://urara-demo.netlify.app/igvc</id>
    <published>2023-06-11T00:00:00.000Z</published>
    <updated>2023-06-14T20:55:04.600Z</updated>
    <content type="html">
      <![CDATA[<h1 id="a-deep-dive-into-my-autonomous-vehicle-software-design-journey"><a href="#a-deep-dive-into-my-autonomous-vehicle-software-design-journey">A Deep Dive into My Autonomous Vehicle Software Design Journey</a></h1><p>Hello everyone! I’m Andrew Nash, and for the past year, I’ve been leading the software development efforts for Paradigm’s autonomous vehicle in the Intelligent Ground Vehicle Competition (IGVC). This has been an exciting journey, full of learning, innovation, and countless lines of code. In this post, I’ll walk you through the technical nuances of the software I developed and implemented. My role involved designing and implimenting all perception (including all ML models), navigation, control, and sensor code.</p><p><img src="./robot.jpg" alt="robot" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="igvc-auto-nav-challenge"><a href="#igvc-auto-nav-challenge">IGVC Auto-Nav Challenge</a></h2><p>To compete at the 30th annual IGVC, Paradigm had to design and build an original vehicle that can autonomously navigate an obstacle course similar to the one shown below. The vehicle must stay within lane lines and avoid obstacles on an unknown track layout.</p><p><img src="./comp_track.PNG" alt="track" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="vehicle"><a href="#vehicle">Vehicle</a></h2><p>The mechanical design of our competition entry is a testament to the team’s innovation and engineering prowess. The vehicle’s distinctive skid-steer drivetrain, with its two separate drivetrain modules, truly stands out. This configuration enables superior maneuverability, allowing the vehicle to smoothly navigate diverse landscapes, from smooth paved surfaces to challenging off-road terrains.</p><p><img src="./render.png" alt="render" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="software-overview"><a href="#software-overview">Software Overview</a></h2><p>I designed Paradigm’s autonomous vehicle software strategy with the singular aim to excel in the Auto-Nav Challenge. This competition involved navigating a course littered with dynamic obstacles, which required a software system capable of advanced obstacle detection, real-time decision-making, and swift path adjustment.</p><p><img src="./ROS.png" alt="ROS" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p>Central to this was the integration of our custom AI and localization pipelines with the ROS2 Nav2 system, a feat that I am particularly proud of. This integration allowed our software to adjust the vehicle’s path in real-time based on the identified obstacles and their estimated locations. The robustness and precision of our software were key in navigating the Auto-Nav Challenge effectively.</p><h3 id="vision-based-obstacle-detection"><a href="#vision-based-obstacle-detection">Vision-based Obstacle Detection</a></h3><p>One of my significant contributions was incorporating advanced computer vision algorithms into our design. Our vehicle was equipped with six high-resolution cameras, capturing 360-degree visuals at 60 frames per second. Two separate instances of Cross-View Transformer (CVT) [1] processed the real-time data from these cameras. Each CVT instance generated a 256x256 matrix that represented the model’s confidence levels in the presence of an obstacle in an approximately one square inch area. I integrated these outputs into a ROS costmap for efficient path planning.</p><p><img src="./ParadigmCVT.png" alt="ParadigmCVT" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p>CVTs use a cross-view, cross-attention mechanism to convert individual camera features into a shared bird’s eye view (BEV) representation. I fine-tuned this model to accurately detect obstacles and predict feasible navigation areas.</p><h3 id="dual-bev-mapping"><a href="#dual-bev-mapping">Dual BEV Mapping</a></h3><p>The two CVTs worked together to form a detailed bird’s-eye-view of the environment. While one CVT instance detected obstacles, the other, dubbed the “driveable” model, mapped feasible navigation areas around the robot. My focus on collaboration and synchronization between these two models played a crucial role in our success in the Auto-Nav Challenge.</p><p>Obstacle model sample Bird’s Eye View prediction (lighter color = higher confidence) <img src="./object_detection.PNG" alt="obstcale" class="rounded-lg my-2" loading="lazy" decoding="async">Driveable model sample Bird’s Eye View prediction (lighter color = higher confidence) <img src="./driveable.PNG" alt="driveable" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h3 id="mapping--path-planning"><a href="#mapping--path-planning">Mapping &amp; Path Planning</a></h3><p>For efficient path planning, I integrated a semantically segmented costmap into the ROS2 navigation system. This strategy involved mapping drivable areas and assigning them a negative cost value, making them more attractive paths for our vehicle.</p><p>Map generation <img src="./map.gif" alt="mapping" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p>I designed our perception system to feed BEV predictions into the ROS Spatio-Temporal Voxel Layer, which helped generate costmaps effectively. In creating the costmap, a two-dimensional grid representing traversal difficulty, I fine-tuned the system to discard low-confidence predictions and apply a temporal filter to reduce false positives. This fine-tuning was a critical aspect of our strategy, helping us maintain an accurate and efficient path planning mechanism.</p><h3 id="localization"><a href="#localization">Localization</a></h3><p>I used the ZED2 stereo camera and the BerryGPS-IMU to accurately localize the vehicle using visual odometry, acceleration, angular velocity, and geolocation data. I used the ROS2 implemented an Extended Kalman Filter (EKF) that consolidated these diverse data inputs, providing an accurate estimate of the vehicle’s position. This localization pipeline is intentionally simple as the high FPS BEV mapping does most of the heavy lifting for navigation.</p><h2 id="simulation"><a href="#simulation">Simulation</a></h2><p>A key element of my development process was the integration of a robust simulation environment into our development cycle. I customized the CARLA [2] simulator, based in Unreal Engine, to replicate the Auto-Nav challenge setting. This involved designing a unique Unreal Engine level that mirrored the competition area with an impressive +/- 0.5 foot accuracy. This level could also be rearranged to simulate various competition layouts for ROS navigation testing and data collection, making it a versatile tool in our development process.</p><p>Paradigm custom IGVC level in Unreal Engine <img src="./sim.PNG" alt="driveable" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p>We leveraged CARLA’s ROS2 Bridge to facilitate a seamless exchange of sensor and control data between the simulator and the ROS2 environment. This enabled us to live stream a range of sensor data, including camera images, IMU readings, and GPS coordinates, directly into our navigation pipeline.</p><p>The use of the CARLA simulator for testing presented us with a myriad of benefits. It allowed us to test and debug our navigation system before the physical vehicle was fully assembled, which significantly accelerated our development cycle. Moreover, it enabled us to simulate various environmental conditions and course layouts in a controlled environment without the associated risks and costs of physical testing.</p><h3 id="model-training-from-simulator-data"><a href="#model-training-from-simulator-data">Model Training from Simulator Data</a></h3><p>I employed the CARLA Python API to generate a diverse dataset for training our computer vision models. This involved integrating our vehicle, into the CARLA level, and adjusting the simulation’s camera positions and intrinsic parameters to match those of the real cameras mounted on the vehicle.</p><p>I then randomly placed the vehicle at different positions within the course, adjusting its orientation each time, to create a robust and diverse dataset. Factors such as the sun’s position, weather conditions, track layout, ground material, and obstacle materials were manipulated to ensure the dataset’s versatility. Total dataset size was about 30,000 image sets (6 cam input &amp; 1 BEV ground truth).</p><p>Sample images collected from sim <img src="./sim_pics.PNG" alt="sim_pics" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p>Inspired by GPT, I pre-trained our CVT models on the nuScenes dataset [3], a large-scale real-world dataset, where they achieved near state-of-the-art BEV prediction performance. Pre-training these models helped them generalize BEV prediction strategies and familiarize themselves with various scenarios. The models were then fine-tuned using our custom-generated IGVC dataset. In initial testing, the CVT models operated in parallel at 58 FPS on the robot’s RTX 4080 GPU and achieved an Intersection Over Union (IoU) score of 92%, a testament to the efficacy of our simulation-based approach.</p><h2 id="compute-hardware"><a href="#compute-hardware">Compute Hardware</a></h2><p>As the creator of the compute subsystem, I incorporated an assembly of high-performance computing modules and camera arrays. I equipped the “flight computer” with an RTX 4080 GPU for real-time machine-learning tasks. My choice of a Jetson Nano and a Raspberry Pi B4 module helped handle camera live-streaming and support IMU and GPS modules over the ROS network.</p><p><img src="./compute_subsystem.png" alt="compute_subsystem" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p>My design of the camera system integrated five cameras with the flight computer, Jetson Nano, and Raspberry Pi B4, along with a standalone ZED 2 camera, constantly feeding real-time visual data into the computer vision models via a router over the ROS network. I can not take credit for the control, or power subsystems, I worked with some fantastic teammates that handled that portion of the design and implementation.</p><h2 id="gpt-for-fundraising"><a href="#gpt-for-fundraising">GPT for Fundraising</a></h2><p>This year, in the absence of a dedicated business team, I used GPT-4 for an unconventional task - fundraising. I designed effective prompts that enabled GPT-4 to create persuasive emails and compelling funding proposals. This innovative application of AI resulted in a successful fundraising campaign, securing $28k to entirely fund our vehicle’s fabrication and competition shipment.</p><h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2><p>Working on Paradigm’s autonomous vehicle software for the IGVC was a rewarding and educational experience. My journey entailed designing and implementing ROS perception, navigation, control, and sensor code, all of which were tailored to master the Auto-Nav Challenge. The advanced computer vision algorithms, efficient path planning, and robust localization approach I employed all culminated in a system that successfully navigated an unpredictable environment in real-time.</p><p>The CARLA simulation environment was instrumental in accelerating our development cycle, providing a versatile platform for testing, debugging, and data generation. I’m proud of the final results and the integral role I played in Paradigm’s success in the competition.</p><p>Stay tuned for more exciting projects and technical insights in my future posts! Thanks for reading.</p><p><img src="./bot.png" alt="bot" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h4 id="references"><a href="#references">References</a></h4><p>[1] B. Zhou and P. Krähenbühl, “Cross-view Transformers for real-time Map-view Semantic Segmentation,” Computer Vision and Pattern Recognition, 2022.</p><p>[2] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez and V. Koltun, “CARLA: An Open Urban Driving Simulator,” Conference on Robot Learning, 2017.</p><p>[3] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Lion, Q. Xu, A. Krishnan, Y. Pan, G. Baldan and O. Beijbom, “nuScenes: A multimodal dataset for autonomous driving,” Computer Vision and Pattern Recognition, 2019.</p><p>Disclaimer: Please note that some details have been simplified for the sake of readability.</p>]]>
    </content>
    <category term="Computer Vision" scheme="https://urara-demo.netlify.app/?tags=Computer%20Vision" />
    <category term="Transformer" scheme="https://urara-demo.netlify.app/?tags=Transformer" />
  </entry>
  <entry>
    <title type="html"><![CDATA[My Research - Herds Eye View]]></title>
    <link href="https://urara-demo.netlify.app/hev" />
    <id>https://urara-demo.netlify.app/hev</id>
    <published>2023-04-20T00:00:00.000Z</published>
    <updated>2023-06-14T20:55:04.596Z</updated>
    <content type="html">
      <![CDATA[<h1 id="herds-eye-view-improving-game-ai-agent-learning-with-collaborative-perception"><a href="#herds-eye-view-improving-game-ai-agent-learning-with-collaborative-perception">Herd’s Eye View: Improving Game AI Agent Learning with Collaborative Perception</a></h1><h2 id="introduction"><a href="#introduction">Introduction</a></h2><p>In the realm of game AI, agents traditionally have access to extensive global information from the game engine. While this configuration assists in efficient decision-making, it doesn’t accurately represent the restrictions encountered by AI applications outside of gaming. Game AI techniques that rely predominantly on game engine data may limit their potential contribution to broader AI applications.</p><p>To address these challenges, we present a novel paradigm named Herd’s Eye View (HEV) that adopts a global perspective derived from multiple agents to boost decision-making capabilities of reinforcement learning (RL) agents in multi-agent environments. The HEV approach utilizes cooperative perception to empower RL agents with a global reasoning ability, enhancing their decision-making.</p><h2 id="herds-eye-view-a-new-perspective"><a href="#herds-eye-view-a-new-perspective">Herd’s Eye View: A New Perspective</a></h2><p>Drawing inspiration from autonomous vehicles research and the Bird’s Eye View (BEV) model, we propose a similar intermediary representation for game AI: the Herd’s Eye View (HEV) model. Unlike the BEV’s individual-centric perspective, the HEV model offers a shared world-centric perception derived from multiple agents. This shared perception model aligns closer to real-world AI applications, where multiple systems often work together to understand and navigate their environment.</p><p>The HEV model presents dual advantages. First, it mirrors the constraints faced by AI outside of gaming, contributing to the development of more believable AI behavior in games. Second, it alleviates the computational demands associated with the BEV model, where each agent maintains its own unique view of the environment. Instead, only a single shared global view is utilized.</p><p><img src="./HEVMethod.png" alt="HEV" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="perspective-views"><a href="#perspective-views">Perspective Views</a></h2><h3 id="world-centric-herds-eye-view"><a href="#world-centric-herds-eye-view">World-Centric (Herd’s Eye View)</a></h3><p>The world-centric view, also known as the Herd’s Eye View (HEV), offers a shared global perception derived from multiple agents. This view allows all agents to have a comprehensive understanding of the entire environment, which can enhance their decision-making capabilities and coordination.</p><h3 id="ego-centric"><a href="#ego-centric">Ego-Centric</a></h3><p>The ego-centric view is an individual-centric perspective where each agent maintains its own unique view of the environment. This view can limit the agents’ understanding of the environment to their immediate surroundings.</p><h3 id="ego-forward"><a href="#ego-forward">Ego-Forward</a></h3><p>The ego-forward view is a variation of the ego-centric view where the perspective is always forward-facing relative to the agent. This view can be limiting as it restricts the agents’ perception to only what is in front of them.</p><p><img src="./POVFigure.png" alt="POV" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="key-contributions"><a href="#key-contributions">Key Contributions</a></h2><p>Our work makes the following contributions:</p><ol><li>We propose a baseline model for performing semantic segmentation in a fixed ”HEV” world-view.</li><li>We demonstrate the effectiveness of the HEV fixed world viewpoint in improving collaborative perception and MARL in games.</li></ol><p>Our exploration of more realistic perception models and the application of reinforcement learning provides```markdownsignificant insights for game AI development, stressing the wider applicability of these techniques beyond the gaming industry.</p><h2 id="environments"><a href="#environments">Environments</a></h2><h3 id="collaborative-push-block"><a href="#collaborative-push-block">Collaborative Push Block</a></h3><p>In this environment, agents are required to push white blocks to a designated green area. The challenge here is that larger blocks require more agents to push, necessitating cooperation and coordination among the agents.</p><h3 id="dungeon-escape"><a href="#dungeon-escape">Dungeon Escape</a></h3><p>This environment presents a unique challenge where agents must sacrifice one of their own to take down a green dragon and obtain a key. The rest of the team must then use this key to escape the dungeon. This scenario tests the agents’ ability to strategize and make sacrifices for the greater good.</p><h3 id="planar-construction"><a href="#planar-construction">Planar Construction</a></h3><p>In the Planar Construction environment, six agents collaborate to push red pucks into desired positions. The positions can be random or static, and are observed via a Grid-Sensor, similar to the push block environment. This environment tests the agents’ ability to work together in a more complex and dynamic setting.</p><p><img src="./envs.PNG" alt="envs" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="experiments"><a href="#experiments">Experiments</a></h2><h3 id="accuracy-of-world-centric-predictions-vs-ego-centric"><a href="#accuracy-of-world-centric-predictions-vs-ego-centric">Accuracy of World-Centric Predictions vs Ego-Centric</a></h3><p>The first experiment compares the accuracy of world-centric predictions using HEV with ego-centric predictions using BEV. We conduct this experiment in three simulated Multi-Agent Reinforcement Learning (MARL) game environments.</p><p>HEV-CVT validation IoU results per coordinate frame in each environment (higher is better)</p><div class="overflow-x-auto mb-4"><table class="table w-full"><thead><tr><th>Environment</th><th>World-Centric</th><th>Ego-Centric</th><th>Ego-Forward</th></tr></thead><tbody><tr><td>Collaborative Push Block</td><td>96.94%</td><td>63.87%</td><td>64.22%</td></tr><tr><td>Dungeon Escape</td><td>43.53%</td><td>1.82%</td><td>26.07%</td></tr><tr><td>Planar Construction</td><td>48.37%</td><td>35.45%</td><td>10.16%</td></tr></tbody></table></div><h3 id="efficiency-of-perspective-view-policies-learned"><a href="#efficiency-of-perspective-view-policies-learned">Efficiency of Perspective View Policies Learned</a></h3><p>The second experiment evaluates the efficiency of policies learned by RL agents trained on HEV views compared to those trained on BEV views.</p><p>Table 2: MA-POCA mean episode length per coordinate frame in each environment (lower is better)</p><div class="overflow-x-auto mb-4"><table class="table w-full"><thead><tr><th>Environment</th><th>World-Centric</th><th>Ego-Centric</th><th>Ego-Forward</th></tr></thead><tbody><tr><td>Collaborative Push Block</td><td>222.6</td><td>246.6</td><td>230.5</td></tr><tr><td>Dungeon Escape</td><td>14.15</td><td>16.74</td><td>23.27</td></tr><tr><td>Planar Construction</td><td>110.5</td><td>120.3</td><td>149.9</td></tr></tbody></table></div><h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2><p>The Herd’s Eye View (HEV) framework offers a superior perception model in MARL environments, providing agents with a more comprehensive understanding of their surroundings, leading to improved decision-making and better overall performance. By using the HEV world-view collaborative perception problem, we show that the accuracy of collaborative perception models is significantly improved, leading to better IoU scores. Our work opens up new possibilities for advanced perception models in MARL, which can greatly enhance the performance of multi-robot systems by enabling better collaboration and coordination.</p><p>The code for this project is available on <a href="https://github.com/andrewnash/Herds-Eye-View" rel="nofollow noopener noreferrer external" target="_blank">GitHub</a>.</p>]]>
    </content>
    <category term="Computer Vision" scheme="https://urara-demo.netlify.app/?tags=Computer%20Vision" />
    <category term="RL" scheme="https://urara-demo.netlify.app/?tags=RL" />
  </entry>
  <entry>
    <title type="html"><![CDATA[Subsea Resident Autonomous Underwater Vehicle]]></title>
    <link href="https://urara-demo.netlify.app/srauv" />
    <id>https://urara-demo.netlify.app/srauv</id>
    <published>2020-05-02T00:00:00.000Z</published>
    <updated>2023-06-14T20:55:04.816Z</updated>
    <content type="html">
      <![CDATA[<h1 id="navigating-the-depths-with-rl"><a href="#navigating-the-depths-with-rl">Navigating the Depths with RL</a></h1><p>Hello folks! My name is Andrew Nash, and I’ve been heavily involved in developing a software system for an autonomous subsea vehicle, the SRAUV, as part of my engineering capstone project. In this blog post, I’ll detail the work I did on the Autopilot/AI perception, navigation, and control. Strap in for an exciting journey deep beneath the waves!</p><center><iframe width="560" height="315" src="https://www.youtube.com/embed/wzNI8wY0UT0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></center><h2 id="introduction"><a href="#introduction">Introduction</a></h2><p>In my capstone project, my team and I have tackled a pervasive problem in offshore oil and gas operations - consistent and reliable subsea monitoring. To address this, we’ve designed a prototype Subsea Resident Autonomous Underwater Vehicle (SRAUV). Our innovative SRAUV operates indefinitely under the sea, utilizing a dock on the seafloor that facilitates wireless charging and communication. It embarks on scheduled or on-demand inspection missions, collects vital data, and then returns to the dock to transmit this information. Through this approach, we’re ushering in a new era of advanced offshore operations, with potential upgrades such as real-time anomaly detection that promises enhanced efficiency and safety.</p><p><img src="./srauv_ocean.png" alt="srauv" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="vehicle"><a href="#vehicle">Vehicle</a></h2><p>My exceptional teammates ensured the SRAUV’s mechanical design adhered to principles of hydrodynamics, stability, and compactness, resulting in a sleek, efficient design. Fitting within the original constraints of a 2-foot length and width, the vehicle’s overall mass is approximately 15 kg, as illustrated below. The team cleverly configured the thrusters to offer a versatile maneuverability, giving the SRAUV 5 degrees of freedom, though effectively using only 4 - excluding pitch and roll - for the project’s final demonstration.</p><p><img src="./mech.png" alt="mech" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="software-overview"><a href="#software-overview">Software Overview</a></h2><p>The SRAUV software is a culmination of collaborative effort, using Unity 3D game engine for fast prototyping, extensive machine learning support, and engaging visual feedback. The interface is an all-in-one GUI Simulator, allowing the user to control the vehicle, visualize thruster direction feedback, and simulate various environmental conditions. All software for the project is openly available on Github.</p><h2 id="simulator"><a href="#simulator">Simulator</a></h2><p>We developed a Unity-based simulator that accurately mimics the mechanics of the SRAUV. This simulator applies individual thrust vectors at each thruster’s respective angle, fine-tuning its underwater movement, often referred to as “driftyness”, to reflect the real-world feedback from experienced pilots. The simulation updates in response to these thrust commands, providing new sensor data which includes the SRAUV’s position and the relative locations of assets in the environment. This helped us efficiently and accurately test our autonomous flying solutions.</p><p><img src="./unity_sim.png" alt="unity_sim" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="model-training"><a href="#model-training">Model Training</a></h2><p>In addition to the main simulator, I created a lighter “ML Tank” for training the DRL algorithms. This environment include a significantly reduced polygon count, a necessary optimization since DRL algorithms are computationally expensive to simulate. Further speed enhancements were achieved by including multiple ML Tanks, allowing multiple agents to collect observations simultaneously, this can be seen below.</p><p>Screenshot of 49 ML Tanks training simultaneously within the Unity environment. <img src="./49_tanks.gif" alt="tanks" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="autonomous-strategy"><a href="#autonomous-strategy">Autonomous Strategy</a></h2><p>As the core designer of the Autopilot, I initially considered employing Q-learning for the SRAUV’s control system. This approach was favored due to its popularity and well-documented success. However, throughout the model selection process, I found Deep Reinforcement Learning (DRL) to be the most effective at driving in the simulator. I utilized Unity ML-Agents Toolkit for DRL training within the Unity simulator, leading to rapid experimentation with the SRAUV’s Flight Computer and Vision System.</p><p><img src="./auto_pilot.png" alt="system" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p>With DRL, I designed an end-to-end solution for our control system. This allowed observations to go into the model and thrust control to emerge, providing optimal control of the SRAUV through a variety of unpredictable scenarios.</p><h2 id="model-development"><a href="#model-development">Model Development</a></h2><p>For model training, I used a simplified subproblem - driving from a random X, Y, Z position to another random position as quickly as possible. I then tested various DRL solutions on these subproblems, progressively introducing complexity until the environment closely mirrored real life. This iterative approach allowed for significant improvement during model development.</p><p><img src="./autnomous.gif" alt="autnomous_flying" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h3 id="model-selection"><a href="#model-selection">Model Selection</a></h3><p>For the SRAUV control system, I considered both discrete and continuous action spaces. While a continuous action space implies real numbers for the DRL model output, a discrete one implies a fixed set of categories. I experimentally found discrete to be more effective during training. I benchmarked the Soft Actor-Critic (SAC) model against a Proximal Policy Optimization (PPO). SAC took a considerable time to train and often failed to reach the goal consistently. On the other hand, PPO found the optimal driving strategy faster and more effectively, which led to its selection for further development.</p><h3 id="reward-structure"><a href="#reward-structure">Reward Structure</a></h3><p>The reward structure found to best solve the easy environment was relatively straightforward, consisting of three different rewards. The first reward was assigned as +1 when the agent arrived within a predefined tolerance of its goal waypoint. The second reward was the negative inverse of maximum time steps -0.002, this small negative reward assigned every timestep encouraged the model to drive to the goal as fast as possible. Finally, the third reward sets the overall reward to -1 if the agent collides with anything, this also resets the run for increased training speed.</p><p><img src="./tank_driving.gif" alt="tank_driving" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p>In further iterations of the training environment, I found that the agent would sometimes overshoot its goal position and accidentally crash into a wall, to help mitigate this problem a stacking negative reward was assigned whenever the agent exceeded 0.3 m/s velocity in any axis of movement. This resulted in reduced crashing and taught the model to operate at a reduced speed when nearing its goal position.</p><h3 id="curriculum-learning"><a href="#curriculum-learning">Curriculum Learning</a></h3><p>Since the PPO algorithm struggled to converge when tolerances of less than 40cm were used in training, I utilized curriculum learning to overcome this challenge. This technique allowed me to progressively decrease the tolerance required to reach the goal waypoint during training, significantly enhancing the model’s performance.</p><p><img src="./reward.PNG" alt="srauv_in_tank" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="localization"><a href="#localization">Localization</a></h2><p>After our initial approach for localization, which relied on a combination of a pressure sensor, time-of-flight (TOF) sensors, and an inertial-measurement-unit (IMU), failed to meet expectations, my team and I were forced to quickly pivot. Due to issues ranging from faulty TOF sensors, an unsteady IMU, and a malfunctioning depth sensor, we had to reimagine our solution, leading us to the exploration of a computer vision-based approach. Utilizing AprilTags, similar to simple QR codes, we established a localization system that successfully tracked the position and rotation of the SRAUV relative to the tank. We installed a grid of these AprilTags on the tank floor, ensuring the bottom camera of the SRAUV was able to constantly locate a tag, significantly improving our vehicle’s spatial awareness. Impressively, this solution not only worked exceedingly well during testing, achieving an accuracy of about +-5 cm in position and +-5 degrees in heading, but it was also considerably more cost-effective than our initial sensor plan.</p><p><img src="./srauv_in_tank.png" alt="srauv_in_tank" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p>The AprilTags detection in the ML Tank was another area I optimized. Running the vision recognition on all 49 tanks in a single environment would have been computationally prohibitive. To address this, I added a camera with matching intrinsic parameters and position to the SRAUV’s bottom pi cam in the simulator. This allowed me to check if the AprilTag was in the camera’s frame during each timestep, providing the agent with the most recently known position &amp; velocity estimate, simulating the noisy conditions it would face in the real world.</p><p><img src="./april_tank.gif" alt="april_tags" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="real-world-testing"><a href="#real-world-testing">Real World Testing</a></h2><p>Below you can see some videos of real world testing the SRAUV in a tank at Memorial University.</p><p><img src="./testing1.gif" alt="testing1" class="rounded-lg my-2" loading="lazy" decoding="async"></p><p><img src="./testing2.gif" alt="testing2" class="rounded-lg my-2" loading="lazy" decoding="async"></p><h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2><p>Wrapping up, I am incredibly proud of the progress and achievements made by my team and myself throughout our capstone project. We successfully designed, implemented, and tested a neural network-based autopilot for our Subsea Resident AUV (SRAUV) prototype. This enabled the SRAUV to perform autonomous missions, including navigating a prescribed route and redocking, a significant milestone for us. Even though we faced setbacks, such as hardware failures limiting the testing of an improved autopilot model, these challenges only fueled our drive to innovate. Our commitment paid off, winning us first place in all capstone award categories. We believe our work has set a strong foundation for future developments in autonomous subsea navigation.</p><p><img src="./srauv.png" alt="srauv" class="rounded-lg my-2" loading="lazy" decoding="async"></p><center><iframe width="560" height="315" src="https://www.youtube.com/embed/v3Z9sVuU5DQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><iframe width="560" height="315" src="https://www.youtube.com/embed/UXqgjFvW4HE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></center>]]>
    </content>
    <category term="Computer Vision" scheme="https://urara-demo.netlify.app/?tags=Computer%20Vision" />
    <category term="RL" scheme="https://urara-demo.netlify.app/?tags=RL" />
  </entry>
</feed>