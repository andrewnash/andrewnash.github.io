import{S as Wi,i as Xi,s as Qi,U as Da,y as v,z as g,A as y,V as Yi,W as Ki,g as w,d as b,B as _,X as Zi,k as n,q as d,a as l,l as o,m as s,r as f,h as t,c as m,n as p,b as i,D as r,E as en}from"../chunks/index.17f79c6b.js";import{P as tn}from"../chunks/post_layout.17c611d9.js";import{I as P}from"../chunks/footer.b2ef5326.js";function an(ae){let u,h,E,T,c,$,A,ie,C,We,V,ne,qt,Xe,oe,Kt,Qe,se,S,Ye,R,re,Zt,et,le,Jt,tt,me,k,at,G,de,Wt,it,fe,Xt,nt,pe,O,ot,ue,Qt,st,B,ce,Yt,rt,he,ea,lt,ve,M,mt,ge,ta,dt,x,ye,aa,ft,we,ia,pt,I,na,L,oa,D,ut,z,be,sa,ct,_e,ra,ht,N,la,j,vt,Pe,ma,gt,H,$e,da,yt,Ee,fa,wt,U,Ie,pa,bt,Te,ua,_t,F,ca,q,Pt,Ae,ha,$t,Ce,va,Et,K,Ve,ga,It,Se,ya,Tt,Re,wa,At,Z,ba,J,Ct,ke,_a,Vt,W,Ge,Pa,St,Oe,$a,Rt,Be,X,kt,Me,Ea,Gt,Q,xe,Ia,Ot,Le,Ta,Bt,Y,De,Aa,Mt,ze,Ca,xt,Ne,Va,Lt,je,Sa,Dt,He,ee,zt,te,Ue,Ra,Nt,Fe,ka,jt,qe,Ga,Ht,Ke,Oa,Ut,Ze,Ba,Ft;return C=new P({props:{src:"./robot.jpg",alt:"robot"}}),S=new P({props:{src:"./comp_track.PNG",alt:"track"}}),k=new P({props:{src:"./render.png",alt:"render"}}),O=new P({props:{src:"./ROS.png",alt:"ROS"}}),M=new P({props:{src:"./ParadigmCVT.png",alt:"ParadigmCVT"}}),L=new P({props:{src:"./object_detection.PNG",alt:"obstcale"}}),D=new P({props:{src:"./driveable.PNG",alt:"driveable"}}),j=new P({props:{src:"./map.gif",alt:"mapping"}}),q=new P({props:{src:"./sim.PNG",alt:"driveable"}}),J=new P({props:{src:"./sim_pics.PNG",alt:"sim_pics"}}),X=new P({props:{src:"./compute_subsystem.png",alt:"compute_subsystem"}}),ee=new P({props:{src:"./bot.png",alt:"bot"}}),{c(){u=n("h1"),h=n("a"),E=d("A Deep Dive into My Autonomous Vehicle Software Design Journey"),T=l(),c=n("p"),$=d("Hello everyone! I’m Andrew Nash, and for the past year, I’ve been leading the software development efforts for Paradigm’s autonomous vehicle in the Intelligent Ground Vehicle Competition (IGVC). This has been an exciting journey, full of learning, innovation, and countless lines of code. In this post, I’ll walk you through the technical nuances of the software I developed and implemented. My role involved designing and implimenting all perception (including all ML models), navigation, control, and sensor code."),A=l(),ie=n("p"),v(C.$$.fragment),We=l(),V=n("h2"),ne=n("a"),qt=d("IGVC Auto-Nav Challenge"),Xe=l(),oe=n("p"),Kt=d("To compete at the 30th annual IGVC, Paradigm had to design and build an original vehicle that can autonomously navigate an obstacle course similar to the one shown below. The vehicle must stay within lane lines and avoid obstacles on an unknown track layout."),Qe=l(),se=n("p"),v(S.$$.fragment),Ye=l(),R=n("h2"),re=n("a"),Zt=d("Vehicle"),et=l(),le=n("p"),Jt=d("The mechanical design of our competition entry is a testament to the team’s innovation and engineering prowess. The vehicle’s distinctive skid-steer drivetrain, with its two separate drivetrain modules, truly stands out. This configuration enables superior maneuverability, allowing the vehicle to smoothly navigate diverse landscapes, from smooth paved surfaces to challenging off-road terrains."),tt=l(),me=n("p"),v(k.$$.fragment),at=l(),G=n("h2"),de=n("a"),Wt=d("Software Overview"),it=l(),fe=n("p"),Xt=d("I designed Paradigm’s autonomous vehicle software strategy with the singular aim to excel in the Auto-Nav Challenge. This competition involved navigating a course littered with dynamic obstacles, which required a software system capable of advanced obstacle detection, real-time decision-making, and swift path adjustment."),nt=l(),pe=n("p"),v(O.$$.fragment),ot=l(),ue=n("p"),Qt=d("Central to this was the integration of our custom AI and localization pipelines with the ROS2 Nav2 system, a feat that I am particularly proud of. This integration allowed our software to adjust the vehicle’s path in real-time based on the identified obstacles and their estimated locations. The robustness and precision of our software were key in navigating the Auto-Nav Challenge effectively."),st=l(),B=n("h3"),ce=n("a"),Yt=d("Vision-based Obstacle Detection and Avoidance"),rt=l(),he=n("p"),ea=d("One of my significant contributions was incorporating advanced computer vision algorithms into our design. Our vehicle was equipped with six high-resolution cameras, capturing 360-degree visuals at 60 frames per second. Two separate instances of Cross-View Transformer (CVT) [1] processed the real-time data from these cameras. Each CVT instance generated a 256x256 matrix that represented the model’s confidence levels in the presence of an obstacle in an approximately one square inch area. I integrated these outputs into a ROS costmap for efficient path planning."),lt=l(),ve=n("p"),v(M.$$.fragment),mt=l(),ge=n("p"),ta=d("CVTs use a cross-view, cross-attention mechanism to convert individual camera features into a shared bird’s eye view (BEV) representation. I fine-tuned this model to accurately detect obstacles and predict feasible navigation areas."),dt=l(),x=n("h4"),ye=n("a"),aa=d("Obstacle Avoidance"),ft=l(),we=n("p"),ia=d("The two CVTs worked together to form a detailed bird’s-eye-view of the environment. While one CVT instance detected obstacles, the other, dubbed the “driveable” model, mapped feasible navigation areas around the robot. My focus on collaboration and synchronization between these two models played a crucial role in our success in the Auto-Nav Challenge."),pt=l(),I=n("p"),na=d("Obstacle model sample Bird’s Eye View prediction (lighter color = higher confidence) "),v(L.$$.fragment),oa=d(`
Driveable model sample Bird’s Eye View prediction (lighter color = higher confidence) `),v(D.$$.fragment),ut=l(),z=n("h3"),be=n("a"),sa=d("Mapping & Path Planning"),ct=l(),_e=n("p"),ra=d("For efficient path planning, I integrated a semantically segmented costmap into the ROS2 navigation system. This strategy involved mapping drivable areas and assigning them a negative cost value, making them more attractive paths for our vehicle."),ht=l(),N=n("p"),la=d("Map generation "),v(j.$$.fragment),vt=l(),Pe=n("p"),ma=d("I designed our perception system to feed BEV predictions into the ROS Spatio-Temporal Voxel Layer, which helped generate costmaps effectively. In creating the costmap, a two-dimensional grid representing traversal difficulty, I fine-tuned the system to discard low-confidence predictions and apply a temporal filter to reduce false positives. This fine-tuning was a critical aspect of our strategy, helping us maintain an accurate and efficient path planning mechanism."),gt=l(),H=n("h3"),$e=n("a"),da=d("Localization"),yt=l(),Ee=n("p"),fa=d("I used the ZED2 stereo camera and the BerryGPS-IMU to accurately localize the vehicle using visual odometry, acceleration, angular velocity, and geolocation data. I used the ROS2 implemented an Extended Kalman Filter (EKF) that consolidated these diverse data inputs, providing an accurate estimate of the vehicle’s position. This localization pipeline is intentionally simple as the high FPS BEV mapping does most of the heavy lifting for navigation."),wt=l(),U=n("h2"),Ie=n("a"),pa=d("Simulation"),bt=l(),Te=n("p"),ua=d("A key element of my development process was the integration of a robust simulation environment into our development cycle. I customized the CARLA [2] simulator, based in Unreal Engine, to replicate the Auto-Nav challenge setting. This involved designing a unique Unreal Engine level that mirrored the competition area with an impressive +/- 0.5 foot accuracy. This level could also be rearranged to simulate various competition layouts for ROS navigation testing and data collection, making it a versatile tool in our development process."),_t=l(),F=n("p"),ca=d("Paradigm custom IGVC level in Unreal Engine "),v(q.$$.fragment),Pt=l(),Ae=n("p"),ha=d("We leveraged CARLA’s ROS2 Bridge to facilitate a seamless exchange of sensor and control data between the simulator and the ROS2 environment. This enabled us to live stream a range of sensor data, including camera images, IMU readings, and GPS coordinates, directly into our navigation pipeline."),$t=l(),Ce=n("p"),va=d("The use of the CARLA simulator for testing presented us with a myriad of benefits. It allowed us to test and debug our navigation system before the physical vehicle was fully assembled, which significantly accelerated our development cycle. Moreover, it enabled us to simulate various environmental conditions and course layouts in a controlled environment without the associated risks and costs of physical testing."),Et=l(),K=n("h3"),Ve=n("a"),ga=d("Model Training from Simulator Data"),It=l(),Se=n("p"),ya=d("I employed the CARLA Python API to generate a diverse dataset for training our computer vision models. This involved integrating our vehicle, into the CARLA level, and adjusting the simulation’s camera positions and intrinsic parameters to match those of the real cameras mounted on the vehicle."),Tt=l(),Re=n("p"),wa=d("I then randomly placed the vehicle at different positions within the course, adjusting its orientation each time, to create a robust and diverse dataset. Factors such as the sun’s position, weather conditions, track layout, ground material, and obstacle materials were manipulated to ensure the dataset’s versatility. Total dataset size was about 30,000 image sets (6 cam input & 1 BEV ground truth)."),At=l(),Z=n("p"),ba=d("Sample images collected from sim "),v(J.$$.fragment),Ct=l(),ke=n("p"),_a=d("Inspired by GPT, I pre-trained our CVT models on the nuScenes dataset [3], a large-scale real-world dataset, where they achieved near state-of-the-art BEV prediction performance. Pre-training these models helped them generalize BEV prediction strategies and familiarize themselves with various scenarios. The models were then fine-tuned using our custom-generated IGVC dataset. In initial testing, the CVT models operated in parallel at 58 FPS on the robot’s RTX 4080 GPU and achieved an Intersection Over Union (IoU) score of 92%, a testament to the efficacy of our simulation-based approach."),Vt=l(),W=n("h2"),Ge=n("a"),Pa=d("Compute Hardware"),St=l(),Oe=n("p"),$a=d("As the creator of the compute subsystem, I incorporated an assembly of high-performance computing modules and camera arrays. I equipped the “flight computer” with an RTX 4080 GPU for real-time machine-learning tasks. My choice of a Jetson Nano and a Raspberry Pi B4 module helped handle camera live-streaming and support IMU and GPS modules over the ROS network."),Rt=l(),Be=n("p"),v(X.$$.fragment),kt=l(),Me=n("p"),Ea=d("My design of the camera system integrated five cameras with the flight computer, Jetson Nano, and Raspberry Pi B4, along with a standalone ZED 2 camera, constantly feeding real-time visual data into the computer vision models via a router over the ROS network. I can not take credit for the control, or power subsystems, I worked with some fantastic teammates that handled that portion of the design and implementation."),Gt=l(),Q=n("h2"),xe=n("a"),Ia=d("GPT for Fundraising"),Ot=l(),Le=n("p"),Ta=d("This year, in the absence of a dedicated business team, I used GPT-4 for an unconventional task - fundraising. I designed effective prompts that enabled GPT-4 to create persuasive emails and compelling funding proposals. This innovative application of AI resulted in a successful fundraising campaign, securing $28k to entirely fund our vehicle’s fabrication and competition shipment."),Bt=l(),Y=n("h2"),De=n("a"),Aa=d("Conclusion"),Mt=l(),ze=n("p"),Ca=d("Working on Paradigm’s autonomous vehicle software for the IGVC was a rewarding and educational experience. My journey entailed designing and implementing ROS perception, navigation, control, and sensor code, all of which were tailored to master the Auto-Nav Challenge. The advanced computer vision algorithms, efficient path planning, and robust localization approach I employed all culminated in a system that successfully navigated an unpredictable environment in real-time."),xt=l(),Ne=n("p"),Va=d("The CARLA simulation environment was instrumental in accelerating our development cycle, providing a versatile platform for testing, debugging, and data generation. I’m proud of the final results and the integral role I played in Paradigm’s success in the competition."),Lt=l(),je=n("p"),Sa=d("Stay tuned for more exciting projects and technical insights in my future posts! Thanks for reading."),Dt=l(),He=n("p"),v(ee.$$.fragment),zt=l(),te=n("h4"),Ue=n("a"),Ra=d("References"),Nt=l(),Fe=n("p"),ka=d("[1] B. Zhou and P. Krähenbühl, “Cross-view Transformers for real-time Map-view Semantic Segmentation,” Computer Vision and Pattern Recognition, 2022."),jt=l(),qe=n("p"),Ga=d("[2] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez and V. Koltun, “CARLA: An Open Urban Driving Simulator,” Conference on Robot Learning, 2017."),Ht=l(),Ke=n("p"),Oa=d("[3] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Lion, Q. Xu, A. Krishnan, Y. Pan, G. Baldan and O. Beijbom, “nuScenes: A multimodal dataset for autonomous driving,” Computer Vision and Pattern Recognition, 2019."),Ut=l(),Ze=n("p"),Ba=d("Disclaimer: Please note that some details have been simplified for the sake of readability."),this.h()},l(e){u=o(e,"H1",{id:!0});var a=s(u);h=o(a,"A",{href:!0});var za=s(h);E=f(za,"A Deep Dive into My Autonomous Vehicle Software Design Journey"),za.forEach(t),a.forEach(t),T=m(e),c=o(e,"P",{});var Na=s(c);$=f(Na,"Hello everyone! I’m Andrew Nash, and for the past year, I’ve been leading the software development efforts for Paradigm’s autonomous vehicle in the Intelligent Ground Vehicle Competition (IGVC). This has been an exciting journey, full of learning, innovation, and countless lines of code. In this post, I’ll walk you through the technical nuances of the software I developed and implemented. My role involved designing and implimenting all perception (including all ML models), navigation, control, and sensor code."),Na.forEach(t),A=m(e),ie=o(e,"P",{});var ja=s(ie);g(C.$$.fragment,ja),ja.forEach(t),We=m(e),V=o(e,"H2",{id:!0});var Ha=s(V);ne=o(Ha,"A",{href:!0});var Ua=s(ne);qt=f(Ua,"IGVC Auto-Nav Challenge"),Ua.forEach(t),Ha.forEach(t),Xe=m(e),oe=o(e,"P",{});var Fa=s(oe);Kt=f(Fa,"To compete at the 30th annual IGVC, Paradigm had to design and build an original vehicle that can autonomously navigate an obstacle course similar to the one shown below. The vehicle must stay within lane lines and avoid obstacles on an unknown track layout."),Fa.forEach(t),Qe=m(e),se=o(e,"P",{});var qa=s(se);g(S.$$.fragment,qa),qa.forEach(t),Ye=m(e),R=o(e,"H2",{id:!0});var Ka=s(R);re=o(Ka,"A",{href:!0});var Za=s(re);Zt=f(Za,"Vehicle"),Za.forEach(t),Ka.forEach(t),et=m(e),le=o(e,"P",{});var Ja=s(le);Jt=f(Ja,"The mechanical design of our competition entry is a testament to the team’s innovation and engineering prowess. The vehicle’s distinctive skid-steer drivetrain, with its two separate drivetrain modules, truly stands out. This configuration enables superior maneuverability, allowing the vehicle to smoothly navigate diverse landscapes, from smooth paved surfaces to challenging off-road terrains."),Ja.forEach(t),tt=m(e),me=o(e,"P",{});var Wa=s(me);g(k.$$.fragment,Wa),Wa.forEach(t),at=m(e),G=o(e,"H2",{id:!0});var Xa=s(G);de=o(Xa,"A",{href:!0});var Qa=s(de);Wt=f(Qa,"Software Overview"),Qa.forEach(t),Xa.forEach(t),it=m(e),fe=o(e,"P",{});var Ya=s(fe);Xt=f(Ya,"I designed Paradigm’s autonomous vehicle software strategy with the singular aim to excel in the Auto-Nav Challenge. This competition involved navigating a course littered with dynamic obstacles, which required a software system capable of advanced obstacle detection, real-time decision-making, and swift path adjustment."),Ya.forEach(t),nt=m(e),pe=o(e,"P",{});var ei=s(pe);g(O.$$.fragment,ei),ei.forEach(t),ot=m(e),ue=o(e,"P",{});var ti=s(ue);Qt=f(ti,"Central to this was the integration of our custom AI and localization pipelines with the ROS2 Nav2 system, a feat that I am particularly proud of. This integration allowed our software to adjust the vehicle’s path in real-time based on the identified obstacles and their estimated locations. The robustness and precision of our software were key in navigating the Auto-Nav Challenge effectively."),ti.forEach(t),st=m(e),B=o(e,"H3",{id:!0});var ai=s(B);ce=o(ai,"A",{href:!0});var ii=s(ce);Yt=f(ii,"Vision-based Obstacle Detection and Avoidance"),ii.forEach(t),ai.forEach(t),rt=m(e),he=o(e,"P",{});var ni=s(he);ea=f(ni,"One of my significant contributions was incorporating advanced computer vision algorithms into our design. Our vehicle was equipped with six high-resolution cameras, capturing 360-degree visuals at 60 frames per second. Two separate instances of Cross-View Transformer (CVT) [1] processed the real-time data from these cameras. Each CVT instance generated a 256x256 matrix that represented the model’s confidence levels in the presence of an obstacle in an approximately one square inch area. I integrated these outputs into a ROS costmap for efficient path planning."),ni.forEach(t),lt=m(e),ve=o(e,"P",{});var oi=s(ve);g(M.$$.fragment,oi),oi.forEach(t),mt=m(e),ge=o(e,"P",{});var si=s(ge);ta=f(si,"CVTs use a cross-view, cross-attention mechanism to convert individual camera features into a shared bird’s eye view (BEV) representation. I fine-tuned this model to accurately detect obstacles and predict feasible navigation areas."),si.forEach(t),dt=m(e),x=o(e,"H4",{id:!0});var ri=s(x);ye=o(ri,"A",{href:!0});var li=s(ye);aa=f(li,"Obstacle Avoidance"),li.forEach(t),ri.forEach(t),ft=m(e),we=o(e,"P",{});var mi=s(we);ia=f(mi,"The two CVTs worked together to form a detailed bird’s-eye-view of the environment. While one CVT instance detected obstacles, the other, dubbed the “driveable” model, mapped feasible navigation areas around the robot. My focus on collaboration and synchronization between these two models played a crucial role in our success in the Auto-Nav Challenge."),mi.forEach(t),pt=m(e),I=o(e,"P",{});var Je=s(I);na=f(Je,"Obstacle model sample Bird’s Eye View prediction (lighter color = higher confidence) "),g(L.$$.fragment,Je),oa=f(Je,`
Driveable model sample Bird’s Eye View prediction (lighter color = higher confidence) `),g(D.$$.fragment,Je),Je.forEach(t),ut=m(e),z=o(e,"H3",{id:!0});var di=s(z);be=o(di,"A",{href:!0});var fi=s(be);sa=f(fi,"Mapping & Path Planning"),fi.forEach(t),di.forEach(t),ct=m(e),_e=o(e,"P",{});var pi=s(_e);ra=f(pi,"For efficient path planning, I integrated a semantically segmented costmap into the ROS2 navigation system. This strategy involved mapping drivable areas and assigning them a negative cost value, making them more attractive paths for our vehicle."),pi.forEach(t),ht=m(e),N=o(e,"P",{});var Ma=s(N);la=f(Ma,"Map generation "),g(j.$$.fragment,Ma),Ma.forEach(t),vt=m(e),Pe=o(e,"P",{});var ui=s(Pe);ma=f(ui,"I designed our perception system to feed BEV predictions into the ROS Spatio-Temporal Voxel Layer, which helped generate costmaps effectively. In creating the costmap, a two-dimensional grid representing traversal difficulty, I fine-tuned the system to discard low-confidence predictions and apply a temporal filter to reduce false positives. This fine-tuning was a critical aspect of our strategy, helping us maintain an accurate and efficient path planning mechanism."),ui.forEach(t),gt=m(e),H=o(e,"H3",{id:!0});var ci=s(H);$e=o(ci,"A",{href:!0});var hi=s($e);da=f(hi,"Localization"),hi.forEach(t),ci.forEach(t),yt=m(e),Ee=o(e,"P",{});var vi=s(Ee);fa=f(vi,"I used the ZED2 stereo camera and the BerryGPS-IMU to accurately localize the vehicle using visual odometry, acceleration, angular velocity, and geolocation data. I used the ROS2 implemented an Extended Kalman Filter (EKF) that consolidated these diverse data inputs, providing an accurate estimate of the vehicle’s position. This localization pipeline is intentionally simple as the high FPS BEV mapping does most of the heavy lifting for navigation."),vi.forEach(t),wt=m(e),U=o(e,"H2",{id:!0});var gi=s(U);Ie=o(gi,"A",{href:!0});var yi=s(Ie);pa=f(yi,"Simulation"),yi.forEach(t),gi.forEach(t),bt=m(e),Te=o(e,"P",{});var wi=s(Te);ua=f(wi,"A key element of my development process was the integration of a robust simulation environment into our development cycle. I customized the CARLA [2] simulator, based in Unreal Engine, to replicate the Auto-Nav challenge setting. This involved designing a unique Unreal Engine level that mirrored the competition area with an impressive +/- 0.5 foot accuracy. This level could also be rearranged to simulate various competition layouts for ROS navigation testing and data collection, making it a versatile tool in our development process."),wi.forEach(t),_t=m(e),F=o(e,"P",{});var xa=s(F);ca=f(xa,"Paradigm custom IGVC level in Unreal Engine "),g(q.$$.fragment,xa),xa.forEach(t),Pt=m(e),Ae=o(e,"P",{});var bi=s(Ae);ha=f(bi,"We leveraged CARLA’s ROS2 Bridge to facilitate a seamless exchange of sensor and control data between the simulator and the ROS2 environment. This enabled us to live stream a range of sensor data, including camera images, IMU readings, and GPS coordinates, directly into our navigation pipeline."),bi.forEach(t),$t=m(e),Ce=o(e,"P",{});var _i=s(Ce);va=f(_i,"The use of the CARLA simulator for testing presented us with a myriad of benefits. It allowed us to test and debug our navigation system before the physical vehicle was fully assembled, which significantly accelerated our development cycle. Moreover, it enabled us to simulate various environmental conditions and course layouts in a controlled environment without the associated risks and costs of physical testing."),_i.forEach(t),Et=m(e),K=o(e,"H3",{id:!0});var Pi=s(K);Ve=o(Pi,"A",{href:!0});var $i=s(Ve);ga=f($i,"Model Training from Simulator Data"),$i.forEach(t),Pi.forEach(t),It=m(e),Se=o(e,"P",{});var Ei=s(Se);ya=f(Ei,"I employed the CARLA Python API to generate a diverse dataset for training our computer vision models. This involved integrating our vehicle, into the CARLA level, and adjusting the simulation’s camera positions and intrinsic parameters to match those of the real cameras mounted on the vehicle."),Ei.forEach(t),Tt=m(e),Re=o(e,"P",{});var Ii=s(Re);wa=f(Ii,"I then randomly placed the vehicle at different positions within the course, adjusting its orientation each time, to create a robust and diverse dataset. Factors such as the sun’s position, weather conditions, track layout, ground material, and obstacle materials were manipulated to ensure the dataset’s versatility. Total dataset size was about 30,000 image sets (6 cam input & 1 BEV ground truth)."),Ii.forEach(t),At=m(e),Z=o(e,"P",{});var La=s(Z);ba=f(La,"Sample images collected from sim "),g(J.$$.fragment,La),La.forEach(t),Ct=m(e),ke=o(e,"P",{});var Ti=s(ke);_a=f(Ti,"Inspired by GPT, I pre-trained our CVT models on the nuScenes dataset [3], a large-scale real-world dataset, where they achieved near state-of-the-art BEV prediction performance. Pre-training these models helped them generalize BEV prediction strategies and familiarize themselves with various scenarios. The models were then fine-tuned using our custom-generated IGVC dataset. In initial testing, the CVT models operated in parallel at 58 FPS on the robot’s RTX 4080 GPU and achieved an Intersection Over Union (IoU) score of 92%, a testament to the efficacy of our simulation-based approach."),Ti.forEach(t),Vt=m(e),W=o(e,"H2",{id:!0});var Ai=s(W);Ge=o(Ai,"A",{href:!0});var Ci=s(Ge);Pa=f(Ci,"Compute Hardware"),Ci.forEach(t),Ai.forEach(t),St=m(e),Oe=o(e,"P",{});var Vi=s(Oe);$a=f(Vi,"As the creator of the compute subsystem, I incorporated an assembly of high-performance computing modules and camera arrays. I equipped the “flight computer” with an RTX 4080 GPU for real-time machine-learning tasks. My choice of a Jetson Nano and a Raspberry Pi B4 module helped handle camera live-streaming and support IMU and GPS modules over the ROS network."),Vi.forEach(t),Rt=m(e),Be=o(e,"P",{});var Si=s(Be);g(X.$$.fragment,Si),Si.forEach(t),kt=m(e),Me=o(e,"P",{});var Ri=s(Me);Ea=f(Ri,"My design of the camera system integrated five cameras with the flight computer, Jetson Nano, and Raspberry Pi B4, along with a standalone ZED 2 camera, constantly feeding real-time visual data into the computer vision models via a router over the ROS network. I can not take credit for the control, or power subsystems, I worked with some fantastic teammates that handled that portion of the design and implementation."),Ri.forEach(t),Gt=m(e),Q=o(e,"H2",{id:!0});var ki=s(Q);xe=o(ki,"A",{href:!0});var Gi=s(xe);Ia=f(Gi,"GPT for Fundraising"),Gi.forEach(t),ki.forEach(t),Ot=m(e),Le=o(e,"P",{});var Oi=s(Le);Ta=f(Oi,"This year, in the absence of a dedicated business team, I used GPT-4 for an unconventional task - fundraising. I designed effective prompts that enabled GPT-4 to create persuasive emails and compelling funding proposals. This innovative application of AI resulted in a successful fundraising campaign, securing $28k to entirely fund our vehicle’s fabrication and competition shipment."),Oi.forEach(t),Bt=m(e),Y=o(e,"H2",{id:!0});var Bi=s(Y);De=o(Bi,"A",{href:!0});var Mi=s(De);Aa=f(Mi,"Conclusion"),Mi.forEach(t),Bi.forEach(t),Mt=m(e),ze=o(e,"P",{});var xi=s(ze);Ca=f(xi,"Working on Paradigm’s autonomous vehicle software for the IGVC was a rewarding and educational experience. My journey entailed designing and implementing ROS perception, navigation, control, and sensor code, all of which were tailored to master the Auto-Nav Challenge. The advanced computer vision algorithms, efficient path planning, and robust localization approach I employed all culminated in a system that successfully navigated an unpredictable environment in real-time."),xi.forEach(t),xt=m(e),Ne=o(e,"P",{});var Li=s(Ne);Va=f(Li,"The CARLA simulation environment was instrumental in accelerating our development cycle, providing a versatile platform for testing, debugging, and data generation. I’m proud of the final results and the integral role I played in Paradigm’s success in the competition."),Li.forEach(t),Lt=m(e),je=o(e,"P",{});var Di=s(je);Sa=f(Di,"Stay tuned for more exciting projects and technical insights in my future posts! Thanks for reading."),Di.forEach(t),Dt=m(e),He=o(e,"P",{});var zi=s(He);g(ee.$$.fragment,zi),zi.forEach(t),zt=m(e),te=o(e,"H4",{id:!0});var Ni=s(te);Ue=o(Ni,"A",{href:!0});var ji=s(Ue);Ra=f(ji,"References"),ji.forEach(t),Ni.forEach(t),Nt=m(e),Fe=o(e,"P",{});var Hi=s(Fe);ka=f(Hi,"[1] B. Zhou and P. Krähenbühl, “Cross-view Transformers for real-time Map-view Semantic Segmentation,” Computer Vision and Pattern Recognition, 2022."),Hi.forEach(t),jt=m(e),qe=o(e,"P",{});var Ui=s(qe);Ga=f(Ui,"[2] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez and V. Koltun, “CARLA: An Open Urban Driving Simulator,” Conference on Robot Learning, 2017."),Ui.forEach(t),Ht=m(e),Ke=o(e,"P",{});var Fi=s(Ke);Oa=f(Fi,"[3] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Lion, Q. Xu, A. Krishnan, Y. Pan, G. Baldan and O. Beijbom, “nuScenes: A multimodal dataset for autonomous driving,” Computer Vision and Pattern Recognition, 2019."),Fi.forEach(t),Ut=m(e),Ze=o(e,"P",{});var qi=s(Ze);Ba=f(qi,"Disclaimer: Please note that some details have been simplified for the sake of readability."),qi.forEach(t),this.h()},h(){p(h,"href","#a-deep-dive-into-my-autonomous-vehicle-software-design-journey"),p(u,"id","a-deep-dive-into-my-autonomous-vehicle-software-design-journey"),p(ne,"href","#igvc-auto-nav-challenge"),p(V,"id","igvc-auto-nav-challenge"),p(re,"href","#vehicle"),p(R,"id","vehicle"),p(de,"href","#software-overview"),p(G,"id","software-overview"),p(ce,"href","#vision-based-obstacle-detection-and-avoidance"),p(B,"id","vision-based-obstacle-detection-and-avoidance"),p(ye,"href","#obstacle-avoidance"),p(x,"id","obstacle-avoidance"),p(be,"href","#mapping--path-planning"),p(z,"id","mapping--path-planning"),p($e,"href","#localization"),p(H,"id","localization"),p(Ie,"href","#simulation"),p(U,"id","simulation"),p(Ve,"href","#model-training-from-simulator-data"),p(K,"id","model-training-from-simulator-data"),p(Ge,"href","#compute-hardware"),p(W,"id","compute-hardware"),p(xe,"href","#gpt-for-fundraising"),p(Q,"id","gpt-for-fundraising"),p(De,"href","#conclusion"),p(Y,"id","conclusion"),p(Ue,"href","#references"),p(te,"id","references")},m(e,a){i(e,u,a),r(u,h),r(h,E),i(e,T,a),i(e,c,a),r(c,$),i(e,A,a),i(e,ie,a),y(C,ie,null),i(e,We,a),i(e,V,a),r(V,ne),r(ne,qt),i(e,Xe,a),i(e,oe,a),r(oe,Kt),i(e,Qe,a),i(e,se,a),y(S,se,null),i(e,Ye,a),i(e,R,a),r(R,re),r(re,Zt),i(e,et,a),i(e,le,a),r(le,Jt),i(e,tt,a),i(e,me,a),y(k,me,null),i(e,at,a),i(e,G,a),r(G,de),r(de,Wt),i(e,it,a),i(e,fe,a),r(fe,Xt),i(e,nt,a),i(e,pe,a),y(O,pe,null),i(e,ot,a),i(e,ue,a),r(ue,Qt),i(e,st,a),i(e,B,a),r(B,ce),r(ce,Yt),i(e,rt,a),i(e,he,a),r(he,ea),i(e,lt,a),i(e,ve,a),y(M,ve,null),i(e,mt,a),i(e,ge,a),r(ge,ta),i(e,dt,a),i(e,x,a),r(x,ye),r(ye,aa),i(e,ft,a),i(e,we,a),r(we,ia),i(e,pt,a),i(e,I,a),r(I,na),y(L,I,null),r(I,oa),y(D,I,null),i(e,ut,a),i(e,z,a),r(z,be),r(be,sa),i(e,ct,a),i(e,_e,a),r(_e,ra),i(e,ht,a),i(e,N,a),r(N,la),y(j,N,null),i(e,vt,a),i(e,Pe,a),r(Pe,ma),i(e,gt,a),i(e,H,a),r(H,$e),r($e,da),i(e,yt,a),i(e,Ee,a),r(Ee,fa),i(e,wt,a),i(e,U,a),r(U,Ie),r(Ie,pa),i(e,bt,a),i(e,Te,a),r(Te,ua),i(e,_t,a),i(e,F,a),r(F,ca),y(q,F,null),i(e,Pt,a),i(e,Ae,a),r(Ae,ha),i(e,$t,a),i(e,Ce,a),r(Ce,va),i(e,Et,a),i(e,K,a),r(K,Ve),r(Ve,ga),i(e,It,a),i(e,Se,a),r(Se,ya),i(e,Tt,a),i(e,Re,a),r(Re,wa),i(e,At,a),i(e,Z,a),r(Z,ba),y(J,Z,null),i(e,Ct,a),i(e,ke,a),r(ke,_a),i(e,Vt,a),i(e,W,a),r(W,Ge),r(Ge,Pa),i(e,St,a),i(e,Oe,a),r(Oe,$a),i(e,Rt,a),i(e,Be,a),y(X,Be,null),i(e,kt,a),i(e,Me,a),r(Me,Ea),i(e,Gt,a),i(e,Q,a),r(Q,xe),r(xe,Ia),i(e,Ot,a),i(e,Le,a),r(Le,Ta),i(e,Bt,a),i(e,Y,a),r(Y,De),r(De,Aa),i(e,Mt,a),i(e,ze,a),r(ze,Ca),i(e,xt,a),i(e,Ne,a),r(Ne,Va),i(e,Lt,a),i(e,je,a),r(je,Sa),i(e,Dt,a),i(e,He,a),y(ee,He,null),i(e,zt,a),i(e,te,a),r(te,Ue),r(Ue,Ra),i(e,Nt,a),i(e,Fe,a),r(Fe,ka),i(e,jt,a),i(e,qe,a),r(qe,Ga),i(e,Ht,a),i(e,Ke,a),r(Ke,Oa),i(e,Ut,a),i(e,Ze,a),r(Ze,Ba),Ft=!0},p:en,i(e){Ft||(w(C.$$.fragment,e),w(S.$$.fragment,e),w(k.$$.fragment,e),w(O.$$.fragment,e),w(M.$$.fragment,e),w(L.$$.fragment,e),w(D.$$.fragment,e),w(j.$$.fragment,e),w(q.$$.fragment,e),w(J.$$.fragment,e),w(X.$$.fragment,e),w(ee.$$.fragment,e),Ft=!0)},o(e){b(C.$$.fragment,e),b(S.$$.fragment,e),b(k.$$.fragment,e),b(O.$$.fragment,e),b(M.$$.fragment,e),b(L.$$.fragment,e),b(D.$$.fragment,e),b(j.$$.fragment,e),b(q.$$.fragment,e),b(J.$$.fragment,e),b(X.$$.fragment,e),b(ee.$$.fragment,e),Ft=!1},d(e){e&&t(u),e&&t(T),e&&t(c),e&&t(A),e&&t(ie),_(C),e&&t(We),e&&t(V),e&&t(Xe),e&&t(oe),e&&t(Qe),e&&t(se),_(S),e&&t(Ye),e&&t(R),e&&t(et),e&&t(le),e&&t(tt),e&&t(me),_(k),e&&t(at),e&&t(G),e&&t(it),e&&t(fe),e&&t(nt),e&&t(pe),_(O),e&&t(ot),e&&t(ue),e&&t(st),e&&t(B),e&&t(rt),e&&t(he),e&&t(lt),e&&t(ve),_(M),e&&t(mt),e&&t(ge),e&&t(dt),e&&t(x),e&&t(ft),e&&t(we),e&&t(pt),e&&t(I),_(L),_(D),e&&t(ut),e&&t(z),e&&t(ct),e&&t(_e),e&&t(ht),e&&t(N),_(j),e&&t(vt),e&&t(Pe),e&&t(gt),e&&t(H),e&&t(yt),e&&t(Ee),e&&t(wt),e&&t(U),e&&t(bt),e&&t(Te),e&&t(_t),e&&t(F),_(q),e&&t(Pt),e&&t(Ae),e&&t($t),e&&t(Ce),e&&t(Et),e&&t(K),e&&t(It),e&&t(Se),e&&t(Tt),e&&t(Re),e&&t(At),e&&t(Z),_(J),e&&t(Ct),e&&t(ke),e&&t(Vt),e&&t(W),e&&t(St),e&&t(Oe),e&&t(Rt),e&&t(Be),_(X),e&&t(kt),e&&t(Me),e&&t(Gt),e&&t(Q),e&&t(Ot),e&&t(Le),e&&t(Bt),e&&t(Y),e&&t(Mt),e&&t(ze),e&&t(xt),e&&t(Ne),e&&t(Lt),e&&t(je),e&&t(Dt),e&&t(He),_(ee),e&&t(zt),e&&t(te),e&&t(Nt),e&&t(Fe),e&&t(jt),e&&t(qe),e&&t(Ht),e&&t(Ke),e&&t(Ut),e&&t(Ze)}}}function nn(ae){let u,h;const E=[ae[0],Ji];let T={$$slots:{default:[an]},$$scope:{ctx:ae}};for(let c=0;c<E.length;c+=1)T=Da(T,E[c]);return u=new tn({props:T}),{c(){v(u.$$.fragment)},l(c){g(u.$$.fragment,c)},m(c,$){y(u,c,$),h=!0},p(c,[$]){const A=$&1?Yi(E,[$&1&&Ki(c[0]),$&0&&Ki(Ji)]):{};$&2&&(A.$$scope={dirty:$,ctx:c}),u.$set(A)},i(c){h||(w(u.$$.fragment,c),h=!0)},o(c){b(u.$$.fragment,c),h=!1},d(c){_(u,c)}}}const Ji={title:"Intelligent Ground Vehicle Competition",image:"/igvc/team.jpg",alt:"IGVC",created:"2023-06-11T00:00:00.000Z",tags:["Computer Vision","Transformer"],updated:"2023-06-14T15:17:10.756Z",images:[],slug:"/igvc/+page.svelte.md",path:"/igvc",toc:[{depth:1,title:"A Deep Dive into My Autonomous Vehicle Software Design Journey",slug:"a-deep-dive-into-my-autonomous-vehicle-software-design-journey"},{depth:2,title:"IGVC Auto-Nav Challenge",slug:"igvc-auto-nav-challenge"},{depth:2,title:"Vehicle",slug:"vehicle"},{depth:2,title:"Software Overview",slug:"software-overview"},{depth:3,title:"Vision-based Obstacle Detection and Avoidance",slug:"vision-based-obstacle-detection-and-avoidance"},{depth:4,title:"Obstacle Avoidance",slug:"obstacle-avoidance"},{depth:3,title:"Mapping & Path Planning",slug:"mapping--path-planning"},{depth:3,title:"Localization",slug:"localization"},{depth:2,title:"Simulation",slug:"simulation"},{depth:3,title:"Model Training from Simulator Data",slug:"model-training-from-simulator-data"},{depth:2,title:"Compute Hardware",slug:"compute-hardware"},{depth:2,title:"GPT for Fundraising",slug:"gpt-for-fundraising"},{depth:2,title:"Conclusion",slug:"conclusion"},{depth:4,title:"References",slug:"references"}]};function on(ae,u,h){return ae.$$set=E=>{h(0,u=Da(Da({},u),Zi(E)))},u=Zi(u),[u]}class mn extends Wi{constructor(u){super(),Xi(this,u,on,nn,Qi,{})}}export{mn as component};
