import{S as sr,i as lr,s as cr,N as dr,k as r,l as o,m as n,h as t,n as u,b as s,D as i,O as pr,P as hr,Q as mr,g as Ke,d as Ye,U as oi,y as pt,z as ht,A as mt,V as vr,W as rr,B as vt,X as or,q as f,a as l,r as d,c,E as fr}from"../chunks/index.17f79c6b.js";import{P as ur}from"../chunks/post_layout.3a62565a.js";import{I as ri}from"../chunks/footer.8df9f7af.js";function Er(q){let p,m,w;const H=q[1].default,h=dr(H,q,q[0],null);return{c(){p=r("div"),m=r("table"),h&&h.c(),this.h()},l(v){p=o(v,"DIV",{class:!0});var $=n(p);m=o($,"TABLE",{class:!0});var F=n(m);h&&h.l(F),F.forEach(t),$.forEach(t),this.h()},h(){u(m,"class","table w-full"),u(p,"class","overflow-x-auto mb-4")},m(v,$){s(v,p,$),i(p,m),h&&h.m(m,null),w=!0},p(v,[$]){h&&h.p&&(!w||$&1)&&pr(h,H,v,v[0],w?mr(H,v[0],$,null):hr(v[0]),null)},i(v){w||(Ke(h,v),w=!0)},o(v){Ye(h,v),w=!1},d(v){v&&t(p),h&&h.d(v)}}}function wr(q,p,m){let{$$slots:w={},$$scope:H}=p;return q.$$set=h=>{"$$scope"in h&&m(0,H=h.$$scope)},[H,w]}class nr extends sr{constructor(p){super(),lr(this,p,wr,Er,cr,{})}}function gr(q){let p,m,w,H,h,v,$,F,D,we,ne,x,ge,Y,b,g,z,ae,Z,U,se,Q,N,le,X,k,ce,S,_,j,fe,K,L,_e,de,R,be,pe,C,J,ye,y,B,Te,he,I,ee,He,O,te,Ve,G,ie;return{c(){p=r("thead"),m=r("tr"),w=r("th"),H=f("Environment"),h=l(),v=r("th"),$=f("World-Centric"),F=l(),D=r("th"),we=f("Ego-Centric"),ne=l(),x=r("th"),ge=f("Ego-Forward"),Y=l(),b=r("tbody"),g=r("tr"),z=r("td"),ae=f("Collaborative Push Block"),Z=l(),U=r("td"),se=f("96.94%"),Q=l(),N=r("td"),le=f("63.87%"),X=l(),k=r("td"),ce=f("64.22%"),S=l(),_=r("tr"),j=r("td"),fe=f("Dungeon Escape"),K=l(),L=r("td"),_e=f("43.53%"),de=l(),R=r("td"),be=f("1.82%"),pe=l(),C=r("td"),J=f("26.07%"),ye=l(),y=r("tr"),B=r("td"),Te=f("Planar Construction"),he=l(),I=r("td"),ee=f("48.37%"),He=l(),O=r("td"),te=f("35.45%"),Ve=l(),G=r("td"),ie=f("10.16%")},l(E){p=o(E,"THEAD",{});var W=n(p);m=o(W,"TR",{});var T=n(m);w=o(T,"TH",{});var me=n(w);H=d(me,"Environment"),me.forEach(t),h=c(T),v=o(T,"TH",{});var Ce=n(v);$=d(Ce,"World-Centric"),Ce.forEach(t),F=c(T),D=o(T,"TH",{});var Ae=n(D);we=d(Ae,"Ego-Centric"),Ae.forEach(t),ne=c(T),x=o(T,"TH",{});var ve=n(x);ge=d(ve,"Ego-Forward"),ve.forEach(t),T.forEach(t),W.forEach(t),Y=c(E),b=o(E,"TBODY",{});var M=n(b);g=o(M,"TR",{});var A=n(g);z=o(A,"TD",{});var re=n(z);ae=d(re,"Collaborative Push Block"),re.forEach(t),Z=c(A),U=o(A,"TD",{});var $e=n(U);se=d($e,"96.94%"),$e.forEach(t),Q=c(A),N=o(A,"TD",{});var Ie=n(N);le=d(Ie,"63.87%"),Ie.forEach(t),X=c(A),k=o(A,"TD",{});var De=n(k);ce=d(De,"64.22%"),De.forEach(t),A.forEach(t),S=c(M),_=o(M,"TR",{});var V=n(_);j=o(V,"TD",{});var xe=n(j);fe=d(xe,"Dungeon Escape"),xe.forEach(t),K=c(V),L=o(V,"TD",{});var Pe=n(L);_e=d(Pe,"43.53%"),Pe.forEach(t),de=c(V),R=o(V,"TD",{});var ue=n(R);be=d(ue,"1.82%"),ue.forEach(t),pe=c(V),C=o(V,"TD",{});var Le=n(C);J=d(Le,"26.07%"),Le.forEach(t),V.forEach(t),ye=c(M),y=o(M,"TR",{});var P=n(y);B=o(P,"TD",{});var oe=n(B);Te=d(oe,"Planar Construction"),oe.forEach(t),he=c(P),I=o(P,"TD",{});var Ee=n(I);ee=d(Ee,"48.37%"),Ee.forEach(t),He=c(P),O=o(P,"TD",{});var Re=n(O);te=d(Re,"35.45%"),Re.forEach(t),Ve=c(P),G=o(P,"TD",{});var ke=n(G);ie=d(ke,"10.16%"),ke.forEach(t),P.forEach(t),M.forEach(t)},m(E,W){s(E,p,W),i(p,m),i(m,w),i(w,H),i(m,h),i(m,v),i(v,$),i(m,F),i(m,D),i(D,we),i(m,ne),i(m,x),i(x,ge),s(E,Y,W),s(E,b,W),i(b,g),i(g,z),i(z,ae),i(g,Z),i(g,U),i(U,se),i(g,Q),i(g,N),i(N,le),i(g,X),i(g,k),i(k,ce),i(b,S),i(b,_),i(_,j),i(j,fe),i(_,K),i(_,L),i(L,_e),i(_,de),i(_,R),i(R,be),i(_,pe),i(_,C),i(C,J),i(b,ye),i(b,y),i(y,B),i(B,Te),i(y,he),i(y,I),i(I,ee),i(y,He),i(y,O),i(O,te),i(y,Ve),i(y,G),i(G,ie)},p:fr,d(E){E&&t(p),E&&t(Y),E&&t(b)}}}function _r(q){let p,m,w,H,h,v,$,F,D,we,ne,x,ge,Y,b,g,z,ae,Z,U,se,Q,N,le,X,k,ce,S,_,j,fe,K,L,_e,de,R,be,pe,C,J,ye,y,B,Te,he,I,ee,He,O,te,Ve,G,ie;return{c(){p=r("thead"),m=r("tr"),w=r("th"),H=f("Environment"),h=l(),v=r("th"),$=f("World-Centric"),F=l(),D=r("th"),we=f("Ego-Centric"),ne=l(),x=r("th"),ge=f("Ego-Forward"),Y=l(),b=r("tbody"),g=r("tr"),z=r("td"),ae=f("Collaborative Push Block"),Z=l(),U=r("td"),se=f("222.6"),Q=l(),N=r("td"),le=f("246.6"),X=l(),k=r("td"),ce=f("230.5"),S=l(),_=r("tr"),j=r("td"),fe=f("Dungeon Escape"),K=l(),L=r("td"),_e=f("14.15"),de=l(),R=r("td"),be=f("16.74"),pe=l(),C=r("td"),J=f("23.27"),ye=l(),y=r("tr"),B=r("td"),Te=f("Planar Construction"),he=l(),I=r("td"),ee=f("110.5"),He=l(),O=r("td"),te=f("120.3"),Ve=l(),G=r("td"),ie=f("149.9")},l(E){p=o(E,"THEAD",{});var W=n(p);m=o(W,"TR",{});var T=n(m);w=o(T,"TH",{});var me=n(w);H=d(me,"Environment"),me.forEach(t),h=c(T),v=o(T,"TH",{});var Ce=n(v);$=d(Ce,"World-Centric"),Ce.forEach(t),F=c(T),D=o(T,"TH",{});var Ae=n(D);we=d(Ae,"Ego-Centric"),Ae.forEach(t),ne=c(T),x=o(T,"TH",{});var ve=n(x);ge=d(ve,"Ego-Forward"),ve.forEach(t),T.forEach(t),W.forEach(t),Y=c(E),b=o(E,"TBODY",{});var M=n(b);g=o(M,"TR",{});var A=n(g);z=o(A,"TD",{});var re=n(z);ae=d(re,"Collaborative Push Block"),re.forEach(t),Z=c(A),U=o(A,"TD",{});var $e=n(U);se=d($e,"222.6"),$e.forEach(t),Q=c(A),N=o(A,"TD",{});var Ie=n(N);le=d(Ie,"246.6"),Ie.forEach(t),X=c(A),k=o(A,"TD",{});var De=n(k);ce=d(De,"230.5"),De.forEach(t),A.forEach(t),S=c(M),_=o(M,"TR",{});var V=n(_);j=o(V,"TD",{});var xe=n(j);fe=d(xe,"Dungeon Escape"),xe.forEach(t),K=c(V),L=o(V,"TD",{});var Pe=n(L);_e=d(Pe,"14.15"),Pe.forEach(t),de=c(V),R=o(V,"TD",{});var ue=n(R);be=d(ue,"16.74"),ue.forEach(t),pe=c(V),C=o(V,"TD",{});var Le=n(C);J=d(Le,"23.27"),Le.forEach(t),V.forEach(t),ye=c(M),y=o(M,"TR",{});var P=n(y);B=o(P,"TD",{});var oe=n(B);Te=d(oe,"Planar Construction"),oe.forEach(t),he=c(P),I=o(P,"TD",{});var Ee=n(I);ee=d(Ee,"110.5"),Ee.forEach(t),He=c(P),O=o(P,"TD",{});var Re=n(O);te=d(Re,"120.3"),Re.forEach(t),Ve=c(P),G=o(P,"TD",{});var ke=n(G);ie=d(ke,"149.9"),ke.forEach(t),P.forEach(t),M.forEach(t)},m(E,W){s(E,p,W),i(p,m),i(m,w),i(w,H),i(m,h),i(m,v),i(v,$),i(m,F),i(m,D),i(D,we),i(m,ne),i(m,x),i(x,ge),s(E,Y,W),s(E,b,W),i(b,g),i(g,z),i(z,ae),i(g,Z),i(g,U),i(U,se),i(g,Q),i(g,N),i(N,le),i(g,X),i(g,k),i(k,ce),i(b,S),i(b,_),i(_,j),i(j,fe),i(_,K),i(_,L),i(L,_e),i(_,de),i(_,R),i(R,be),i(_,pe),i(_,C),i(C,J),i(b,ye),i(b,y),i(y,B),i(B,Te),i(y,he),i(y,I),i(I,ee),i(y,He),i(y,O),i(O,te),i(y,Ve),i(y,G),i(G,ie)},p:fr,d(E){E&&t(p),E&&t(Y),E&&t(b)}}}function br(q){let p,m,w,H,h,v,$,F,D,we,ne,x,ge,Y,b,g,z,ae,Z,U,se,Q,N,le,X,k,ce,S,_,j,fe,K,L,_e,de,R,be,pe,C,J,ye,y,B,Te,he,I,ee,He,O,te,Ve,G,ie,E,W,T,me,Ce,Ae,ve,M,A,re,$e,Ie,De,V,xe,Pe,ue,Le,P,oe,Ee,Re,ke,Oe,Ze,Mt,wt,Qe,Ot,gt,Ge,Xe,Gt,_t,Je,Ft,bt,Fe,et,zt,yt,tt,Ut,Tt,it,ze,Ht,Ue,rt,Nt,Vt,Ne,ot,St,$t,nt,jt,At,at,Kt,Pt,Be,kt,Se,st,Yt,Ct,lt,Zt,It,ct,Qt,Dt,We,xt,je,ft,Xt,Lt,dt,Jt,Rt,qe,ei,Me,ti,ii,Bt;return k=new ri({props:{src:"./HEVMethod.png",alt:"HEV"}}),E=new ri({props:{src:"./POVFigure.png",alt:"POV"}}),ze=new ri({props:{src:"./envs.PNG",alt:"envs"}}),Be=new nr({props:{$$slots:{default:[gr]},$$scope:{ctx:q}}}),We=new nr({props:{$$slots:{default:[_r]},$$scope:{ctx:q}}}),{c(){p=r("h1"),m=r("a"),w=f("Herd’s Eye View: Improving Game AI Agent Learning with Collaborative Perception"),H=l(),h=r("h2"),v=r("a"),$=f("Introduction"),F=l(),D=r("p"),we=f("In the realm of game AI, agents traditionally have access to extensive global information from the game engine. While this configuration assists in efficient decision-making, it doesn’t accurately represent the restrictions encountered by AI applications outside of gaming. Game AI techniques that rely predominantly on game engine data may limit their potential contribution to broader AI applications."),ne=l(),x=r("p"),ge=f("To address these challenges, we present a novel paradigm named Herd’s Eye View (HEV) that adopts a global perspective derived from multiple agents to boost decision-making capabilities of reinforcement learning (RL) agents in multi-agent environments. The HEV approach utilizes cooperative perception to empower RL agents with a global reasoning ability, enhancing their decision-making."),Y=l(),b=r("h2"),g=r("a"),z=f("Herd’s Eye View: A New Perspective"),ae=l(),Z=r("p"),U=f("Drawing inspiration from autonomous vehicles research and the Bird’s Eye View (BEV) model, we propose a similar intermediary representation for game AI: the Herd’s Eye View (HEV) model. Unlike the BEV’s individual-centric perspective, the HEV model offers a shared world-centric perception derived from multiple agents. This shared perception model aligns closer to real-world AI applications, where multiple systems often work together to understand and navigate their environment."),se=l(),Q=r("p"),N=f("The HEV model presents dual advantages. First, it mirrors the constraints faced by AI outside of gaming, contributing to the development of more believable AI behavior in games. Second, it alleviates the computational demands associated with the BEV model, where each agent maintains its own unique view of the environment. Instead, only a single shared global view is utilized."),le=l(),X=r("p"),pt(k.$$.fragment),ce=l(),S=r("h2"),_=r("a"),j=f("Perspective Views"),fe=l(),K=r("h3"),L=r("a"),_e=f("World-Centric (Herd’s Eye View)"),de=l(),R=r("p"),be=f("The world-centric view, also known as the Herd’s Eye View (HEV), offers a shared global perception derived from multiple agents. This view allows all agents to have a comprehensive understanding of the entire environment, which can enhance their decision-making capabilities and coordination."),pe=l(),C=r("h3"),J=r("a"),ye=f("Ego-Centric"),y=l(),B=r("p"),Te=f("The ego-centric view is an individual-centric perspective where each agent maintains its own unique view of the environment. This view can limit the agents’ understanding of the environment to their immediate surroundings."),he=l(),I=r("h3"),ee=r("a"),He=f("Ego-Forward"),O=l(),te=r("p"),Ve=f("The ego-forward view is a variation of the ego-centric view where the perspective is always forward-facing relative to the agent. This view can be limiting as it restricts the agents’ perception to only what is in front of them."),G=l(),ie=r("p"),pt(E.$$.fragment),W=l(),T=r("h2"),me=r("a"),Ce=f("Key Contributions"),Ae=l(),ve=r("p"),M=f("Our work makes the following contributions:"),A=l(),re=r("ol"),$e=r("li"),Ie=f("We propose a baseline model for performing semantic segmentation in a fixed ”HEV” world-view."),De=l(),V=r("li"),xe=f("We demonstrate the effectiveness of the HEV fixed world viewpoint in improving collaborative perception and MARL in games."),Pe=l(),ue=r("p"),Le=f("Our exploration of more realistic perception models and the application of reinforcement learning provides```markdown\nsignificant insights for game AI development, stressing the wider applicability of these techniques beyond the gaming industry."),P=l(),oe=r("h2"),Ee=r("a"),Re=f("Environments"),ke=l(),Oe=r("h3"),Ze=r("a"),Mt=f("Collaborative Push Block"),wt=l(),Qe=r("p"),Ot=f("In this environment, agents are required to push white blocks to a designated green area. The challenge here is that larger blocks require more agents to push, necessitating cooperation and coordination among the agents."),gt=l(),Ge=r("h3"),Xe=r("a"),Gt=f("Dungeon Escape"),_t=l(),Je=r("p"),Ft=f("This environment presents a unique challenge where agents must sacrifice one of their own to take down a green dragon and obtain a key. The rest of the team must then use this key to escape the dungeon. This scenario tests the agents’ ability to strategize and make sacrifices for the greater good."),bt=l(),Fe=r("h3"),et=r("a"),zt=f("Planar Construction"),yt=l(),tt=r("p"),Ut=f("In the Planar Construction environment, six agents collaborate to push red pucks into desired positions. The positions can be random or static, and are observed via a Grid-Sensor, similar to the push block environment. This environment tests the agents’ ability to work together in a more complex and dynamic setting."),Tt=l(),it=r("p"),pt(ze.$$.fragment),Ht=l(),Ue=r("h2"),rt=r("a"),Nt=f("Experiments"),Vt=l(),Ne=r("h3"),ot=r("a"),St=f("Accuracy of World-Centric Predictions vs Ego-Centric"),$t=l(),nt=r("p"),jt=f("The first experiment compares the accuracy of world-centric predictions using HEV with ego-centric predictions using BEV. We conduct this experiment in three simulated Multi-Agent Reinforcement Learning (MARL) game environments."),At=l(),at=r("p"),Kt=f("HEV-CVT validation IoU results per coordinate frame in each environment (higher is better)"),Pt=l(),pt(Be.$$.fragment),kt=l(),Se=r("h3"),st=r("a"),Yt=f("Efficiency of Perspective View Policies Learned"),Ct=l(),lt=r("p"),Zt=f("The second experiment evaluates the efficiency of policies learned by RL agents trained on HEV views compared to those trained on BEV views."),It=l(),ct=r("p"),Qt=f("Table 2: MA-POCA mean episode length per coordinate frame in each environment (lower is better)"),Dt=l(),pt(We.$$.fragment),xt=l(),je=r("h2"),ft=r("a"),Xt=f("Conclusion"),Lt=l(),dt=r("p"),Jt=f("The Herd’s Eye View (HEV) framework offers a superior perception model in MARL environments, providing agents with a more comprehensive understanding of their surroundings, leading to improved decision-making and better overall performance. By using the HEV world-view collaborative perception problem, we show that the accuracy of collaborative perception models is significantly improved, leading to better IoU scores. Our work opens up new possibilities for advanced perception models in MARL, which can greatly enhance the performance of multi-robot systems by enabling better collaboration and coordination."),Rt=l(),qe=r("p"),ei=f("The code for this project is available on "),Me=r("a"),ti=f("GitHub"),ii=f("."),this.h()},l(e){p=o(e,"H1",{id:!0});var a=n(p);m=o(a,"A",{href:!0});var ut=n(m);w=d(ut,"Herd’s Eye View: Improving Game AI Agent Learning with Collaborative Perception"),ut.forEach(t),a.forEach(t),H=c(e),h=o(e,"H2",{id:!0});var Et=n(h);v=o(Et,"A",{href:!0});var ni=n(v);$=d(ni,"Introduction"),ni.forEach(t),Et.forEach(t),F=c(e),D=o(e,"P",{});var ai=n(D);we=d(ai,"In the realm of game AI, agents traditionally have access to extensive global information from the game engine. While this configuration assists in efficient decision-making, it doesn’t accurately represent the restrictions encountered by AI applications outside of gaming. Game AI techniques that rely predominantly on game engine data may limit their potential contribution to broader AI applications."),ai.forEach(t),ne=c(e),x=o(e,"P",{});var si=n(x);ge=d(si,"To address these challenges, we present a novel paradigm named Herd’s Eye View (HEV) that adopts a global perspective derived from multiple agents to boost decision-making capabilities of reinforcement learning (RL) agents in multi-agent environments. The HEV approach utilizes cooperative perception to empower RL agents with a global reasoning ability, enhancing their decision-making."),si.forEach(t),Y=c(e),b=o(e,"H2",{id:!0});var li=n(b);g=o(li,"A",{href:!0});var ci=n(g);z=d(ci,"Herd’s Eye View: A New Perspective"),ci.forEach(t),li.forEach(t),ae=c(e),Z=o(e,"P",{});var fi=n(Z);U=d(fi,"Drawing inspiration from autonomous vehicles research and the Bird’s Eye View (BEV) model, we propose a similar intermediary representation for game AI: the Herd’s Eye View (HEV) model. Unlike the BEV’s individual-centric perspective, the HEV model offers a shared world-centric perception derived from multiple agents. This shared perception model aligns closer to real-world AI applications, where multiple systems often work together to understand and navigate their environment."),fi.forEach(t),se=c(e),Q=o(e,"P",{});var di=n(Q);N=d(di,"The HEV model presents dual advantages. First, it mirrors the constraints faced by AI outside of gaming, contributing to the development of more believable AI behavior in games. Second, it alleviates the computational demands associated with the BEV model, where each agent maintains its own unique view of the environment. Instead, only a single shared global view is utilized."),di.forEach(t),le=c(e),X=o(e,"P",{});var pi=n(X);ht(k.$$.fragment,pi),pi.forEach(t),ce=c(e),S=o(e,"H2",{id:!0});var hi=n(S);_=o(hi,"A",{href:!0});var mi=n(_);j=d(mi,"Perspective Views"),mi.forEach(t),hi.forEach(t),fe=c(e),K=o(e,"H3",{id:!0});var vi=n(K);L=o(vi,"A",{href:!0});var ui=n(L);_e=d(ui,"World-Centric (Herd’s Eye View)"),ui.forEach(t),vi.forEach(t),de=c(e),R=o(e,"P",{});var Ei=n(R);be=d(Ei,"The world-centric view, also known as the Herd’s Eye View (HEV), offers a shared global perception derived from multiple agents. This view allows all agents to have a comprehensive understanding of the entire environment, which can enhance their decision-making capabilities and coordination."),Ei.forEach(t),pe=c(e),C=o(e,"H3",{id:!0});var wi=n(C);J=o(wi,"A",{href:!0});var gi=n(J);ye=d(gi,"Ego-Centric"),gi.forEach(t),wi.forEach(t),y=c(e),B=o(e,"P",{});var _i=n(B);Te=d(_i,"The ego-centric view is an individual-centric perspective where each agent maintains its own unique view of the environment. This view can limit the agents’ understanding of the environment to their immediate surroundings."),_i.forEach(t),he=c(e),I=o(e,"H3",{id:!0});var bi=n(I);ee=o(bi,"A",{href:!0});var yi=n(ee);He=d(yi,"Ego-Forward"),yi.forEach(t),bi.forEach(t),O=c(e),te=o(e,"P",{});var Ti=n(te);Ve=d(Ti,"The ego-forward view is a variation of the ego-centric view where the perspective is always forward-facing relative to the agent. This view can be limiting as it restricts the agents’ perception to only what is in front of them."),Ti.forEach(t),G=c(e),ie=o(e,"P",{});var Hi=n(ie);ht(E.$$.fragment,Hi),Hi.forEach(t),W=c(e),T=o(e,"H2",{id:!0});var Vi=n(T);me=o(Vi,"A",{href:!0});var $i=n(me);Ce=d($i,"Key Contributions"),$i.forEach(t),Vi.forEach(t),Ae=c(e),ve=o(e,"P",{});var Ai=n(ve);M=d(Ai,"Our work makes the following contributions:"),Ai.forEach(t),A=c(e),re=o(e,"OL",{});var Wt=n(re);$e=o(Wt,"LI",{});var Pi=n($e);Ie=d(Pi,"We propose a baseline model for performing semantic segmentation in a fixed ”HEV” world-view."),Pi.forEach(t),De=c(Wt),V=o(Wt,"LI",{});var ki=n(V);xe=d(ki,"We demonstrate the effectiveness of the HEV fixed world viewpoint in improving collaborative perception and MARL in games."),ki.forEach(t),Wt.forEach(t),Pe=c(e),ue=o(e,"P",{});var Ci=n(ue);Le=d(Ci,"Our exploration of more realistic perception models and the application of reinforcement learning provides```markdown\nsignificant insights for game AI development, stressing the wider applicability of these techniques beyond the gaming industry."),Ci.forEach(t),P=c(e),oe=o(e,"H2",{id:!0});var Ii=n(oe);Ee=o(Ii,"A",{href:!0});var Di=n(Ee);Re=d(Di,"Environments"),Di.forEach(t),Ii.forEach(t),ke=c(e),Oe=o(e,"H3",{id:!0});var xi=n(Oe);Ze=o(xi,"A",{href:!0});var Li=n(Ze);Mt=d(Li,"Collaborative Push Block"),Li.forEach(t),xi.forEach(t),wt=c(e),Qe=o(e,"P",{});var Ri=n(Qe);Ot=d(Ri,"In this environment, agents are required to push white blocks to a designated green area. The challenge here is that larger blocks require more agents to push, necessitating cooperation and coordination among the agents."),Ri.forEach(t),gt=c(e),Ge=o(e,"H3",{id:!0});var Bi=n(Ge);Xe=o(Bi,"A",{href:!0});var Wi=n(Xe);Gt=d(Wi,"Dungeon Escape"),Wi.forEach(t),Bi.forEach(t),_t=c(e),Je=o(e,"P",{});var qi=n(Je);Ft=d(qi,"This environment presents a unique challenge where agents must sacrifice one of their own to take down a green dragon and obtain a key. The rest of the team must then use this key to escape the dungeon. This scenario tests the agents’ ability to strategize and make sacrifices for the greater good."),qi.forEach(t),bt=c(e),Fe=o(e,"H3",{id:!0});var Mi=n(Fe);et=o(Mi,"A",{href:!0});var Oi=n(et);zt=d(Oi,"Planar Construction"),Oi.forEach(t),Mi.forEach(t),yt=c(e),tt=o(e,"P",{});var Gi=n(tt);Ut=d(Gi,"In the Planar Construction environment, six agents collaborate to push red pucks into desired positions. The positions can be random or static, and are observed via a Grid-Sensor, similar to the push block environment. This environment tests the agents’ ability to work together in a more complex and dynamic setting."),Gi.forEach(t),Tt=c(e),it=o(e,"P",{});var Fi=n(it);ht(ze.$$.fragment,Fi),Fi.forEach(t),Ht=c(e),Ue=o(e,"H2",{id:!0});var zi=n(Ue);rt=o(zi,"A",{href:!0});var Ui=n(rt);Nt=d(Ui,"Experiments"),Ui.forEach(t),zi.forEach(t),Vt=c(e),Ne=o(e,"H3",{id:!0});var Ni=n(Ne);ot=o(Ni,"A",{href:!0});var Si=n(ot);St=d(Si,"Accuracy of World-Centric Predictions vs Ego-Centric"),Si.forEach(t),Ni.forEach(t),$t=c(e),nt=o(e,"P",{});var ji=n(nt);jt=d(ji,"The first experiment compares the accuracy of world-centric predictions using HEV with ego-centric predictions using BEV. We conduct this experiment in three simulated Multi-Agent Reinforcement Learning (MARL) game environments."),ji.forEach(t),At=c(e),at=o(e,"P",{});var Ki=n(at);Kt=d(Ki,"HEV-CVT validation IoU results per coordinate frame in each environment (higher is better)"),Ki.forEach(t),Pt=c(e),ht(Be.$$.fragment,e),kt=c(e),Se=o(e,"H3",{id:!0});var Yi=n(Se);st=o(Yi,"A",{href:!0});var Zi=n(st);Yt=d(Zi,"Efficiency of Perspective View Policies Learned"),Zi.forEach(t),Yi.forEach(t),Ct=c(e),lt=o(e,"P",{});var Qi=n(lt);Zt=d(Qi,"The second experiment evaluates the efficiency of policies learned by RL agents trained on HEV views compared to those trained on BEV views."),Qi.forEach(t),It=c(e),ct=o(e,"P",{});var Xi=n(ct);Qt=d(Xi,"Table 2: MA-POCA mean episode length per coordinate frame in each environment (lower is better)"),Xi.forEach(t),Dt=c(e),ht(We.$$.fragment,e),xt=c(e),je=o(e,"H2",{id:!0});var Ji=n(je);ft=o(Ji,"A",{href:!0});var er=n(ft);Xt=d(er,"Conclusion"),er.forEach(t),Ji.forEach(t),Lt=c(e),dt=o(e,"P",{});var tr=n(dt);Jt=d(tr,"The Herd’s Eye View (HEV) framework offers a superior perception model in MARL environments, providing agents with a more comprehensive understanding of their surroundings, leading to improved decision-making and better overall performance. By using the HEV world-view collaborative perception problem, we show that the accuracy of collaborative perception models is significantly improved, leading to better IoU scores. Our work opens up new possibilities for advanced perception models in MARL, which can greatly enhance the performance of multi-robot systems by enabling better collaboration and coordination."),tr.forEach(t),Rt=c(e),qe=o(e,"P",{});var qt=n(qe);ei=d(qt,"The code for this project is available on "),Me=o(qt,"A",{href:!0,rel:!0,target:!0});var ir=n(Me);ti=d(ir,"GitHub"),ir.forEach(t),ii=d(qt,"."),qt.forEach(t),this.h()},h(){u(m,"href","#herds-eye-view-improving-game-ai-agent-learning-with-collaborative-perception"),u(p,"id","herds-eye-view-improving-game-ai-agent-learning-with-collaborative-perception"),u(v,"href","#introduction"),u(h,"id","introduction"),u(g,"href","#herds-eye-view-a-new-perspective"),u(b,"id","herds-eye-view-a-new-perspective"),u(_,"href","#perspective-views"),u(S,"id","perspective-views"),u(L,"href","#world-centric-herds-eye-view"),u(K,"id","world-centric-herds-eye-view"),u(J,"href","#ego-centric"),u(C,"id","ego-centric"),u(ee,"href","#ego-forward"),u(I,"id","ego-forward"),u(me,"href","#key-contributions"),u(T,"id","key-contributions"),u(Ee,"href","#environments"),u(oe,"id","environments"),u(Ze,"href","#collaborative-push-block"),u(Oe,"id","collaborative-push-block"),u(Xe,"href","#dungeon-escape"),u(Ge,"id","dungeon-escape"),u(et,"href","#planar-construction"),u(Fe,"id","planar-construction"),u(rt,"href","#experiments"),u(Ue,"id","experiments"),u(ot,"href","#accuracy-of-world-centric-predictions-vs-ego-centric"),u(Ne,"id","accuracy-of-world-centric-predictions-vs-ego-centric"),u(st,"href","#efficiency-of-perspective-view-policies-learned"),u(Se,"id","efficiency-of-perspective-view-policies-learned"),u(ft,"href","#conclusion"),u(je,"id","conclusion"),u(Me,"href","https://github.com/andrewnash/Herds-Eye-View"),u(Me,"rel","nofollow noopener noreferrer external"),u(Me,"target","_blank")},m(e,a){s(e,p,a),i(p,m),i(m,w),s(e,H,a),s(e,h,a),i(h,v),i(v,$),s(e,F,a),s(e,D,a),i(D,we),s(e,ne,a),s(e,x,a),i(x,ge),s(e,Y,a),s(e,b,a),i(b,g),i(g,z),s(e,ae,a),s(e,Z,a),i(Z,U),s(e,se,a),s(e,Q,a),i(Q,N),s(e,le,a),s(e,X,a),mt(k,X,null),s(e,ce,a),s(e,S,a),i(S,_),i(_,j),s(e,fe,a),s(e,K,a),i(K,L),i(L,_e),s(e,de,a),s(e,R,a),i(R,be),s(e,pe,a),s(e,C,a),i(C,J),i(J,ye),s(e,y,a),s(e,B,a),i(B,Te),s(e,he,a),s(e,I,a),i(I,ee),i(ee,He),s(e,O,a),s(e,te,a),i(te,Ve),s(e,G,a),s(e,ie,a),mt(E,ie,null),s(e,W,a),s(e,T,a),i(T,me),i(me,Ce),s(e,Ae,a),s(e,ve,a),i(ve,M),s(e,A,a),s(e,re,a),i(re,$e),i($e,Ie),i(re,De),i(re,V),i(V,xe),s(e,Pe,a),s(e,ue,a),i(ue,Le),s(e,P,a),s(e,oe,a),i(oe,Ee),i(Ee,Re),s(e,ke,a),s(e,Oe,a),i(Oe,Ze),i(Ze,Mt),s(e,wt,a),s(e,Qe,a),i(Qe,Ot),s(e,gt,a),s(e,Ge,a),i(Ge,Xe),i(Xe,Gt),s(e,_t,a),s(e,Je,a),i(Je,Ft),s(e,bt,a),s(e,Fe,a),i(Fe,et),i(et,zt),s(e,yt,a),s(e,tt,a),i(tt,Ut),s(e,Tt,a),s(e,it,a),mt(ze,it,null),s(e,Ht,a),s(e,Ue,a),i(Ue,rt),i(rt,Nt),s(e,Vt,a),s(e,Ne,a),i(Ne,ot),i(ot,St),s(e,$t,a),s(e,nt,a),i(nt,jt),s(e,At,a),s(e,at,a),i(at,Kt),s(e,Pt,a),mt(Be,e,a),s(e,kt,a),s(e,Se,a),i(Se,st),i(st,Yt),s(e,Ct,a),s(e,lt,a),i(lt,Zt),s(e,It,a),s(e,ct,a),i(ct,Qt),s(e,Dt,a),mt(We,e,a),s(e,xt,a),s(e,je,a),i(je,ft),i(ft,Xt),s(e,Lt,a),s(e,dt,a),i(dt,Jt),s(e,Rt,a),s(e,qe,a),i(qe,ei),i(qe,Me),i(Me,ti),i(qe,ii),Bt=!0},p(e,a){const ut={};a&2&&(ut.$$scope={dirty:a,ctx:e}),Be.$set(ut);const Et={};a&2&&(Et.$$scope={dirty:a,ctx:e}),We.$set(Et)},i(e){Bt||(Ke(k.$$.fragment,e),Ke(E.$$.fragment,e),Ke(ze.$$.fragment,e),Ke(Be.$$.fragment,e),Ke(We.$$.fragment,e),Bt=!0)},o(e){Ye(k.$$.fragment,e),Ye(E.$$.fragment,e),Ye(ze.$$.fragment,e),Ye(Be.$$.fragment,e),Ye(We.$$.fragment,e),Bt=!1},d(e){e&&t(p),e&&t(H),e&&t(h),e&&t(F),e&&t(D),e&&t(ne),e&&t(x),e&&t(Y),e&&t(b),e&&t(ae),e&&t(Z),e&&t(se),e&&t(Q),e&&t(le),e&&t(X),vt(k),e&&t(ce),e&&t(S),e&&t(fe),e&&t(K),e&&t(de),e&&t(R),e&&t(pe),e&&t(C),e&&t(y),e&&t(B),e&&t(he),e&&t(I),e&&t(O),e&&t(te),e&&t(G),e&&t(ie),vt(E),e&&t(W),e&&t(T),e&&t(Ae),e&&t(ve),e&&t(A),e&&t(re),e&&t(Pe),e&&t(ue),e&&t(P),e&&t(oe),e&&t(ke),e&&t(Oe),e&&t(wt),e&&t(Qe),e&&t(gt),e&&t(Ge),e&&t(_t),e&&t(Je),e&&t(bt),e&&t(Fe),e&&t(yt),e&&t(tt),e&&t(Tt),e&&t(it),vt(ze),e&&t(Ht),e&&t(Ue),e&&t(Vt),e&&t(Ne),e&&t($t),e&&t(nt),e&&t(At),e&&t(at),e&&t(Pt),vt(Be,e),e&&t(kt),e&&t(Se),e&&t(Ct),e&&t(lt),e&&t(It),e&&t(ct),e&&t(Dt),vt(We,e),e&&t(xt),e&&t(je),e&&t(Lt),e&&t(dt),e&&t(Rt),e&&t(qe)}}}function yr(q){let p,m;const w=[q[0],ar];let H={$$slots:{default:[br]},$$scope:{ctx:q}};for(let h=0;h<w.length;h+=1)H=oi(H,w[h]);return p=new ur({props:H}),{c(){pt(p.$$.fragment)},l(h){ht(p.$$.fragment,h)},m(h,v){mt(p,h,v),m=!0},p(h,[v]){const $=v&1?vr(w,[v&1&&rr(h[0]),v&0&&rr(ar)]):{};v&2&&($.$$scope={dirty:v,ctx:h}),p.$set($)},i(h){m||(Ke(p.$$.fragment,h),m=!0)},o(h){Ye(p.$$.fragment,h),m=!1},d(h){vt(p,h)}}}const ar={title:"My Research - Herds Eye View",image:"/hev/envs.PNG",alt:"HEV",created:"2023-04-20T00:00:00.000Z",tags:["Computer Vision","RL"],updated:"2023-06-14T20:55:04.596Z",images:[],slug:"/hev/+page.svelte.md",path:"/hev",toc:[{depth:1,title:"Herd’s Eye View: Improving Game AI Agent Learning with Collaborative Perception",slug:"herds-eye-view-improving-game-ai-agent-learning-with-collaborative-perception"},{depth:2,title:"Introduction",slug:"introduction"},{depth:2,title:"Herd’s Eye View: A New Perspective",slug:"herds-eye-view-a-new-perspective"},{depth:2,title:"Perspective Views",slug:"perspective-views"},{depth:3,title:"World-Centric (Herd’s Eye View)",slug:"world-centric-herds-eye-view"},{depth:3,title:"Ego-Centric",slug:"ego-centric"},{depth:3,title:"Ego-Forward",slug:"ego-forward"},{depth:2,title:"Key Contributions",slug:"key-contributions"},{depth:2,title:"Environments",slug:"environments"},{depth:3,title:"Collaborative Push Block",slug:"collaborative-push-block"},{depth:3,title:"Dungeon Escape",slug:"dungeon-escape"},{depth:3,title:"Planar Construction",slug:"planar-construction"},{depth:2,title:"Experiments",slug:"experiments"},{depth:3,title:"Accuracy of World-Centric Predictions vs Ego-Centric",slug:"accuracy-of-world-centric-predictions-vs-ego-centric"},{depth:3,title:"Efficiency of Perspective View Policies Learned",slug:"efficiency-of-perspective-view-policies-learned"},{depth:2,title:"Conclusion",slug:"conclusion"}]};function Tr(q,p,m){return q.$$set=w=>{m(0,p=oi(oi({},p),or(w)))},p=or(p),[p]}class Ar extends sr{constructor(p){super(),lr(this,p,Tr,yr,cr,{})}}export{Ar as component};
