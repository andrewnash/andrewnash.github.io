import{S as zo,i as Do,s as Fo,U as Ai,y as c,z as v,A as g,V as Co,W as xo,g as w,d as y,B as b,X as Lo,k as a,q as m,a as l,l as n,m as r,r as d,h as t,c as u,n as f,T as $i,b as o,D as s,E as Oo}from"../chunks/index.17f79c6b.js";import{P as Ho}from"../chunks/post_layout.facd0968.js";import{I as $}from"../chunks/footer.a30eaa02.js";function Wo(ne){let p,_,k,I,h,S,P,re,A,Ei,Xe,V,se,Ot,Be,le,Ht,Je,ue,x,Ke,L,fe,Wt,et,me,jt,tt,de,M,it,z,pe,qt,ot,he,Nt,at,D,ce,Yt,nt,ve,Gt,rt,ge,F,st,C,we,Qt,lt,ye,Zt,ut,O,Xt,H,ft,W,be,Bt,mt,_e,Jt,dt,$e,j,pt,Ae,Kt,ht,q,Ee,ei,ct,Te,ti,vt,Re,N,gt,Y,Se,ii,wt,ke,oi,yt,G,Ie,ai,bt,Ue,ni,_t,Pe,Q,$t,Ve,ri,At,Z,xe,si,Et,Le,li,Tt,Me,X,Rt,B,ze,ui,St,De,fi,kt,Fe,J,It,Ce,mi,Ut,Oe,K,Pt,ee,He,di,Vt,We,pi,xt,je,te,Lt,qe,ie,Mt,oe,Ne,hi,zt,Ye,ci,Dt,Ge,ae,Ft,E,Qe,vi,gi,T,Ti,wi,Ze,yi,bi,R,Ri,Ct;return x=new $({props:{src:"./srauv_ocean.png",alt:"srauv"}}),M=new $({props:{src:"./mech.png",alt:"mech"}}),F=new $({props:{src:"./unity_sim.png",alt:"unity_sim"}}),H=new $({props:{src:"./49_tanks.gif",alt:"tanks"}}),j=new $({props:{src:"./auto_pilot.png",alt:"system"}}),N=new $({props:{src:"./autnomous.gif",alt:"autnomous_flying"}}),Q=new $({props:{src:"./tank_driving.gif",alt:"tank_driving"}}),X=new $({props:{src:"./reward.PNG",alt:"srauv_in_tank"}}),J=new $({props:{src:"./srauv_in_tank.png",alt:"srauv_in_tank"}}),K=new $({props:{src:"./april_tank.gif",alt:"april_tags"}}),te=new $({props:{src:"./testing1.gif",alt:"testing1"}}),ie=new $({props:{src:"./testing2.gif",alt:"testing2"}}),ae=new $({props:{src:"./srauv.png",alt:"srauv"}}),{c(){p=a("h1"),_=a("a"),k=m("Navigating the Depths with RL"),I=l(),h=a("p"),S=m("Hello folks! My name is Andrew Nash, and I’ve been heavily involved in developing a software system for an Autonomous Subsea Vehicle, the SRAUV, as part of my engineering capstone project. In this blog post, I’ll detail the work I did on the Autopilot/AI perception, navigation, and control. Strap in for an exciting journey deep beneath the waves!"),P=l(),re=a("center"),A=a("iframe"),Xe=l(),V=a("h2"),se=a("a"),Ot=m("Introduction"),Be=l(),le=a("p"),Ht=m("In my capstone project, my team and I have tackled a pervasive problem in offshore oil and gas operations - consistent and reliable subsea monitoring. To address this, we’ve designed a prototype Subsea Resident Autonomous Underwater Vehicle (SRAUV). Our innovative SRAUV operates indefinitely under the sea, utilizing a dock on the seafloor that facilitates wireless charging and communication. It embarks on scheduled or on-demand inspection missions, collects vital data, and then returns to the dock to transmit this information. Through this approach, we’re ushering in a new era of advanced offshore operations, with potential upgrades such as real-time anomaly detection that promises enhanced efficiency and safety."),Je=l(),ue=a("p"),c(x.$$.fragment),Ke=l(),L=a("h2"),fe=a("a"),Wt=m("Vehicle"),et=l(),me=a("p"),jt=m("My exceptional teammates ensured the SRAUV’s mechanical design adhered to principles of hydrodynamics, stability, and compactness, resulting in a sleek, efficient design. Fitting within the original constraints of a 2-foot length and width, the vehicle’s overall mass is approximately 15 kg, as illustrated below. The team cleverly configured the thrusters to offer a versatile maneuverability, giving the SRAUV 5 degrees of freedom, though effectively using only 4 - excluding pitch and roll - for the project’s final demonstration."),tt=l(),de=a("p"),c(M.$$.fragment),it=l(),z=a("h2"),pe=a("a"),qt=m("Software Overview"),ot=l(),he=a("p"),Nt=m("The SRAUV software is a culmination of collaborative effort, using Unity 3D game engine for fast prototyping, extensive machine learning support, and engaging visual feedback. The interface is an all-in-one GUI Simulator, allowing the user to control the vehicle, visualize thruster direction feedback, and simulate various environmental conditions. All software for the project is openly available on Github."),at=l(),D=a("h2"),ce=a("a"),Yt=m("Simulator"),nt=l(),ve=a("p"),Gt=m("We developed a Unity-based simulator that accurately mimics the mechanics of the SRAUV. This simulator applies individual thrust vectors at each thruster’s respective angle, fine-tuning its underwater movement, often referred to as “driftyness”, to reflect the real-world feedback from experienced pilots. The simulation updates in response to these thrust commands, providing new sensor data which includes the SRAUV’s position and the relative locations of assets in the environment. This helped us efficiently and accurately test our autonomous flying solutions."),rt=l(),ge=a("p"),c(F.$$.fragment),st=l(),C=a("h2"),we=a("a"),Qt=m("Model Training"),lt=l(),ye=a("p"),Zt=m("In addition to the main simulator, I created a lighter “ML Tank” for training the DRL algorithms. This environment include a significantly reduced polygon count, a necessary optimization since DRL algorithms are computationally expensive to simulate. Further speed enhancements were achieved by including multiple ML Tanks, allowing multiple agents to collect observations simultaneously, this can be seen below."),ut=l(),O=a("p"),Xt=m("Screenshot of 49 ML Tanks training simultaneously within the Unity environment. "),c(H.$$.fragment),ft=l(),W=a("h2"),be=a("a"),Bt=m("Autonomous Strategy"),mt=l(),_e=a("p"),Jt=m("As the core designer of the Autopilot, I initially considered employing Q-learning for the SRAUV’s control system. This approach was favored due to its popularity and well-documented success. However, throughout the model selection process, I found Deep Reinforcement Learning (DRL) to be the most effective at driving in the simulator. I utilized Unity ML-Agents Toolkit for DRL training within the Unity simulator, leading to rapid experimentation with the SRAUV’s Flight Computer and Vision System."),dt=l(),$e=a("p"),c(j.$$.fragment),pt=l(),Ae=a("p"),Kt=m("With DRL, I designed an end-to-end solution for our control system. This allowed observations to go into the model and thrust control to emerge, providing optimal control of the SRAUV through a variety of unpredictable scenarios."),ht=l(),q=a("h2"),Ee=a("a"),ei=m("Model Development"),ct=l(),Te=a("p"),ti=m("For model training, I used a simplified subproblem - driving from a random X, Y, Z position to another random position as quickly as possible. I then tested various DRL solutions on these subproblems, progressively introducing complexity until the environment closely mirrored real life. This iterative approach allowed for significant improvement during model development."),vt=l(),Re=a("p"),c(N.$$.fragment),gt=l(),Y=a("h3"),Se=a("a"),ii=m("Model Selection"),wt=l(),ke=a("p"),oi=m("For the SRAUV control system, I considered both discrete and continuous action spaces. While a continuous action space implies real numbers for the DRL model output, a discrete one implies a fixed set of categories. I experimentally found discrete to be more effective during training. I benchmarked the Soft Actor-Critic (SAC) model against a Proximal Policy Optimization (PPO). SAC took a considerable time to train and often failed to reach the goal consistently. On the other hand, PPO found the optimal driving strategy faster and more effectively, which led to its selection for further development."),yt=l(),G=a("h3"),Ie=a("a"),ai=m("Reward Structure"),bt=l(),Ue=a("p"),ni=m("The reward structure found to best solve the easy environment was relatively straightforward, consisting of three different rewards. The first reward was assigned as +1 when the agent arrived within a predefined tolerance of its goal waypoint. The second reward was the negative inverse of maximum time steps -0.002, this small negative reward assigned every timestep encouraged the model to drive to the goal as fast as possible. Finally, the third reward sets the overall reward to -1 if the agent collides with anything, this also resets the run for increased training speed."),_t=l(),Pe=a("p"),c(Q.$$.fragment),$t=l(),Ve=a("p"),ri=m("In further iterations of the training environment, I found that the agent would sometimes overshoot its goal position and accidentally crash into a wall, to help mitigate this problem a stacking negative reward was assigned whenever the agent exceeded 0.3 m/s velocity in any axis of movement. This resulted in reduced crashing and taught the model to operate at a reduced speed when nearing its goal position."),At=l(),Z=a("h3"),xe=a("a"),si=m("Curriculum Learning"),Et=l(),Le=a("p"),li=m("Since the PPO algorithm struggled to converge when tolerances of less than 40cm were used in training, I utilized curriculum learning to overcome this challenge. This technique allowed me to progressively decrease the tolerance required to reach the goal waypoint during training, significantly enhancing the model’s performance."),Tt=l(),Me=a("p"),c(X.$$.fragment),Rt=l(),B=a("h2"),ze=a("a"),ui=m("Localization"),St=l(),De=a("p"),fi=m("After our initial approach for localization, which relied on a combination of a pressure sensor, time-of-flight (TOF) sensors, and an inertial-measurement-unit (IMU), failed to meet expectations, my team and I were forced to quickly pivot. Due to issues ranging from faulty TOF sensors, an unsteady IMU, and a malfunctioning depth sensor, we had to reimagine our solution, leading us to the exploration of a computer vision-based approach. Utilizing AprilTags, similar to simple QR codes, we established a localization system that successfully tracked the position and rotation of the SRAUV relative to the tank. We installed a grid of these AprilTags on the tank floor, ensuring the bottom camera of the SRAUV was able to constantly locate a tag, significantly improving our vehicle’s spatial awareness. Impressively, this solution not only worked exceedingly well during testing, achieving an accuracy of about +-5 cm in position and +-5 degrees in heading, but it was also considerably more cost-effective than our initial sensor plan."),kt=l(),Fe=a("p"),c(J.$$.fragment),It=l(),Ce=a("p"),mi=m("The AprilTags detection in the ML Tank was another area I optimized. Running the vision recognition on all 49 tanks in a single environment would have been computationally prohibitive. To address this, I added a camera with matching intrinsic parameters and position to the SRAUV’s bottom pi cam in the simulator. This allowed me to check if the AprilTag was in the camera’s frame during each timestep, providing the agent with the most recently known position & velocity estimate, simulating the noisy conditions it would face in the real world."),Ut=l(),Oe=a("p"),c(K.$$.fragment),Pt=l(),ee=a("h2"),He=a("a"),di=m("Real World Testing"),Vt=l(),We=a("p"),pi=m("Below you can see some videos of real world testing the SRAUV in a tank at Memorial University."),xt=l(),je=a("p"),c(te.$$.fragment),Lt=l(),qe=a("p"),c(ie.$$.fragment),Mt=l(),oe=a("h2"),Ne=a("a"),hi=m("Conclusion"),zt=l(),Ye=a("p"),ci=m("Wrapping up, I am incredibly proud of the progress and achievements made by my team and myself throughout our capstone project. We successfully designed, implemented, and tested a neural network-based autopilot for our Subsea Resident AUV (SRAUV) prototype. This enabled the SRAUV to perform autonomous missions, including navigating a prescribed route and redocking, a significant milestone for us. Even though we faced setbacks, such as hardware failures limiting the testing of an improved autopilot model, these challenges only fueled our drive to innovate. Our commitment paid off, winning us first place in all capstone award categories. We believe our work has set a strong foundation for future developments in autonomous subsea navigation."),Dt=l(),Ge=a("p"),c(ae.$$.fragment),Ft=l(),E=a("center"),Qe=a("p"),vi=m("Capstone Trailer (Awarded best video)"),gi=l(),T=a("iframe"),wi=l(),Ze=a("p"),yi=m("IEEE Night Presentation (Awarded best Presentation)"),bi=l(),R=a("iframe"),this.h()},l(e){p=n(e,"H1",{id:!0});var i=r(p);_=n(i,"A",{href:!0});var Si=r(_);k=d(Si,"Navigating the Depths with RL"),Si.forEach(t),i.forEach(t),I=u(e),h=n(e,"P",{});var ki=r(h);S=d(ki,"Hello folks! My name is Andrew Nash, and I’ve been heavily involved in developing a software system for an Autonomous Subsea Vehicle, the SRAUV, as part of my engineering capstone project. In this blog post, I’ll detail the work I did on the Autopilot/AI perception, navigation, and control. Strap in for an exciting journey deep beneath the waves!"),ki.forEach(t),P=u(e),re=n(e,"CENTER",{});var Ii=r(re);A=n(Ii,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),r(A).forEach(t),Ii.forEach(t),Xe=u(e),V=n(e,"H2",{id:!0});var Ui=r(V);se=n(Ui,"A",{href:!0});var Pi=r(se);Ot=d(Pi,"Introduction"),Pi.forEach(t),Ui.forEach(t),Be=u(e),le=n(e,"P",{});var Vi=r(le);Ht=d(Vi,"In my capstone project, my team and I have tackled a pervasive problem in offshore oil and gas operations - consistent and reliable subsea monitoring. To address this, we’ve designed a prototype Subsea Resident Autonomous Underwater Vehicle (SRAUV). Our innovative SRAUV operates indefinitely under the sea, utilizing a dock on the seafloor that facilitates wireless charging and communication. It embarks on scheduled or on-demand inspection missions, collects vital data, and then returns to the dock to transmit this information. Through this approach, we’re ushering in a new era of advanced offshore operations, with potential upgrades such as real-time anomaly detection that promises enhanced efficiency and safety."),Vi.forEach(t),Je=u(e),ue=n(e,"P",{});var xi=r(ue);v(x.$$.fragment,xi),xi.forEach(t),Ke=u(e),L=n(e,"H2",{id:!0});var Li=r(L);fe=n(Li,"A",{href:!0});var Mi=r(fe);Wt=d(Mi,"Vehicle"),Mi.forEach(t),Li.forEach(t),et=u(e),me=n(e,"P",{});var zi=r(me);jt=d(zi,"My exceptional teammates ensured the SRAUV’s mechanical design adhered to principles of hydrodynamics, stability, and compactness, resulting in a sleek, efficient design. Fitting within the original constraints of a 2-foot length and width, the vehicle’s overall mass is approximately 15 kg, as illustrated below. The team cleverly configured the thrusters to offer a versatile maneuverability, giving the SRAUV 5 degrees of freedom, though effectively using only 4 - excluding pitch and roll - for the project’s final demonstration."),zi.forEach(t),tt=u(e),de=n(e,"P",{});var Di=r(de);v(M.$$.fragment,Di),Di.forEach(t),it=u(e),z=n(e,"H2",{id:!0});var Fi=r(z);pe=n(Fi,"A",{href:!0});var Ci=r(pe);qt=d(Ci,"Software Overview"),Ci.forEach(t),Fi.forEach(t),ot=u(e),he=n(e,"P",{});var Oi=r(he);Nt=d(Oi,"The SRAUV software is a culmination of collaborative effort, using Unity 3D game engine for fast prototyping, extensive machine learning support, and engaging visual feedback. The interface is an all-in-one GUI Simulator, allowing the user to control the vehicle, visualize thruster direction feedback, and simulate various environmental conditions. All software for the project is openly available on Github."),Oi.forEach(t),at=u(e),D=n(e,"H2",{id:!0});var Hi=r(D);ce=n(Hi,"A",{href:!0});var Wi=r(ce);Yt=d(Wi,"Simulator"),Wi.forEach(t),Hi.forEach(t),nt=u(e),ve=n(e,"P",{});var ji=r(ve);Gt=d(ji,"We developed a Unity-based simulator that accurately mimics the mechanics of the SRAUV. This simulator applies individual thrust vectors at each thruster’s respective angle, fine-tuning its underwater movement, often referred to as “driftyness”, to reflect the real-world feedback from experienced pilots. The simulation updates in response to these thrust commands, providing new sensor data which includes the SRAUV’s position and the relative locations of assets in the environment. This helped us efficiently and accurately test our autonomous flying solutions."),ji.forEach(t),rt=u(e),ge=n(e,"P",{});var qi=r(ge);v(F.$$.fragment,qi),qi.forEach(t),st=u(e),C=n(e,"H2",{id:!0});var Ni=r(C);we=n(Ni,"A",{href:!0});var Yi=r(we);Qt=d(Yi,"Model Training"),Yi.forEach(t),Ni.forEach(t),lt=u(e),ye=n(e,"P",{});var Gi=r(ye);Zt=d(Gi,"In addition to the main simulator, I created a lighter “ML Tank” for training the DRL algorithms. This environment include a significantly reduced polygon count, a necessary optimization since DRL algorithms are computationally expensive to simulate. Further speed enhancements were achieved by including multiple ML Tanks, allowing multiple agents to collect observations simultaneously, this can be seen below."),Gi.forEach(t),ut=u(e),O=n(e,"P",{});var _i=r(O);Xt=d(_i,"Screenshot of 49 ML Tanks training simultaneously within the Unity environment. "),v(H.$$.fragment,_i),_i.forEach(t),ft=u(e),W=n(e,"H2",{id:!0});var Qi=r(W);be=n(Qi,"A",{href:!0});var Zi=r(be);Bt=d(Zi,"Autonomous Strategy"),Zi.forEach(t),Qi.forEach(t),mt=u(e),_e=n(e,"P",{});var Xi=r(_e);Jt=d(Xi,"As the core designer of the Autopilot, I initially considered employing Q-learning for the SRAUV’s control system. This approach was favored due to its popularity and well-documented success. However, throughout the model selection process, I found Deep Reinforcement Learning (DRL) to be the most effective at driving in the simulator. I utilized Unity ML-Agents Toolkit for DRL training within the Unity simulator, leading to rapid experimentation with the SRAUV’s Flight Computer and Vision System."),Xi.forEach(t),dt=u(e),$e=n(e,"P",{});var Bi=r($e);v(j.$$.fragment,Bi),Bi.forEach(t),pt=u(e),Ae=n(e,"P",{});var Ji=r(Ae);Kt=d(Ji,"With DRL, I designed an end-to-end solution for our control system. This allowed observations to go into the model and thrust control to emerge, providing optimal control of the SRAUV through a variety of unpredictable scenarios."),Ji.forEach(t),ht=u(e),q=n(e,"H2",{id:!0});var Ki=r(q);Ee=n(Ki,"A",{href:!0});var eo=r(Ee);ei=d(eo,"Model Development"),eo.forEach(t),Ki.forEach(t),ct=u(e),Te=n(e,"P",{});var to=r(Te);ti=d(to,"For model training, I used a simplified subproblem - driving from a random X, Y, Z position to another random position as quickly as possible. I then tested various DRL solutions on these subproblems, progressively introducing complexity until the environment closely mirrored real life. This iterative approach allowed for significant improvement during model development."),to.forEach(t),vt=u(e),Re=n(e,"P",{});var io=r(Re);v(N.$$.fragment,io),io.forEach(t),gt=u(e),Y=n(e,"H3",{id:!0});var oo=r(Y);Se=n(oo,"A",{href:!0});var ao=r(Se);ii=d(ao,"Model Selection"),ao.forEach(t),oo.forEach(t),wt=u(e),ke=n(e,"P",{});var no=r(ke);oi=d(no,"For the SRAUV control system, I considered both discrete and continuous action spaces. While a continuous action space implies real numbers for the DRL model output, a discrete one implies a fixed set of categories. I experimentally found discrete to be more effective during training. I benchmarked the Soft Actor-Critic (SAC) model against a Proximal Policy Optimization (PPO). SAC took a considerable time to train and often failed to reach the goal consistently. On the other hand, PPO found the optimal driving strategy faster and more effectively, which led to its selection for further development."),no.forEach(t),yt=u(e),G=n(e,"H3",{id:!0});var ro=r(G);Ie=n(ro,"A",{href:!0});var so=r(Ie);ai=d(so,"Reward Structure"),so.forEach(t),ro.forEach(t),bt=u(e),Ue=n(e,"P",{});var lo=r(Ue);ni=d(lo,"The reward structure found to best solve the easy environment was relatively straightforward, consisting of three different rewards. The first reward was assigned as +1 when the agent arrived within a predefined tolerance of its goal waypoint. The second reward was the negative inverse of maximum time steps -0.002, this small negative reward assigned every timestep encouraged the model to drive to the goal as fast as possible. Finally, the third reward sets the overall reward to -1 if the agent collides with anything, this also resets the run for increased training speed."),lo.forEach(t),_t=u(e),Pe=n(e,"P",{});var uo=r(Pe);v(Q.$$.fragment,uo),uo.forEach(t),$t=u(e),Ve=n(e,"P",{});var fo=r(Ve);ri=d(fo,"In further iterations of the training environment, I found that the agent would sometimes overshoot its goal position and accidentally crash into a wall, to help mitigate this problem a stacking negative reward was assigned whenever the agent exceeded 0.3 m/s velocity in any axis of movement. This resulted in reduced crashing and taught the model to operate at a reduced speed when nearing its goal position."),fo.forEach(t),At=u(e),Z=n(e,"H3",{id:!0});var mo=r(Z);xe=n(mo,"A",{href:!0});var po=r(xe);si=d(po,"Curriculum Learning"),po.forEach(t),mo.forEach(t),Et=u(e),Le=n(e,"P",{});var ho=r(Le);li=d(ho,"Since the PPO algorithm struggled to converge when tolerances of less than 40cm were used in training, I utilized curriculum learning to overcome this challenge. This technique allowed me to progressively decrease the tolerance required to reach the goal waypoint during training, significantly enhancing the model’s performance."),ho.forEach(t),Tt=u(e),Me=n(e,"P",{});var co=r(Me);v(X.$$.fragment,co),co.forEach(t),Rt=u(e),B=n(e,"H2",{id:!0});var vo=r(B);ze=n(vo,"A",{href:!0});var go=r(ze);ui=d(go,"Localization"),go.forEach(t),vo.forEach(t),St=u(e),De=n(e,"P",{});var wo=r(De);fi=d(wo,"After our initial approach for localization, which relied on a combination of a pressure sensor, time-of-flight (TOF) sensors, and an inertial-measurement-unit (IMU), failed to meet expectations, my team and I were forced to quickly pivot. Due to issues ranging from faulty TOF sensors, an unsteady IMU, and a malfunctioning depth sensor, we had to reimagine our solution, leading us to the exploration of a computer vision-based approach. Utilizing AprilTags, similar to simple QR codes, we established a localization system that successfully tracked the position and rotation of the SRAUV relative to the tank. We installed a grid of these AprilTags on the tank floor, ensuring the bottom camera of the SRAUV was able to constantly locate a tag, significantly improving our vehicle’s spatial awareness. Impressively, this solution not only worked exceedingly well during testing, achieving an accuracy of about +-5 cm in position and +-5 degrees in heading, but it was also considerably more cost-effective than our initial sensor plan."),wo.forEach(t),kt=u(e),Fe=n(e,"P",{});var yo=r(Fe);v(J.$$.fragment,yo),yo.forEach(t),It=u(e),Ce=n(e,"P",{});var bo=r(Ce);mi=d(bo,"The AprilTags detection in the ML Tank was another area I optimized. Running the vision recognition on all 49 tanks in a single environment would have been computationally prohibitive. To address this, I added a camera with matching intrinsic parameters and position to the SRAUV’s bottom pi cam in the simulator. This allowed me to check if the AprilTag was in the camera’s frame during each timestep, providing the agent with the most recently known position & velocity estimate, simulating the noisy conditions it would face in the real world."),bo.forEach(t),Ut=u(e),Oe=n(e,"P",{});var _o=r(Oe);v(K.$$.fragment,_o),_o.forEach(t),Pt=u(e),ee=n(e,"H2",{id:!0});var $o=r(ee);He=n($o,"A",{href:!0});var Ao=r(He);di=d(Ao,"Real World Testing"),Ao.forEach(t),$o.forEach(t),Vt=u(e),We=n(e,"P",{});var Eo=r(We);pi=d(Eo,"Below you can see some videos of real world testing the SRAUV in a tank at Memorial University."),Eo.forEach(t),xt=u(e),je=n(e,"P",{});var To=r(je);v(te.$$.fragment,To),To.forEach(t),Lt=u(e),qe=n(e,"P",{});var Ro=r(qe);v(ie.$$.fragment,Ro),Ro.forEach(t),Mt=u(e),oe=n(e,"H2",{id:!0});var So=r(oe);Ne=n(So,"A",{href:!0});var ko=r(Ne);hi=d(ko,"Conclusion"),ko.forEach(t),So.forEach(t),zt=u(e),Ye=n(e,"P",{});var Io=r(Ye);ci=d(Io,"Wrapping up, I am incredibly proud of the progress and achievements made by my team and myself throughout our capstone project. We successfully designed, implemented, and tested a neural network-based autopilot for our Subsea Resident AUV (SRAUV) prototype. This enabled the SRAUV to perform autonomous missions, including navigating a prescribed route and redocking, a significant milestone for us. Even though we faced setbacks, such as hardware failures limiting the testing of an improved autopilot model, these challenges only fueled our drive to innovate. Our commitment paid off, winning us first place in all capstone award categories. We believe our work has set a strong foundation for future developments in autonomous subsea navigation."),Io.forEach(t),Dt=u(e),Ge=n(e,"P",{});var Uo=r(Ge);v(ae.$$.fragment,Uo),Uo.forEach(t),Ft=u(e),E=n(e,"CENTER",{});var U=r(E);Qe=n(U,"P",{});var Po=r(Qe);vi=d(Po,"Capstone Trailer (Awarded best video)"),Po.forEach(t),gi=u(U),T=n(U,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),r(T).forEach(t),wi=u(U),Ze=n(U,"P",{});var Vo=r(Ze);yi=d(Vo,"IEEE Night Presentation (Awarded best Presentation)"),Vo.forEach(t),bi=u(U),R=n(U,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),r(R).forEach(t),U.forEach(t),this.h()},h(){f(_,"href","#navigating-the-depths-with-rl"),f(p,"id","navigating-the-depths-with-rl"),f(A,"width","560"),f(A,"height","315"),$i(A.src,Ei="https://www.youtube.com/embed/wzNI8wY0UT0")||f(A,"src",Ei),f(A,"title","YouTube video player"),f(A,"frameborder","0"),f(A,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"),A.allowFullscreen=!0,f(se,"href","#introduction"),f(V,"id","introduction"),f(fe,"href","#vehicle"),f(L,"id","vehicle"),f(pe,"href","#software-overview"),f(z,"id","software-overview"),f(ce,"href","#simulator"),f(D,"id","simulator"),f(we,"href","#model-training"),f(C,"id","model-training"),f(be,"href","#autonomous-strategy"),f(W,"id","autonomous-strategy"),f(Ee,"href","#model-development"),f(q,"id","model-development"),f(Se,"href","#model-selection"),f(Y,"id","model-selection"),f(Ie,"href","#reward-structure"),f(G,"id","reward-structure"),f(xe,"href","#curriculum-learning"),f(Z,"id","curriculum-learning"),f(ze,"href","#localization"),f(B,"id","localization"),f(He,"href","#real-world-testing"),f(ee,"id","real-world-testing"),f(Ne,"href","#conclusion"),f(oe,"id","conclusion"),f(T,"width","560"),f(T,"height","315"),$i(T.src,Ti="https://www.youtube.com/embed/v3Z9sVuU5DQ")||f(T,"src",Ti),f(T,"title","YouTube video player"),f(T,"frameborder","0"),f(T,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"),T.allowFullscreen=!0,f(R,"width","560"),f(R,"height","315"),$i(R.src,Ri="https://www.youtube.com/embed/UXqgjFvW4HE")||f(R,"src",Ri),f(R,"title","YouTube video player"),f(R,"frameborder","0"),f(R,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"),R.allowFullscreen=!0},m(e,i){o(e,p,i),s(p,_),s(_,k),o(e,I,i),o(e,h,i),s(h,S),o(e,P,i),o(e,re,i),s(re,A),o(e,Xe,i),o(e,V,i),s(V,se),s(se,Ot),o(e,Be,i),o(e,le,i),s(le,Ht),o(e,Je,i),o(e,ue,i),g(x,ue,null),o(e,Ke,i),o(e,L,i),s(L,fe),s(fe,Wt),o(e,et,i),o(e,me,i),s(me,jt),o(e,tt,i),o(e,de,i),g(M,de,null),o(e,it,i),o(e,z,i),s(z,pe),s(pe,qt),o(e,ot,i),o(e,he,i),s(he,Nt),o(e,at,i),o(e,D,i),s(D,ce),s(ce,Yt),o(e,nt,i),o(e,ve,i),s(ve,Gt),o(e,rt,i),o(e,ge,i),g(F,ge,null),o(e,st,i),o(e,C,i),s(C,we),s(we,Qt),o(e,lt,i),o(e,ye,i),s(ye,Zt),o(e,ut,i),o(e,O,i),s(O,Xt),g(H,O,null),o(e,ft,i),o(e,W,i),s(W,be),s(be,Bt),o(e,mt,i),o(e,_e,i),s(_e,Jt),o(e,dt,i),o(e,$e,i),g(j,$e,null),o(e,pt,i),o(e,Ae,i),s(Ae,Kt),o(e,ht,i),o(e,q,i),s(q,Ee),s(Ee,ei),o(e,ct,i),o(e,Te,i),s(Te,ti),o(e,vt,i),o(e,Re,i),g(N,Re,null),o(e,gt,i),o(e,Y,i),s(Y,Se),s(Se,ii),o(e,wt,i),o(e,ke,i),s(ke,oi),o(e,yt,i),o(e,G,i),s(G,Ie),s(Ie,ai),o(e,bt,i),o(e,Ue,i),s(Ue,ni),o(e,_t,i),o(e,Pe,i),g(Q,Pe,null),o(e,$t,i),o(e,Ve,i),s(Ve,ri),o(e,At,i),o(e,Z,i),s(Z,xe),s(xe,si),o(e,Et,i),o(e,Le,i),s(Le,li),o(e,Tt,i),o(e,Me,i),g(X,Me,null),o(e,Rt,i),o(e,B,i),s(B,ze),s(ze,ui),o(e,St,i),o(e,De,i),s(De,fi),o(e,kt,i),o(e,Fe,i),g(J,Fe,null),o(e,It,i),o(e,Ce,i),s(Ce,mi),o(e,Ut,i),o(e,Oe,i),g(K,Oe,null),o(e,Pt,i),o(e,ee,i),s(ee,He),s(He,di),o(e,Vt,i),o(e,We,i),s(We,pi),o(e,xt,i),o(e,je,i),g(te,je,null),o(e,Lt,i),o(e,qe,i),g(ie,qe,null),o(e,Mt,i),o(e,oe,i),s(oe,Ne),s(Ne,hi),o(e,zt,i),o(e,Ye,i),s(Ye,ci),o(e,Dt,i),o(e,Ge,i),g(ae,Ge,null),o(e,Ft,i),o(e,E,i),s(E,Qe),s(Qe,vi),s(E,gi),s(E,T),s(E,wi),s(E,Ze),s(Ze,yi),s(E,bi),s(E,R),Ct=!0},p:Oo,i(e){Ct||(w(x.$$.fragment,e),w(M.$$.fragment,e),w(F.$$.fragment,e),w(H.$$.fragment,e),w(j.$$.fragment,e),w(N.$$.fragment,e),w(Q.$$.fragment,e),w(X.$$.fragment,e),w(J.$$.fragment,e),w(K.$$.fragment,e),w(te.$$.fragment,e),w(ie.$$.fragment,e),w(ae.$$.fragment,e),Ct=!0)},o(e){y(x.$$.fragment,e),y(M.$$.fragment,e),y(F.$$.fragment,e),y(H.$$.fragment,e),y(j.$$.fragment,e),y(N.$$.fragment,e),y(Q.$$.fragment,e),y(X.$$.fragment,e),y(J.$$.fragment,e),y(K.$$.fragment,e),y(te.$$.fragment,e),y(ie.$$.fragment,e),y(ae.$$.fragment,e),Ct=!1},d(e){e&&t(p),e&&t(I),e&&t(h),e&&t(P),e&&t(re),e&&t(Xe),e&&t(V),e&&t(Be),e&&t(le),e&&t(Je),e&&t(ue),b(x),e&&t(Ke),e&&t(L),e&&t(et),e&&t(me),e&&t(tt),e&&t(de),b(M),e&&t(it),e&&t(z),e&&t(ot),e&&t(he),e&&t(at),e&&t(D),e&&t(nt),e&&t(ve),e&&t(rt),e&&t(ge),b(F),e&&t(st),e&&t(C),e&&t(lt),e&&t(ye),e&&t(ut),e&&t(O),b(H),e&&t(ft),e&&t(W),e&&t(mt),e&&t(_e),e&&t(dt),e&&t($e),b(j),e&&t(pt),e&&t(Ae),e&&t(ht),e&&t(q),e&&t(ct),e&&t(Te),e&&t(vt),e&&t(Re),b(N),e&&t(gt),e&&t(Y),e&&t(wt),e&&t(ke),e&&t(yt),e&&t(G),e&&t(bt),e&&t(Ue),e&&t(_t),e&&t(Pe),b(Q),e&&t($t),e&&t(Ve),e&&t(At),e&&t(Z),e&&t(Et),e&&t(Le),e&&t(Tt),e&&t(Me),b(X),e&&t(Rt),e&&t(B),e&&t(St),e&&t(De),e&&t(kt),e&&t(Fe),b(J),e&&t(It),e&&t(Ce),e&&t(Ut),e&&t(Oe),b(K),e&&t(Pt),e&&t(ee),e&&t(Vt),e&&t(We),e&&t(xt),e&&t(je),b(te),e&&t(Lt),e&&t(qe),b(ie),e&&t(Mt),e&&t(oe),e&&t(zt),e&&t(Ye),e&&t(Dt),e&&t(Ge),b(ae),e&&t(Ft),e&&t(E)}}}function jo(ne){let p,_;const k=[ne[0],Mo];let I={$$slots:{default:[Wo]},$$scope:{ctx:ne}};for(let h=0;h<k.length;h+=1)I=Ai(I,k[h]);return p=new Ho({props:I}),{c(){c(p.$$.fragment)},l(h){v(p.$$.fragment,h)},m(h,S){g(p,h,S),_=!0},p(h,[S]){const P=S&1?Co(k,[S&1&&xo(h[0]),S&0&&xo(Mo)]):{};S&2&&(P.$$scope={dirty:S,ctx:h}),p.$set(P)},i(h){_||(w(p.$$.fragment,h),_=!0)},o(h){y(p.$$.fragment,h),_=!1},d(h){b(p,h)}}}const Mo={title:"Subsea Resident Autonomous Underwater Vehicle",image:"/srauv/team.jpg",alt:"SRAUV",created:"2020-05-02T00:00:00.000Z",tags:["Computer Vision","RL","Unity","TensorFlow","C#"],updated:"2023-06-16T14:28:50.738Z",images:[],slug:"/srauv/+page.svelte.md",path:"/srauv",toc:[{depth:1,title:"Navigating the Depths with RL",slug:"navigating-the-depths-with-rl"},{depth:2,title:"Introduction",slug:"introduction"},{depth:2,title:"Vehicle",slug:"vehicle"},{depth:2,title:"Software Overview",slug:"software-overview"},{depth:2,title:"Simulator",slug:"simulator"},{depth:2,title:"Model Training",slug:"model-training"},{depth:2,title:"Autonomous Strategy",slug:"autonomous-strategy"},{depth:2,title:"Model Development",slug:"model-development"},{depth:3,title:"Model Selection",slug:"model-selection"},{depth:3,title:"Reward Structure",slug:"reward-structure"},{depth:3,title:"Curriculum Learning",slug:"curriculum-learning"},{depth:2,title:"Localization",slug:"localization"},{depth:2,title:"Real World Testing",slug:"real-world-testing"},{depth:2,title:"Conclusion",slug:"conclusion"}]};function qo(ne,p,_){return ne.$$set=k=>{_(0,p=Ai(Ai({},p),Lo(k)))},p=Lo(p),[p]}class Qo extends zo{constructor(p){super(),Do(this,p,qo,jo,Fo,{})}}export{Qo as component};
