import{S as cr,i as dr,s as pr,N as hr,k as r,l as o,m as n,h as t,n as u,b as s,D as i,O as mr,P as vr,Q as ur,g as Ce,d as De,U as li,y as qe,z as Oe,A as Me,V as Er,W as sr,B as Ge,X as lr,q as c,a as l,r as d,c as f,E as fi}from"../chunks/index.17f79c6b.js";import{P as wr}from"../chunks/post_layout.c162e9f1.js";import{I as _t}from"../chunks/footer.ef185f6b.js";function gr(R){let p,h,E;const T=R[1].default,m=hr(T,R,R[0],null);return{c(){p=r("div"),h=r("table"),m&&m.c(),this.h()},l(v){p=o(v,"DIV",{class:!0});var H=n(p);h=o(H,"TABLE",{class:!0});var x=n(h);m&&m.l(x),x.forEach(t),H.forEach(t),this.h()},h(){u(h,"class","table w-full"),u(p,"class","overflow-x-auto mb-4")},m(v,H){s(v,p,H),i(p,h),m&&m.m(h,null),E=!0},p(v,[H]){m&&m.p&&(!E||H&1)&&mr(m,T,v,v[0],E?ur(T,v[0],H,null):vr(v[0]),null)},i(v){E||(Ce(m,v),E=!0)},o(v){De(m,v),E=!1},d(v){v&&t(p),m&&m.d(v)}}}function _r(R,p,h){let{$$slots:E={},$$scope:T}=p;return R.$$set=m=>{"$$scope"in m&&h(0,T=m.$$scope)},[T,E]}class si extends cr{constructor(p){super(),dr(this,p,_r,gr,pr,{})}}function br(R){let p,h,E,T,m,v,H,x,P,L,W,V,J,I,w,g,_,q,O,U,ee,K,B,te,Y,M,de,ie,y,re,pe,oe,z,_e,he,N,be,me,G,ne,ye,$,S,$e,ve,F,ae,Te,Q,se,He,X,le;return{c(){p=r("thead"),h=r("tr"),E=r("th"),T=c("Environment"),m=l(),v=r("th"),H=c("World-Centric"),x=l(),P=r("th"),L=c("Ego-Centric"),W=l(),V=r("th"),J=c("Ego-Forward"),I=l(),w=r("tbody"),g=r("tr"),_=r("td"),q=c("Collaborative Push Block"),O=l(),U=r("td"),ee=c("96.94%"),K=l(),B=r("td"),te=c("63.87%"),Y=l(),M=r("td"),de=c("64.22%"),ie=l(),y=r("tr"),re=r("td"),pe=c("Dungeon Escape"),oe=l(),z=r("td"),_e=c("43.53%"),he=l(),N=r("td"),be=c("1.82%"),me=l(),G=r("td"),ne=c("26.07%"),ye=l(),$=r("tr"),S=r("td"),$e=c("Planar Construction"),ve=l(),F=r("td"),ae=c("48.37%"),Te=l(),Q=r("td"),se=c("35.45%"),He=l(),X=r("td"),le=c("10.16%")},l(b){p=o(b,"THEAD",{});var j=n(p);h=o(j,"TR",{});var A=n(h);E=o(A,"TH",{});var ue=n(E);T=d(ue,"Environment"),ue.forEach(t),m=f(A),v=o(A,"TH",{});var Ie=n(v);H=d(Ie,"World-Centric"),Ie.forEach(t),x=f(A),P=o(A,"TH",{});var Pe=n(P);L=d(Pe,"Ego-Centric"),Pe.forEach(t),W=f(A),V=o(A,"TH",{});var Ee=n(V);J=d(Ee,"Ego-Forward"),Ee.forEach(t),A.forEach(t),j.forEach(t),I=f(b),w=o(b,"TBODY",{});var Z=n(w);g=o(Z,"TR",{});var C=n(g);_=o(C,"TD",{});var fe=n(_);q=d(fe,"Collaborative Push Block"),fe.forEach(t),O=f(C),U=o(C,"TD",{});var Ve=n(U);ee=d(Ve,"96.94%"),Ve.forEach(t),K=f(C),B=o(C,"TD",{});var xe=n(B);te=d(xe,"63.87%"),xe.forEach(t),Y=f(C),M=o(C,"TD",{});var Be=n(M);de=d(Be,"64.22%"),Be.forEach(t),C.forEach(t),ie=f(Z),y=o(Z,"TR",{});var k=n(y);re=o(k,"TD",{});var Re=n(re);pe=d(Re,"Dungeon Escape"),Re.forEach(t),oe=f(k),z=o(k,"TD",{});var Ae=n(z);_e=d(Ae,"43.53%"),Ae.forEach(t),he=f(k),N=o(k,"TD",{});var we=n(N);be=d(we,"1.82%"),we.forEach(t),me=f(k),G=o(k,"TD",{});var Le=n(G);ne=d(Le,"26.07%"),Le.forEach(t),k.forEach(t),ye=f(Z),$=o(Z,"TR",{});var D=n($);S=o(D,"TD",{});var ce=n(S);$e=d(ce,"Planar Construction"),ce.forEach(t),ve=f(D),F=o(D,"TD",{});var ge=n(F);ae=d(ge,"48.37%"),ge.forEach(t),Te=f(D),Q=o(D,"TD",{});var We=n(Q);se=d(We,"35.45%"),We.forEach(t),He=f(D),X=o(D,"TD",{});var ke=n(X);le=d(ke,"10.16%"),ke.forEach(t),D.forEach(t),Z.forEach(t)},m(b,j){s(b,p,j),i(p,h),i(h,E),i(E,T),i(h,m),i(h,v),i(v,H),i(h,x),i(h,P),i(P,L),i(h,W),i(h,V),i(V,J),s(b,I,j),s(b,w,j),i(w,g),i(g,_),i(_,q),i(g,O),i(g,U),i(U,ee),i(g,K),i(g,B),i(B,te),i(g,Y),i(g,M),i(M,de),i(w,ie),i(w,y),i(y,re),i(re,pe),i(y,oe),i(y,z),i(z,_e),i(y,he),i(y,N),i(N,be),i(y,me),i(y,G),i(G,ne),i(w,ye),i(w,$),i($,S),i(S,$e),i($,ve),i($,F),i(F,ae),i($,Te),i($,Q),i(Q,se),i($,He),i($,X),i(X,le)},p:fi,d(b){b&&t(p),b&&t(I),b&&t(w)}}}function yr(R){let p,h,E,T,m,v,H,x,P,L,W,V,J,I,w,g;return V=new _t({props:{src:"./pushblock.gif",alt:"pb"}}),w=new _t({props:{src:"dungeonescape.gif",alt:"de"}}),{c(){p=r("thead"),h=r("tr"),E=r("th"),T=c("Push Block Environment"),m=l(),v=r("th"),H=c("Dungeon Escape Environment"),x=l(),P=r("tbody"),L=r("tr"),W=r("td"),qe(V.$$.fragment),J=l(),I=r("td"),qe(w.$$.fragment),this.h()},l(_){p=o(_,"THEAD",{});var q=n(p);h=o(q,"TR",{});var O=n(h);E=o(O,"TH",{align:!0});var U=n(E);T=d(U,"Push Block Environment"),U.forEach(t),m=f(O),v=o(O,"TH",{align:!0});var ee=n(v);H=d(ee,"Dungeon Escape Environment"),ee.forEach(t),O.forEach(t),q.forEach(t),x=f(_),P=o(_,"TBODY",{});var K=n(P);L=o(K,"TR",{});var B=n(L);W=o(B,"TD",{align:!0});var te=n(W);Oe(V.$$.fragment,te),te.forEach(t),J=f(B),I=o(B,"TD",{align:!0});var Y=n(I);Oe(w.$$.fragment,Y),Y.forEach(t),B.forEach(t),K.forEach(t),this.h()},h(){u(E,"align","center"),u(v,"align","center"),u(W,"align","center"),u(I,"align","center")},m(_,q){s(_,p,q),i(p,h),i(h,E),i(E,T),i(h,m),i(h,v),i(v,H),s(_,x,q),s(_,P,q),i(P,L),i(L,W),Me(V,W,null),i(L,J),i(L,I),Me(w,I,null),g=!0},p:fi,i(_){g||(Ce(V.$$.fragment,_),Ce(w.$$.fragment,_),g=!0)},o(_){De(V.$$.fragment,_),De(w.$$.fragment,_),g=!1},d(_){_&&t(p),_&&t(x),_&&t(P),Ge(V),Ge(w)}}}function $r(R){let p,h,E,T,m,v,H,x,P,L,W,V,J,I,w,g,_,q,O,U,ee,K,B,te,Y,M,de,ie,y,re,pe,oe,z,_e,he,N,be,me,G,ne,ye,$,S,$e,ve,F,ae,Te,Q,se,He,X,le;return{c(){p=r("thead"),h=r("tr"),E=r("th"),T=c("Environment"),m=l(),v=r("th"),H=c("World-Centric"),x=l(),P=r("th"),L=c("Ego-Centric"),W=l(),V=r("th"),J=c("Ego-Forward"),I=l(),w=r("tbody"),g=r("tr"),_=r("td"),q=c("Collaborative Push Block"),O=l(),U=r("td"),ee=c("222.6"),K=l(),B=r("td"),te=c("246.6"),Y=l(),M=r("td"),de=c("230.5"),ie=l(),y=r("tr"),re=r("td"),pe=c("Dungeon Escape"),oe=l(),z=r("td"),_e=c("14.15"),he=l(),N=r("td"),be=c("16.74"),me=l(),G=r("td"),ne=c("23.27"),ye=l(),$=r("tr"),S=r("td"),$e=c("Planar Construction"),ve=l(),F=r("td"),ae=c("110.5"),Te=l(),Q=r("td"),se=c("120.3"),He=l(),X=r("td"),le=c("149.9")},l(b){p=o(b,"THEAD",{});var j=n(p);h=o(j,"TR",{});var A=n(h);E=o(A,"TH",{});var ue=n(E);T=d(ue,"Environment"),ue.forEach(t),m=f(A),v=o(A,"TH",{});var Ie=n(v);H=d(Ie,"World-Centric"),Ie.forEach(t),x=f(A),P=o(A,"TH",{});var Pe=n(P);L=d(Pe,"Ego-Centric"),Pe.forEach(t),W=f(A),V=o(A,"TH",{});var Ee=n(V);J=d(Ee,"Ego-Forward"),Ee.forEach(t),A.forEach(t),j.forEach(t),I=f(b),w=o(b,"TBODY",{});var Z=n(w);g=o(Z,"TR",{});var C=n(g);_=o(C,"TD",{});var fe=n(_);q=d(fe,"Collaborative Push Block"),fe.forEach(t),O=f(C),U=o(C,"TD",{});var Ve=n(U);ee=d(Ve,"222.6"),Ve.forEach(t),K=f(C),B=o(C,"TD",{});var xe=n(B);te=d(xe,"246.6"),xe.forEach(t),Y=f(C),M=o(C,"TD",{});var Be=n(M);de=d(Be,"230.5"),Be.forEach(t),C.forEach(t),ie=f(Z),y=o(Z,"TR",{});var k=n(y);re=o(k,"TD",{});var Re=n(re);pe=d(Re,"Dungeon Escape"),Re.forEach(t),oe=f(k),z=o(k,"TD",{});var Ae=n(z);_e=d(Ae,"14.15"),Ae.forEach(t),he=f(k),N=o(k,"TD",{});var we=n(N);be=d(we,"16.74"),we.forEach(t),me=f(k),G=o(k,"TD",{});var Le=n(G);ne=d(Le,"23.27"),Le.forEach(t),k.forEach(t),ye=f(Z),$=o(Z,"TR",{});var D=n($);S=o(D,"TD",{});var ce=n(S);$e=d(ce,"Planar Construction"),ce.forEach(t),ve=f(D),F=o(D,"TD",{});var ge=n(F);ae=d(ge,"110.5"),ge.forEach(t),Te=f(D),Q=o(D,"TD",{});var We=n(Q);se=d(We,"120.3"),We.forEach(t),He=f(D),X=o(D,"TD",{});var ke=n(X);le=d(ke,"149.9"),ke.forEach(t),D.forEach(t),Z.forEach(t)},m(b,j){s(b,p,j),i(p,h),i(h,E),i(E,T),i(h,m),i(h,v),i(v,H),i(h,x),i(h,P),i(P,L),i(h,W),i(h,V),i(V,J),s(b,I,j),s(b,w,j),i(w,g),i(g,_),i(_,q),i(g,O),i(g,U),i(U,ee),i(g,K),i(g,B),i(B,te),i(g,Y),i(g,M),i(M,de),i(w,ie),i(w,y),i(y,re),i(re,pe),i(y,oe),i(y,z),i(z,_e),i(y,he),i(y,N),i(N,be),i(y,me),i(y,G),i(G,ne),i(w,ye),i(w,$),i($,S),i(S,$e),i($,ve),i($,F),i(F,ae),i($,Te),i($,Q),i(Q,se),i($,He),i($,X),i(X,le)},p:fi,d(b){b&&t(p),b&&t(I),b&&t(w)}}}function Tr(R){let p,h,E,T,m,v,H,x,P,L,W,V,J,I,w,g,_,q,O,U,ee,K,B,te,Y,M,de,ie,y,re,pe,oe,z,_e,he,N,be,me,G,ne,ye,$,S,$e,ve,F,ae,Te,Q,se,He,X,le,b,j,A,ue,Ie,Pe,Ee,Z,C,fe,Ve,xe,Be,k,Re,Ae,we,Le,D,ce,ge,We,ke,je,tt,Ut,bt,it,zt,yt,Ke,rt,Nt,$t,ot,St,Tt,Ye,nt,jt,Ht,at,Kt,Vt,st,Ze,Pt,Qe,lt,Yt,At,Xe,ft,Zt,kt,ct,Qt,Ct,dt,Xt,Dt,Fe,It,Je,pt,Jt,xt,ht,ei,Bt,Ue,Rt,mt,ti,Lt,ze,Wt,et,vt,ii,qt,ut,ri,Ot,Ne,oi,Se,ni,ai,Mt;return M=new _t({props:{src:"./HEVMethod.png",alt:"HEV"}}),b=new _t({props:{src:"./POVFigure.png",alt:"POV"}}),Ze=new _t({props:{src:"./envs.PNG",alt:"envs"}}),Fe=new si({props:{$$slots:{default:[br]},$$scope:{ctx:R}}}),Ue=new si({props:{$$slots:{default:[yr]},$$scope:{ctx:R}}}),ze=new si({props:{$$slots:{default:[$r]},$$scope:{ctx:R}}}),{c(){p=r("h1"),h=r("a"),E=c("Herd’s Eye View: Improving Game AI Agent Learning with Collaborative Perception"),T=l(),m=r("h2"),v=r("a"),H=c("Introduction"),x=l(),P=r("p"),L=c("In the realm of game AI, agents traditionally have access to extensive global information from the game engine. While this configuration assists in efficient decision-making, it doesn’t accurately represent the restrictions encountered by AI applications outside of gaming. Game AI techniques that rely predominantly on game engine data may limit their potential contribution to broader AI applications."),W=l(),V=r("p"),J=c("To address these challenges, we present a novel paradigm named Herd’s Eye View (HEV) that adopts a global perspective derived from multiple agents to boost decision-making capabilities of reinforcement learning (RL) agents in multi-agent environments. The HEV approach utilizes cooperative perception to empower RL agents with a global reasoning ability, enhancing their decision-making."),I=l(),w=r("h2"),g=r("a"),_=c("Herd’s Eye View: A New Perspective"),q=l(),O=r("p"),U=c("Drawing inspiration from autonomous vehicles research and the Bird’s Eye View (BEV) model, we propose a similar intermediary representation for game AI: the Herd’s Eye View (HEV) model. Unlike the BEV’s individual-centric perspective, the HEV model offers a shared world-centric perception derived from multiple agents. This shared perception model aligns closer to real-world AI applications, where multiple systems often work together to understand and navigate their environment."),ee=l(),K=r("p"),B=c("The HEV model presents dual advantages. First, it mirrors the constraints faced by AI outside of gaming, contributing to the development of more believable AI behavior in games. Second, it alleviates the computational demands associated with the BEV model, where each agent maintains its own unique view of the environment. Instead, only a single shared global view is utilized."),te=l(),Y=r("p"),qe(M.$$.fragment),de=l(),ie=r("h2"),y=r("a"),re=c("Perspective Views"),pe=l(),oe=r("h3"),z=r("a"),_e=c("World-Centric (Herd’s Eye View)"),he=l(),N=r("p"),be=c("The world-centric view, also known as the Herd’s Eye View (HEV), offers a shared global perception derived from multiple agents. This view allows all agents to have a comprehensive understanding of the entire environment, which can enhance their decision-making capabilities and coordination."),me=l(),G=r("h3"),ne=r("a"),ye=c("Ego-Centric"),$=l(),S=r("p"),$e=c("The ego-centric view is an individual-centric perspective where each agent maintains its own unique view of the environment. This view can limit the agents’ understanding of the environment to their immediate surroundings."),ve=l(),F=r("h3"),ae=r("a"),Te=c("Ego-Forward"),Q=l(),se=r("p"),He=c("The ego-forward view is a variation of the ego-centric view where the perspective is always forward-facing relative to the agent. This view can be limiting as it restricts the agents’ perception to only what is in front of them."),X=l(),le=r("p"),qe(b.$$.fragment),j=l(),A=r("h2"),ue=r("a"),Ie=c("Key Contributions"),Pe=l(),Ee=r("p"),Z=c("Our work makes the following contributions:"),C=l(),fe=r("ol"),Ve=r("li"),xe=c("We propose a baseline model for performing semantic segmentation in a fixed ”HEV” world-view."),Be=l(),k=r("li"),Re=c("We demonstrate the effectiveness of the HEV fixed world viewpoint in improving collaborative perception and MARL in games."),Ae=l(),we=r("p"),Le=c("Our exploration of more realistic perception models and the application of reinforcement learning provides```markdown\nsignificant insights for game AI development, stressing the wider applicability of these techniques beyond the gaming industry."),D=l(),ce=r("h2"),ge=r("a"),We=c("Environments"),ke=l(),je=r("h3"),tt=r("a"),Ut=c("Collaborative Push Block"),bt=l(),it=r("p"),zt=c("In this environment, agents are required to push white blocks to a designated green area. The challenge here is that larger blocks require more agents to push, necessitating cooperation and coordination among the agents."),yt=l(),Ke=r("h3"),rt=r("a"),Nt=c("Dungeon Escape"),$t=l(),ot=r("p"),St=c("This environment presents a unique challenge where agents must sacrifice one of their own to take down a green dragon and obtain a key. The rest of the team must then use this key to escape the dungeon. This scenario tests the agents’ ability to strategize and make sacrifices for the greater good."),Tt=l(),Ye=r("h3"),nt=r("a"),jt=c("Planar Construction"),Ht=l(),at=r("p"),Kt=c("In the Planar Construction environment, six agents collaborate to push red pucks into desired positions. The positions can be random or static, and are observed via a Grid-Sensor, similar to the push block environment. This environment tests the agents’ ability to work together in a more complex and dynamic setting."),Vt=l(),st=r("p"),qe(Ze.$$.fragment),Pt=l(),Qe=r("h2"),lt=r("a"),Yt=c("Experiments"),At=l(),Xe=r("h3"),ft=r("a"),Zt=c("Accuracy of World-Centric Predictions vs Ego-Centric"),kt=l(),ct=r("p"),Qt=c("The first experiment compares the accuracy of world-centric predictions using HEV with ego-centric predictions using BEV. We conduct this experiment in three simulated Multi-Agent Reinforcement Learning (MARL) game environments."),Ct=l(),dt=r("p"),Xt=c("HEV-CVT validation IoU results per coordinate frame in each environment (higher is better)"),Dt=l(),qe(Fe.$$.fragment),It=l(),Je=r("h3"),pt=r("a"),Jt=c("Efficiency of Perspective View Policies Learned"),xt=l(),ht=r("p"),ei=c("The second experiment evaluates the efficiency of policies learned by RL agents trained on HEV views compared to those trained on BEV views."),Bt=l(),qe(Ue.$$.fragment),Rt=l(),mt=r("p"),ti=c("Table 2: MA-POCA mean episode length per coordinate frame in each environment (lower is better)"),Lt=l(),qe(ze.$$.fragment),Wt=l(),et=r("h2"),vt=r("a"),ii=c("Conclusion"),qt=l(),ut=r("p"),ri=c("The Herd’s Eye View (HEV) framework offers a superior perception model in MARL environments, providing agents with a more comprehensive understanding of their surroundings, leading to improved decision-making and better overall performance. By using the HEV world-view collaborative perception problem, we show that the accuracy of collaborative perception models is significantly improved, leading to better IoU scores. Our work opens up new possibilities for advanced perception models in MARL, which can greatly enhance the performance of multi-robot systems by enabling better collaboration and coordination."),Ot=l(),Ne=r("p"),oi=c("The code for this project is available on "),Se=r("a"),ni=c("GitHub"),ai=c("."),this.h()},l(e){p=o(e,"H1",{id:!0});var a=n(p);h=o(a,"A",{href:!0});var Et=n(h);E=d(Et,"Herd’s Eye View: Improving Game AI Agent Learning with Collaborative Perception"),Et.forEach(t),a.forEach(t),T=f(e),m=o(e,"H2",{id:!0});var wt=n(m);v=o(wt,"A",{href:!0});var gt=n(v);H=d(gt,"Introduction"),gt.forEach(t),wt.forEach(t),x=f(e),P=o(e,"P",{});var ci=n(P);L=d(ci,"In the realm of game AI, agents traditionally have access to extensive global information from the game engine. While this configuration assists in efficient decision-making, it doesn’t accurately represent the restrictions encountered by AI applications outside of gaming. Game AI techniques that rely predominantly on game engine data may limit their potential contribution to broader AI applications."),ci.forEach(t),W=f(e),V=o(e,"P",{});var di=n(V);J=d(di,"To address these challenges, we present a novel paradigm named Herd’s Eye View (HEV) that adopts a global perspective derived from multiple agents to boost decision-making capabilities of reinforcement learning (RL) agents in multi-agent environments. The HEV approach utilizes cooperative perception to empower RL agents with a global reasoning ability, enhancing their decision-making."),di.forEach(t),I=f(e),w=o(e,"H2",{id:!0});var pi=n(w);g=o(pi,"A",{href:!0});var hi=n(g);_=d(hi,"Herd’s Eye View: A New Perspective"),hi.forEach(t),pi.forEach(t),q=f(e),O=o(e,"P",{});var mi=n(O);U=d(mi,"Drawing inspiration from autonomous vehicles research and the Bird’s Eye View (BEV) model, we propose a similar intermediary representation for game AI: the Herd’s Eye View (HEV) model. Unlike the BEV’s individual-centric perspective, the HEV model offers a shared world-centric perception derived from multiple agents. This shared perception model aligns closer to real-world AI applications, where multiple systems often work together to understand and navigate their environment."),mi.forEach(t),ee=f(e),K=o(e,"P",{});var vi=n(K);B=d(vi,"The HEV model presents dual advantages. First, it mirrors the constraints faced by AI outside of gaming, contributing to the development of more believable AI behavior in games. Second, it alleviates the computational demands associated with the BEV model, where each agent maintains its own unique view of the environment. Instead, only a single shared global view is utilized."),vi.forEach(t),te=f(e),Y=o(e,"P",{});var ui=n(Y);Oe(M.$$.fragment,ui),ui.forEach(t),de=f(e),ie=o(e,"H2",{id:!0});var Ei=n(ie);y=o(Ei,"A",{href:!0});var wi=n(y);re=d(wi,"Perspective Views"),wi.forEach(t),Ei.forEach(t),pe=f(e),oe=o(e,"H3",{id:!0});var gi=n(oe);z=o(gi,"A",{href:!0});var _i=n(z);_e=d(_i,"World-Centric (Herd’s Eye View)"),_i.forEach(t),gi.forEach(t),he=f(e),N=o(e,"P",{});var bi=n(N);be=d(bi,"The world-centric view, also known as the Herd’s Eye View (HEV), offers a shared global perception derived from multiple agents. This view allows all agents to have a comprehensive understanding of the entire environment, which can enhance their decision-making capabilities and coordination."),bi.forEach(t),me=f(e),G=o(e,"H3",{id:!0});var yi=n(G);ne=o(yi,"A",{href:!0});var $i=n(ne);ye=d($i,"Ego-Centric"),$i.forEach(t),yi.forEach(t),$=f(e),S=o(e,"P",{});var Ti=n(S);$e=d(Ti,"The ego-centric view is an individual-centric perspective where each agent maintains its own unique view of the environment. This view can limit the agents’ understanding of the environment to their immediate surroundings."),Ti.forEach(t),ve=f(e),F=o(e,"H3",{id:!0});var Hi=n(F);ae=o(Hi,"A",{href:!0});var Vi=n(ae);Te=d(Vi,"Ego-Forward"),Vi.forEach(t),Hi.forEach(t),Q=f(e),se=o(e,"P",{});var Pi=n(se);He=d(Pi,"The ego-forward view is a variation of the ego-centric view where the perspective is always forward-facing relative to the agent. This view can be limiting as it restricts the agents’ perception to only what is in front of them."),Pi.forEach(t),X=f(e),le=o(e,"P",{});var Ai=n(le);Oe(b.$$.fragment,Ai),Ai.forEach(t),j=f(e),A=o(e,"H2",{id:!0});var ki=n(A);ue=o(ki,"A",{href:!0});var Ci=n(ue);Ie=d(Ci,"Key Contributions"),Ci.forEach(t),ki.forEach(t),Pe=f(e),Ee=o(e,"P",{});var Di=n(Ee);Z=d(Di,"Our work makes the following contributions:"),Di.forEach(t),C=f(e),fe=o(e,"OL",{});var Gt=n(fe);Ve=o(Gt,"LI",{});var Ii=n(Ve);xe=d(Ii,"We propose a baseline model for performing semantic segmentation in a fixed ”HEV” world-view."),Ii.forEach(t),Be=f(Gt),k=o(Gt,"LI",{});var xi=n(k);Re=d(xi,"We demonstrate the effectiveness of the HEV fixed world viewpoint in improving collaborative perception and MARL in games."),xi.forEach(t),Gt.forEach(t),Ae=f(e),we=o(e,"P",{});var Bi=n(we);Le=d(Bi,"Our exploration of more realistic perception models and the application of reinforcement learning provides```markdown\nsignificant insights for game AI development, stressing the wider applicability of these techniques beyond the gaming industry."),Bi.forEach(t),D=f(e),ce=o(e,"H2",{id:!0});var Ri=n(ce);ge=o(Ri,"A",{href:!0});var Li=n(ge);We=d(Li,"Environments"),Li.forEach(t),Ri.forEach(t),ke=f(e),je=o(e,"H3",{id:!0});var Wi=n(je);tt=o(Wi,"A",{href:!0});var qi=n(tt);Ut=d(qi,"Collaborative Push Block"),qi.forEach(t),Wi.forEach(t),bt=f(e),it=o(e,"P",{});var Oi=n(it);zt=d(Oi,"In this environment, agents are required to push white blocks to a designated green area. The challenge here is that larger blocks require more agents to push, necessitating cooperation and coordination among the agents."),Oi.forEach(t),yt=f(e),Ke=o(e,"H3",{id:!0});var Mi=n(Ke);rt=o(Mi,"A",{href:!0});var Gi=n(rt);Nt=d(Gi,"Dungeon Escape"),Gi.forEach(t),Mi.forEach(t),$t=f(e),ot=o(e,"P",{});var Fi=n(ot);St=d(Fi,"This environment presents a unique challenge where agents must sacrifice one of their own to take down a green dragon and obtain a key. The rest of the team must then use this key to escape the dungeon. This scenario tests the agents’ ability to strategize and make sacrifices for the greater good."),Fi.forEach(t),Tt=f(e),Ye=o(e,"H3",{id:!0});var Ui=n(Ye);nt=o(Ui,"A",{href:!0});var zi=n(nt);jt=d(zi,"Planar Construction"),zi.forEach(t),Ui.forEach(t),Ht=f(e),at=o(e,"P",{});var Ni=n(at);Kt=d(Ni,"In the Planar Construction environment, six agents collaborate to push red pucks into desired positions. The positions can be random or static, and are observed via a Grid-Sensor, similar to the push block environment. This environment tests the agents’ ability to work together in a more complex and dynamic setting."),Ni.forEach(t),Vt=f(e),st=o(e,"P",{});var Si=n(st);Oe(Ze.$$.fragment,Si),Si.forEach(t),Pt=f(e),Qe=o(e,"H2",{id:!0});var ji=n(Qe);lt=o(ji,"A",{href:!0});var Ki=n(lt);Yt=d(Ki,"Experiments"),Ki.forEach(t),ji.forEach(t),At=f(e),Xe=o(e,"H3",{id:!0});var Yi=n(Xe);ft=o(Yi,"A",{href:!0});var Zi=n(ft);Zt=d(Zi,"Accuracy of World-Centric Predictions vs Ego-Centric"),Zi.forEach(t),Yi.forEach(t),kt=f(e),ct=o(e,"P",{});var Qi=n(ct);Qt=d(Qi,"The first experiment compares the accuracy of world-centric predictions using HEV with ego-centric predictions using BEV. We conduct this experiment in three simulated Multi-Agent Reinforcement Learning (MARL) game environments."),Qi.forEach(t),Ct=f(e),dt=o(e,"P",{});var Xi=n(dt);Xt=d(Xi,"HEV-CVT validation IoU results per coordinate frame in each environment (higher is better)"),Xi.forEach(t),Dt=f(e),Oe(Fe.$$.fragment,e),It=f(e),Je=o(e,"H3",{id:!0});var Ji=n(Je);pt=o(Ji,"A",{href:!0});var er=n(pt);Jt=d(er,"Efficiency of Perspective View Policies Learned"),er.forEach(t),Ji.forEach(t),xt=f(e),ht=o(e,"P",{});var tr=n(ht);ei=d(tr,"The second experiment evaluates the efficiency of policies learned by RL agents trained on HEV views compared to those trained on BEV views."),tr.forEach(t),Bt=f(e),Oe(Ue.$$.fragment,e),Rt=f(e),mt=o(e,"P",{});var ir=n(mt);ti=d(ir,"Table 2: MA-POCA mean episode length per coordinate frame in each environment (lower is better)"),ir.forEach(t),Lt=f(e),Oe(ze.$$.fragment,e),Wt=f(e),et=o(e,"H2",{id:!0});var rr=n(et);vt=o(rr,"A",{href:!0});var or=n(vt);ii=d(or,"Conclusion"),or.forEach(t),rr.forEach(t),qt=f(e),ut=o(e,"P",{});var nr=n(ut);ri=d(nr,"The Herd’s Eye View (HEV) framework offers a superior perception model in MARL environments, providing agents with a more comprehensive understanding of their surroundings, leading to improved decision-making and better overall performance. By using the HEV world-view collaborative perception problem, we show that the accuracy of collaborative perception models is significantly improved, leading to better IoU scores. Our work opens up new possibilities for advanced perception models in MARL, which can greatly enhance the performance of multi-robot systems by enabling better collaboration and coordination."),nr.forEach(t),Ot=f(e),Ne=o(e,"P",{});var Ft=n(Ne);oi=d(Ft,"The code for this project is available on "),Se=o(Ft,"A",{href:!0,rel:!0,target:!0});var ar=n(Se);ni=d(ar,"GitHub"),ar.forEach(t),ai=d(Ft,"."),Ft.forEach(t),this.h()},h(){u(h,"href","#herds-eye-view-improving-game-ai-agent-learning-with-collaborative-perception"),u(p,"id","herds-eye-view-improving-game-ai-agent-learning-with-collaborative-perception"),u(v,"href","#introduction"),u(m,"id","introduction"),u(g,"href","#herds-eye-view-a-new-perspective"),u(w,"id","herds-eye-view-a-new-perspective"),u(y,"href","#perspective-views"),u(ie,"id","perspective-views"),u(z,"href","#world-centric-herds-eye-view"),u(oe,"id","world-centric-herds-eye-view"),u(ne,"href","#ego-centric"),u(G,"id","ego-centric"),u(ae,"href","#ego-forward"),u(F,"id","ego-forward"),u(ue,"href","#key-contributions"),u(A,"id","key-contributions"),u(ge,"href","#environments"),u(ce,"id","environments"),u(tt,"href","#collaborative-push-block"),u(je,"id","collaborative-push-block"),u(rt,"href","#dungeon-escape"),u(Ke,"id","dungeon-escape"),u(nt,"href","#planar-construction"),u(Ye,"id","planar-construction"),u(lt,"href","#experiments"),u(Qe,"id","experiments"),u(ft,"href","#accuracy-of-world-centric-predictions-vs-ego-centric"),u(Xe,"id","accuracy-of-world-centric-predictions-vs-ego-centric"),u(pt,"href","#efficiency-of-perspective-view-policies-learned"),u(Je,"id","efficiency-of-perspective-view-policies-learned"),u(vt,"href","#conclusion"),u(et,"id","conclusion"),u(Se,"href","https://github.com/andrewnash/Herds-Eye-View"),u(Se,"rel","nofollow noopener noreferrer external"),u(Se,"target","_blank")},m(e,a){s(e,p,a),i(p,h),i(h,E),s(e,T,a),s(e,m,a),i(m,v),i(v,H),s(e,x,a),s(e,P,a),i(P,L),s(e,W,a),s(e,V,a),i(V,J),s(e,I,a),s(e,w,a),i(w,g),i(g,_),s(e,q,a),s(e,O,a),i(O,U),s(e,ee,a),s(e,K,a),i(K,B),s(e,te,a),s(e,Y,a),Me(M,Y,null),s(e,de,a),s(e,ie,a),i(ie,y),i(y,re),s(e,pe,a),s(e,oe,a),i(oe,z),i(z,_e),s(e,he,a),s(e,N,a),i(N,be),s(e,me,a),s(e,G,a),i(G,ne),i(ne,ye),s(e,$,a),s(e,S,a),i(S,$e),s(e,ve,a),s(e,F,a),i(F,ae),i(ae,Te),s(e,Q,a),s(e,se,a),i(se,He),s(e,X,a),s(e,le,a),Me(b,le,null),s(e,j,a),s(e,A,a),i(A,ue),i(ue,Ie),s(e,Pe,a),s(e,Ee,a),i(Ee,Z),s(e,C,a),s(e,fe,a),i(fe,Ve),i(Ve,xe),i(fe,Be),i(fe,k),i(k,Re),s(e,Ae,a),s(e,we,a),i(we,Le),s(e,D,a),s(e,ce,a),i(ce,ge),i(ge,We),s(e,ke,a),s(e,je,a),i(je,tt),i(tt,Ut),s(e,bt,a),s(e,it,a),i(it,zt),s(e,yt,a),s(e,Ke,a),i(Ke,rt),i(rt,Nt),s(e,$t,a),s(e,ot,a),i(ot,St),s(e,Tt,a),s(e,Ye,a),i(Ye,nt),i(nt,jt),s(e,Ht,a),s(e,at,a),i(at,Kt),s(e,Vt,a),s(e,st,a),Me(Ze,st,null),s(e,Pt,a),s(e,Qe,a),i(Qe,lt),i(lt,Yt),s(e,At,a),s(e,Xe,a),i(Xe,ft),i(ft,Zt),s(e,kt,a),s(e,ct,a),i(ct,Qt),s(e,Ct,a),s(e,dt,a),i(dt,Xt),s(e,Dt,a),Me(Fe,e,a),s(e,It,a),s(e,Je,a),i(Je,pt),i(pt,Jt),s(e,xt,a),s(e,ht,a),i(ht,ei),s(e,Bt,a),Me(Ue,e,a),s(e,Rt,a),s(e,mt,a),i(mt,ti),s(e,Lt,a),Me(ze,e,a),s(e,Wt,a),s(e,et,a),i(et,vt),i(vt,ii),s(e,qt,a),s(e,ut,a),i(ut,ri),s(e,Ot,a),s(e,Ne,a),i(Ne,oi),i(Ne,Se),i(Se,ni),i(Ne,ai),Mt=!0},p(e,a){const Et={};a&2&&(Et.$$scope={dirty:a,ctx:e}),Fe.$set(Et);const wt={};a&2&&(wt.$$scope={dirty:a,ctx:e}),Ue.$set(wt);const gt={};a&2&&(gt.$$scope={dirty:a,ctx:e}),ze.$set(gt)},i(e){Mt||(Ce(M.$$.fragment,e),Ce(b.$$.fragment,e),Ce(Ze.$$.fragment,e),Ce(Fe.$$.fragment,e),Ce(Ue.$$.fragment,e),Ce(ze.$$.fragment,e),Mt=!0)},o(e){De(M.$$.fragment,e),De(b.$$.fragment,e),De(Ze.$$.fragment,e),De(Fe.$$.fragment,e),De(Ue.$$.fragment,e),De(ze.$$.fragment,e),Mt=!1},d(e){e&&t(p),e&&t(T),e&&t(m),e&&t(x),e&&t(P),e&&t(W),e&&t(V),e&&t(I),e&&t(w),e&&t(q),e&&t(O),e&&t(ee),e&&t(K),e&&t(te),e&&t(Y),Ge(M),e&&t(de),e&&t(ie),e&&t(pe),e&&t(oe),e&&t(he),e&&t(N),e&&t(me),e&&t(G),e&&t($),e&&t(S),e&&t(ve),e&&t(F),e&&t(Q),e&&t(se),e&&t(X),e&&t(le),Ge(b),e&&t(j),e&&t(A),e&&t(Pe),e&&t(Ee),e&&t(C),e&&t(fe),e&&t(Ae),e&&t(we),e&&t(D),e&&t(ce),e&&t(ke),e&&t(je),e&&t(bt),e&&t(it),e&&t(yt),e&&t(Ke),e&&t($t),e&&t(ot),e&&t(Tt),e&&t(Ye),e&&t(Ht),e&&t(at),e&&t(Vt),e&&t(st),Ge(Ze),e&&t(Pt),e&&t(Qe),e&&t(At),e&&t(Xe),e&&t(kt),e&&t(ct),e&&t(Ct),e&&t(dt),e&&t(Dt),Ge(Fe,e),e&&t(It),e&&t(Je),e&&t(xt),e&&t(ht),e&&t(Bt),Ge(Ue,e),e&&t(Rt),e&&t(mt),e&&t(Lt),Ge(ze,e),e&&t(Wt),e&&t(et),e&&t(qt),e&&t(ut),e&&t(Ot),e&&t(Ne)}}}function Hr(R){let p,h;const E=[R[0],fr];let T={$$slots:{default:[Tr]},$$scope:{ctx:R}};for(let m=0;m<E.length;m+=1)T=li(T,E[m]);return p=new wr({props:T}),{c(){qe(p.$$.fragment)},l(m){Oe(p.$$.fragment,m)},m(m,v){Me(p,m,v),h=!0},p(m,[v]){const H=v&1?Er(E,[v&1&&sr(m[0]),v&0&&sr(fr)]):{};v&2&&(H.$$scope={dirty:v,ctx:m}),p.$set(H)},i(m){h||(Ce(p.$$.fragment,m),h=!0)},o(m){De(p.$$.fragment,m),h=!1},d(m){Ge(p,m)}}}const fr={title:"My Research - Herds Eye View",image:"/hev/envs.PNG",alt:"HEV",created:"2023-04-20T00:00:00.000Z",tags:["Computer Vision","Transformer","RL","Unity","PyTorch","C#"],updated:"2023-06-16T14:41:12.054Z",images:[],slug:"/hev/+page.svelte.md",path:"/hev",toc:[{depth:1,title:"Herd’s Eye View: Improving Game AI Agent Learning with Collaborative Perception",slug:"herds-eye-view-improving-game-ai-agent-learning-with-collaborative-perception"},{depth:2,title:"Introduction",slug:"introduction"},{depth:2,title:"Herd’s Eye View: A New Perspective",slug:"herds-eye-view-a-new-perspective"},{depth:2,title:"Perspective Views",slug:"perspective-views"},{depth:3,title:"World-Centric (Herd’s Eye View)",slug:"world-centric-herds-eye-view"},{depth:3,title:"Ego-Centric",slug:"ego-centric"},{depth:3,title:"Ego-Forward",slug:"ego-forward"},{depth:2,title:"Key Contributions",slug:"key-contributions"},{depth:2,title:"Environments",slug:"environments"},{depth:3,title:"Collaborative Push Block",slug:"collaborative-push-block"},{depth:3,title:"Dungeon Escape",slug:"dungeon-escape"},{depth:3,title:"Planar Construction",slug:"planar-construction"},{depth:2,title:"Experiments",slug:"experiments"},{depth:3,title:"Accuracy of World-Centric Predictions vs Ego-Centric",slug:"accuracy-of-world-centric-predictions-vs-ego-centric"},{depth:3,title:"Efficiency of Perspective View Policies Learned",slug:"efficiency-of-perspective-view-policies-learned"},{depth:2,title:"Conclusion",slug:"conclusion"}]};function Vr(R,p,h){return R.$$set=E=>{h(0,p=li(li({},p),lr(E)))},p=lr(p),[p]}class Cr extends cr{constructor(p){super(),dr(this,p,Vr,Hr,pr,{})}}export{Cr as component};
