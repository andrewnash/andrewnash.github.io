import{S as Eo,i as Io,s as Uo,U as gi,y as c,z as v,A as g,V as Po,W as Ro,g as w,d as y,B as b,X as ko,k as a,q as m,a as l,l as n,m as r,r as d,h as t,c as u,n as f,T as vi,b as o,D as s,E as Vo}from"../chunks/index.17f79c6b.js";import{P as xo}from"../chunks/post_layout.3a62565a.js";import{I as $}from"../chunks/footer.8df9f7af.js";function Lo(ae){let p,_,S,E,h,k,U,ne,A,wi,Ge,P,re,Ft,Qe,se,Ot,Ze,le,V,Xe,x,ue,Ht,Be,fe,Wt,Je,me,L,Ke,M,de,Ct,et,pe,jt,tt,z,he,qt,it,ce,Nt,ot,ve,D,at,F,ge,Yt,nt,we,Gt,rt,O,Qt,H,st,W,ye,Zt,lt,be,Xt,ut,_e,C,ft,$e,Bt,mt,j,Ae,Jt,dt,Te,Kt,pt,Re,q,ht,N,ke,ei,ct,Se,ti,vt,Y,Ee,ii,gt,Ie,oi,wt,Ue,G,yt,Pe,ai,bt,Q,Ve,ni,_t,xe,ri,$t,Le,Z,At,X,Me,si,Tt,ze,li,Rt,De,B,kt,Fe,ui,St,Oe,J,Et,K,He,fi,It,We,mi,Ut,Ce,ee,Pt,je,te,Vt,ie,qe,di,xt,Ne,pi,Lt,Ye,oe,Mt,I,T,yi,hi,R,bi,zt;return V=new $({props:{src:"./srauv_ocean.png",alt:"srauv"}}),L=new $({props:{src:"./mech.png",alt:"mech"}}),D=new $({props:{src:"./unity_sim.png",alt:"unity_sim"}}),H=new $({props:{src:"./49_tanks.gif",alt:"tanks"}}),C=new $({props:{src:"./auto_pilot.png",alt:"system"}}),q=new $({props:{src:"./autnomous.gif",alt:"autnomous_flying"}}),G=new $({props:{src:"./tank_driving.gif",alt:"tank_driving"}}),Z=new $({props:{src:"./reward.PNG",alt:"srauv_in_tank"}}),B=new $({props:{src:"./srauv_in_tank.png",alt:"srauv_in_tank"}}),J=new $({props:{src:"./april_tank.gif",alt:"april_tags"}}),ee=new $({props:{src:"./testing1.gif",alt:"testing1"}}),te=new $({props:{src:"./testing2.gif",alt:"testing2"}}),oe=new $({props:{src:"./srauv.png",alt:"srauv"}}),{c(){p=a("h1"),_=a("a"),S=m("Navigating the Depths with RL"),E=l(),h=a("p"),k=m("Hello folks! My name is Andrew Nash, and I’ve been heavily involved in developing a software system for an autonomous subsea vehicle, the SRAUV, as part of my engineering capstone project. In this blog post, I’ll detail the work I did on the Autopilot/AI perception, navigation, and control. Strap in for an exciting journey deep beneath the waves!"),U=l(),ne=a("center"),A=a("iframe"),Ge=l(),P=a("h2"),re=a("a"),Ft=m("Introduction"),Qe=l(),se=a("p"),Ot=m("In my capstone project, my team and I have tackled a pervasive problem in offshore oil and gas operations - consistent and reliable subsea monitoring. To address this, we’ve designed a prototype Subsea Resident Autonomous Underwater Vehicle (SRAUV). Our innovative SRAUV operates indefinitely under the sea, utilizing a dock on the seafloor that facilitates wireless charging and communication. It embarks on scheduled or on-demand inspection missions, collects vital data, and then returns to the dock to transmit this information. Through this approach, we’re ushering in a new era of advanced offshore operations, with potential upgrades such as real-time anomaly detection that promises enhanced efficiency and safety."),Ze=l(),le=a("p"),c(V.$$.fragment),Xe=l(),x=a("h2"),ue=a("a"),Ht=m("Vehicle"),Be=l(),fe=a("p"),Wt=m("My exceptional teammates ensured the SRAUV’s mechanical design adhered to principles of hydrodynamics, stability, and compactness, resulting in a sleek, efficient design. Fitting within the original constraints of a 2-foot length and width, the vehicle’s overall mass is approximately 15 kg, as illustrated below. The team cleverly configured the thrusters to offer a versatile maneuverability, giving the SRAUV 5 degrees of freedom, though effectively using only 4 - excluding pitch and roll - for the project’s final demonstration."),Je=l(),me=a("p"),c(L.$$.fragment),Ke=l(),M=a("h2"),de=a("a"),Ct=m("Software Overview"),et=l(),pe=a("p"),jt=m("The SRAUV software is a culmination of collaborative effort, using Unity 3D game engine for fast prototyping, extensive machine learning support, and engaging visual feedback. The interface is an all-in-one GUI Simulator, allowing the user to control the vehicle, visualize thruster direction feedback, and simulate various environmental conditions. All software for the project is openly available on Github."),tt=l(),z=a("h2"),he=a("a"),qt=m("Simulator"),it=l(),ce=a("p"),Nt=m("We developed a Unity-based simulator that accurately mimics the mechanics of the SRAUV. This simulator applies individual thrust vectors at each thruster’s respective angle, fine-tuning its underwater movement, often referred to as “driftyness”, to reflect the real-world feedback from experienced pilots. The simulation updates in response to these thrust commands, providing new sensor data which includes the SRAUV’s position and the relative locations of assets in the environment. This helped us efficiently and accurately test our autonomous flying solutions."),ot=l(),ve=a("p"),c(D.$$.fragment),at=l(),F=a("h2"),ge=a("a"),Yt=m("Model Training"),nt=l(),we=a("p"),Gt=m("In addition to the main simulator, I created a lighter “ML Tank” for training the DRL algorithms. This environment include a significantly reduced polygon count, a necessary optimization since DRL algorithms are computationally expensive to simulate. Further speed enhancements were achieved by including multiple ML Tanks, allowing multiple agents to collect observations simultaneously, this can be seen below."),rt=l(),O=a("p"),Qt=m("Screenshot of 49 ML Tanks training simultaneously within the Unity environment. "),c(H.$$.fragment),st=l(),W=a("h2"),ye=a("a"),Zt=m("Autonomous Strategy"),lt=l(),be=a("p"),Xt=m("As the core designer of the Autopilot, I initially considered employing Q-learning for the SRAUV’s control system. This approach was favored due to its popularity and well-documented success. However, throughout the model selection process, I found Deep Reinforcement Learning (DRL) to be the most effective at driving in the simulator. I utilized Unity ML-Agents Toolkit for DRL training within the Unity simulator, leading to rapid experimentation with the SRAUV’s Flight Computer and Vision System."),ut=l(),_e=a("p"),c(C.$$.fragment),ft=l(),$e=a("p"),Bt=m("With DRL, I designed an end-to-end solution for our control system. This allowed observations to go into the model and thrust control to emerge, providing optimal control of the SRAUV through a variety of unpredictable scenarios."),mt=l(),j=a("h2"),Ae=a("a"),Jt=m("Model Development"),dt=l(),Te=a("p"),Kt=m("For model training, I used a simplified subproblem - driving from a random X, Y, Z position to another random position as quickly as possible. I then tested various DRL solutions on these subproblems, progressively introducing complexity until the environment closely mirrored real life. This iterative approach allowed for significant improvement during model development."),pt=l(),Re=a("p"),c(q.$$.fragment),ht=l(),N=a("h3"),ke=a("a"),ei=m("Model Selection"),ct=l(),Se=a("p"),ti=m("For the SRAUV control system, I considered both discrete and continuous action spaces. While a continuous action space implies real numbers for the DRL model output, a discrete one implies a fixed set of categories. I experimentally found discrete to be more effective during training. I benchmarked the Soft Actor-Critic (SAC) model against a Proximal Policy Optimization (PPO). SAC took a considerable time to train and often failed to reach the goal consistently. On the other hand, PPO found the optimal driving strategy faster and more effectively, which led to its selection for further development."),vt=l(),Y=a("h3"),Ee=a("a"),ii=m("Reward Structure"),gt=l(),Ie=a("p"),oi=m("The reward structure found to best solve the easy environment was relatively straightforward, consisting of three different rewards. The first reward was assigned as +1 when the agent arrived within a predefined tolerance of its goal waypoint. The second reward was the negative inverse of maximum time steps -0.002, this small negative reward assigned every timestep encouraged the model to drive to the goal as fast as possible. Finally, the third reward sets the overall reward to -1 if the agent collides with anything, this also resets the run for increased training speed."),wt=l(),Ue=a("p"),c(G.$$.fragment),yt=l(),Pe=a("p"),ai=m("In further iterations of the training environment, I found that the agent would sometimes overshoot its goal position and accidentally crash into a wall, to help mitigate this problem a stacking negative reward was assigned whenever the agent exceeded 0.3 m/s velocity in any axis of movement. This resulted in reduced crashing and taught the model to operate at a reduced speed when nearing its goal position."),bt=l(),Q=a("h3"),Ve=a("a"),ni=m("Curriculum Learning"),_t=l(),xe=a("p"),ri=m("Since the PPO algorithm struggled to converge when tolerances of less than 40cm were used in training, I utilized curriculum learning to overcome this challenge. This technique allowed me to progressively decrease the tolerance required to reach the goal waypoint during training, significantly enhancing the model’s performance."),$t=l(),Le=a("p"),c(Z.$$.fragment),At=l(),X=a("h2"),Me=a("a"),si=m("Localization"),Tt=l(),ze=a("p"),li=m("After our initial approach for localization, which relied on a combination of a pressure sensor, time-of-flight (TOF) sensors, and an inertial-measurement-unit (IMU), failed to meet expectations, my team and I were forced to quickly pivot. Due to issues ranging from faulty TOF sensors, an unsteady IMU, and a malfunctioning depth sensor, we had to reimagine our solution, leading us to the exploration of a computer vision-based approach. Utilizing AprilTags, similar to simple QR codes, we established a localization system that successfully tracked the position and rotation of the SRAUV relative to the tank. We installed a grid of these AprilTags on the tank floor, ensuring the bottom camera of the SRAUV was able to constantly locate a tag, significantly improving our vehicle’s spatial awareness. Impressively, this solution not only worked exceedingly well during testing, achieving an accuracy of about +-5 cm in position and +-5 degrees in heading, but it was also considerably more cost-effective than our initial sensor plan."),Rt=l(),De=a("p"),c(B.$$.fragment),kt=l(),Fe=a("p"),ui=m("The AprilTags detection in the ML Tank was another area I optimized. Running the vision recognition on all 49 tanks in a single environment would have been computationally prohibitive. To address this, I added a camera with matching intrinsic parameters and position to the SRAUV’s bottom pi cam in the simulator. This allowed me to check if the AprilTag was in the camera’s frame during each timestep, providing the agent with the most recently known position & velocity estimate, simulating the noisy conditions it would face in the real world."),St=l(),Oe=a("p"),c(J.$$.fragment),Et=l(),K=a("h2"),He=a("a"),fi=m("Real World Testing"),It=l(),We=a("p"),mi=m("Below you can see some videos of real world testing the SRAUV in a tank at Memorial University."),Ut=l(),Ce=a("p"),c(ee.$$.fragment),Pt=l(),je=a("p"),c(te.$$.fragment),Vt=l(),ie=a("h2"),qe=a("a"),di=m("Conclusion"),xt=l(),Ne=a("p"),pi=m("Wrapping up, I am incredibly proud of the progress and achievements made by my team and myself throughout our capstone project. We successfully designed, implemented, and tested a neural network-based autopilot for our Subsea Resident AUV (SRAUV) prototype. This enabled the SRAUV to perform autonomous missions, including navigating a prescribed route and redocking, a significant milestone for us. Even though we faced setbacks, such as hardware failures limiting the testing of an improved autopilot model, these challenges only fueled our drive to innovate. Our commitment paid off, winning us first place in all capstone award categories. We believe our work has set a strong foundation for future developments in autonomous subsea navigation."),Lt=l(),Ye=a("p"),c(oe.$$.fragment),Mt=l(),I=a("center"),T=a("iframe"),hi=l(),R=a("iframe"),this.h()},l(e){p=n(e,"H1",{id:!0});var i=r(p);_=n(i,"A",{href:!0});var _i=r(_);S=d(_i,"Navigating the Depths with RL"),_i.forEach(t),i.forEach(t),E=u(e),h=n(e,"P",{});var $i=r(h);k=d($i,"Hello folks! My name is Andrew Nash, and I’ve been heavily involved in developing a software system for an autonomous subsea vehicle, the SRAUV, as part of my engineering capstone project. In this blog post, I’ll detail the work I did on the Autopilot/AI perception, navigation, and control. Strap in for an exciting journey deep beneath the waves!"),$i.forEach(t),U=u(e),ne=n(e,"CENTER",{});var Ai=r(ne);A=n(Ai,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),r(A).forEach(t),Ai.forEach(t),Ge=u(e),P=n(e,"H2",{id:!0});var Ti=r(P);re=n(Ti,"A",{href:!0});var Ri=r(re);Ft=d(Ri,"Introduction"),Ri.forEach(t),Ti.forEach(t),Qe=u(e),se=n(e,"P",{});var ki=r(se);Ot=d(ki,"In my capstone project, my team and I have tackled a pervasive problem in offshore oil and gas operations - consistent and reliable subsea monitoring. To address this, we’ve designed a prototype Subsea Resident Autonomous Underwater Vehicle (SRAUV). Our innovative SRAUV operates indefinitely under the sea, utilizing a dock on the seafloor that facilitates wireless charging and communication. It embarks on scheduled or on-demand inspection missions, collects vital data, and then returns to the dock to transmit this information. Through this approach, we’re ushering in a new era of advanced offshore operations, with potential upgrades such as real-time anomaly detection that promises enhanced efficiency and safety."),ki.forEach(t),Ze=u(e),le=n(e,"P",{});var Si=r(le);v(V.$$.fragment,Si),Si.forEach(t),Xe=u(e),x=n(e,"H2",{id:!0});var Ei=r(x);ue=n(Ei,"A",{href:!0});var Ii=r(ue);Ht=d(Ii,"Vehicle"),Ii.forEach(t),Ei.forEach(t),Be=u(e),fe=n(e,"P",{});var Ui=r(fe);Wt=d(Ui,"My exceptional teammates ensured the SRAUV’s mechanical design adhered to principles of hydrodynamics, stability, and compactness, resulting in a sleek, efficient design. Fitting within the original constraints of a 2-foot length and width, the vehicle’s overall mass is approximately 15 kg, as illustrated below. The team cleverly configured the thrusters to offer a versatile maneuverability, giving the SRAUV 5 degrees of freedom, though effectively using only 4 - excluding pitch and roll - for the project’s final demonstration."),Ui.forEach(t),Je=u(e),me=n(e,"P",{});var Pi=r(me);v(L.$$.fragment,Pi),Pi.forEach(t),Ke=u(e),M=n(e,"H2",{id:!0});var Vi=r(M);de=n(Vi,"A",{href:!0});var xi=r(de);Ct=d(xi,"Software Overview"),xi.forEach(t),Vi.forEach(t),et=u(e),pe=n(e,"P",{});var Li=r(pe);jt=d(Li,"The SRAUV software is a culmination of collaborative effort, using Unity 3D game engine for fast prototyping, extensive machine learning support, and engaging visual feedback. The interface is an all-in-one GUI Simulator, allowing the user to control the vehicle, visualize thruster direction feedback, and simulate various environmental conditions. All software for the project is openly available on Github."),Li.forEach(t),tt=u(e),z=n(e,"H2",{id:!0});var Mi=r(z);he=n(Mi,"A",{href:!0});var zi=r(he);qt=d(zi,"Simulator"),zi.forEach(t),Mi.forEach(t),it=u(e),ce=n(e,"P",{});var Di=r(ce);Nt=d(Di,"We developed a Unity-based simulator that accurately mimics the mechanics of the SRAUV. This simulator applies individual thrust vectors at each thruster’s respective angle, fine-tuning its underwater movement, often referred to as “driftyness”, to reflect the real-world feedback from experienced pilots. The simulation updates in response to these thrust commands, providing new sensor data which includes the SRAUV’s position and the relative locations of assets in the environment. This helped us efficiently and accurately test our autonomous flying solutions."),Di.forEach(t),ot=u(e),ve=n(e,"P",{});var Fi=r(ve);v(D.$$.fragment,Fi),Fi.forEach(t),at=u(e),F=n(e,"H2",{id:!0});var Oi=r(F);ge=n(Oi,"A",{href:!0});var Hi=r(ge);Yt=d(Hi,"Model Training"),Hi.forEach(t),Oi.forEach(t),nt=u(e),we=n(e,"P",{});var Wi=r(we);Gt=d(Wi,"In addition to the main simulator, I created a lighter “ML Tank” for training the DRL algorithms. This environment include a significantly reduced polygon count, a necessary optimization since DRL algorithms are computationally expensive to simulate. Further speed enhancements were achieved by including multiple ML Tanks, allowing multiple agents to collect observations simultaneously, this can be seen below."),Wi.forEach(t),rt=u(e),O=n(e,"P",{});var ci=r(O);Qt=d(ci,"Screenshot of 49 ML Tanks training simultaneously within the Unity environment. "),v(H.$$.fragment,ci),ci.forEach(t),st=u(e),W=n(e,"H2",{id:!0});var Ci=r(W);ye=n(Ci,"A",{href:!0});var ji=r(ye);Zt=d(ji,"Autonomous Strategy"),ji.forEach(t),Ci.forEach(t),lt=u(e),be=n(e,"P",{});var qi=r(be);Xt=d(qi,"As the core designer of the Autopilot, I initially considered employing Q-learning for the SRAUV’s control system. This approach was favored due to its popularity and well-documented success. However, throughout the model selection process, I found Deep Reinforcement Learning (DRL) to be the most effective at driving in the simulator. I utilized Unity ML-Agents Toolkit for DRL training within the Unity simulator, leading to rapid experimentation with the SRAUV’s Flight Computer and Vision System."),qi.forEach(t),ut=u(e),_e=n(e,"P",{});var Ni=r(_e);v(C.$$.fragment,Ni),Ni.forEach(t),ft=u(e),$e=n(e,"P",{});var Yi=r($e);Bt=d(Yi,"With DRL, I designed an end-to-end solution for our control system. This allowed observations to go into the model and thrust control to emerge, providing optimal control of the SRAUV through a variety of unpredictable scenarios."),Yi.forEach(t),mt=u(e),j=n(e,"H2",{id:!0});var Gi=r(j);Ae=n(Gi,"A",{href:!0});var Qi=r(Ae);Jt=d(Qi,"Model Development"),Qi.forEach(t),Gi.forEach(t),dt=u(e),Te=n(e,"P",{});var Zi=r(Te);Kt=d(Zi,"For model training, I used a simplified subproblem - driving from a random X, Y, Z position to another random position as quickly as possible. I then tested various DRL solutions on these subproblems, progressively introducing complexity until the environment closely mirrored real life. This iterative approach allowed for significant improvement during model development."),Zi.forEach(t),pt=u(e),Re=n(e,"P",{});var Xi=r(Re);v(q.$$.fragment,Xi),Xi.forEach(t),ht=u(e),N=n(e,"H3",{id:!0});var Bi=r(N);ke=n(Bi,"A",{href:!0});var Ji=r(ke);ei=d(Ji,"Model Selection"),Ji.forEach(t),Bi.forEach(t),ct=u(e),Se=n(e,"P",{});var Ki=r(Se);ti=d(Ki,"For the SRAUV control system, I considered both discrete and continuous action spaces. While a continuous action space implies real numbers for the DRL model output, a discrete one implies a fixed set of categories. I experimentally found discrete to be more effective during training. I benchmarked the Soft Actor-Critic (SAC) model against a Proximal Policy Optimization (PPO). SAC took a considerable time to train and often failed to reach the goal consistently. On the other hand, PPO found the optimal driving strategy faster and more effectively, which led to its selection for further development."),Ki.forEach(t),vt=u(e),Y=n(e,"H3",{id:!0});var eo=r(Y);Ee=n(eo,"A",{href:!0});var to=r(Ee);ii=d(to,"Reward Structure"),to.forEach(t),eo.forEach(t),gt=u(e),Ie=n(e,"P",{});var io=r(Ie);oi=d(io,"The reward structure found to best solve the easy environment was relatively straightforward, consisting of three different rewards. The first reward was assigned as +1 when the agent arrived within a predefined tolerance of its goal waypoint. The second reward was the negative inverse of maximum time steps -0.002, this small negative reward assigned every timestep encouraged the model to drive to the goal as fast as possible. Finally, the third reward sets the overall reward to -1 if the agent collides with anything, this also resets the run for increased training speed."),io.forEach(t),wt=u(e),Ue=n(e,"P",{});var oo=r(Ue);v(G.$$.fragment,oo),oo.forEach(t),yt=u(e),Pe=n(e,"P",{});var ao=r(Pe);ai=d(ao,"In further iterations of the training environment, I found that the agent would sometimes overshoot its goal position and accidentally crash into a wall, to help mitigate this problem a stacking negative reward was assigned whenever the agent exceeded 0.3 m/s velocity in any axis of movement. This resulted in reduced crashing and taught the model to operate at a reduced speed when nearing its goal position."),ao.forEach(t),bt=u(e),Q=n(e,"H3",{id:!0});var no=r(Q);Ve=n(no,"A",{href:!0});var ro=r(Ve);ni=d(ro,"Curriculum Learning"),ro.forEach(t),no.forEach(t),_t=u(e),xe=n(e,"P",{});var so=r(xe);ri=d(so,"Since the PPO algorithm struggled to converge when tolerances of less than 40cm were used in training, I utilized curriculum learning to overcome this challenge. This technique allowed me to progressively decrease the tolerance required to reach the goal waypoint during training, significantly enhancing the model’s performance."),so.forEach(t),$t=u(e),Le=n(e,"P",{});var lo=r(Le);v(Z.$$.fragment,lo),lo.forEach(t),At=u(e),X=n(e,"H2",{id:!0});var uo=r(X);Me=n(uo,"A",{href:!0});var fo=r(Me);si=d(fo,"Localization"),fo.forEach(t),uo.forEach(t),Tt=u(e),ze=n(e,"P",{});var mo=r(ze);li=d(mo,"After our initial approach for localization, which relied on a combination of a pressure sensor, time-of-flight (TOF) sensors, and an inertial-measurement-unit (IMU), failed to meet expectations, my team and I were forced to quickly pivot. Due to issues ranging from faulty TOF sensors, an unsteady IMU, and a malfunctioning depth sensor, we had to reimagine our solution, leading us to the exploration of a computer vision-based approach. Utilizing AprilTags, similar to simple QR codes, we established a localization system that successfully tracked the position and rotation of the SRAUV relative to the tank. We installed a grid of these AprilTags on the tank floor, ensuring the bottom camera of the SRAUV was able to constantly locate a tag, significantly improving our vehicle’s spatial awareness. Impressively, this solution not only worked exceedingly well during testing, achieving an accuracy of about +-5 cm in position and +-5 degrees in heading, but it was also considerably more cost-effective than our initial sensor plan."),mo.forEach(t),Rt=u(e),De=n(e,"P",{});var po=r(De);v(B.$$.fragment,po),po.forEach(t),kt=u(e),Fe=n(e,"P",{});var ho=r(Fe);ui=d(ho,"The AprilTags detection in the ML Tank was another area I optimized. Running the vision recognition on all 49 tanks in a single environment would have been computationally prohibitive. To address this, I added a camera with matching intrinsic parameters and position to the SRAUV’s bottom pi cam in the simulator. This allowed me to check if the AprilTag was in the camera’s frame during each timestep, providing the agent with the most recently known position & velocity estimate, simulating the noisy conditions it would face in the real world."),ho.forEach(t),St=u(e),Oe=n(e,"P",{});var co=r(Oe);v(J.$$.fragment,co),co.forEach(t),Et=u(e),K=n(e,"H2",{id:!0});var vo=r(K);He=n(vo,"A",{href:!0});var go=r(He);fi=d(go,"Real World Testing"),go.forEach(t),vo.forEach(t),It=u(e),We=n(e,"P",{});var wo=r(We);mi=d(wo,"Below you can see some videos of real world testing the SRAUV in a tank at Memorial University."),wo.forEach(t),Ut=u(e),Ce=n(e,"P",{});var yo=r(Ce);v(ee.$$.fragment,yo),yo.forEach(t),Pt=u(e),je=n(e,"P",{});var bo=r(je);v(te.$$.fragment,bo),bo.forEach(t),Vt=u(e),ie=n(e,"H2",{id:!0});var _o=r(ie);qe=n(_o,"A",{href:!0});var $o=r(qe);di=d($o,"Conclusion"),$o.forEach(t),_o.forEach(t),xt=u(e),Ne=n(e,"P",{});var Ao=r(Ne);pi=d(Ao,"Wrapping up, I am incredibly proud of the progress and achievements made by my team and myself throughout our capstone project. We successfully designed, implemented, and tested a neural network-based autopilot for our Subsea Resident AUV (SRAUV) prototype. This enabled the SRAUV to perform autonomous missions, including navigating a prescribed route and redocking, a significant milestone for us. Even though we faced setbacks, such as hardware failures limiting the testing of an improved autopilot model, these challenges only fueled our drive to innovate. Our commitment paid off, winning us first place in all capstone award categories. We believe our work has set a strong foundation for future developments in autonomous subsea navigation."),Ao.forEach(t),Lt=u(e),Ye=n(e,"P",{});var To=r(Ye);v(oe.$$.fragment,To),To.forEach(t),Mt=u(e),I=n(e,"CENTER",{});var Dt=r(I);T=n(Dt,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),r(T).forEach(t),hi=u(Dt),R=n(Dt,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),r(R).forEach(t),Dt.forEach(t),this.h()},h(){f(_,"href","#navigating-the-depths-with-rl"),f(p,"id","navigating-the-depths-with-rl"),f(A,"width","560"),f(A,"height","315"),vi(A.src,wi="https://www.youtube.com/embed/wzNI8wY0UT0")||f(A,"src",wi),f(A,"title","YouTube video player"),f(A,"frameborder","0"),f(A,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"),A.allowFullscreen=!0,f(re,"href","#introduction"),f(P,"id","introduction"),f(ue,"href","#vehicle"),f(x,"id","vehicle"),f(de,"href","#software-overview"),f(M,"id","software-overview"),f(he,"href","#simulator"),f(z,"id","simulator"),f(ge,"href","#model-training"),f(F,"id","model-training"),f(ye,"href","#autonomous-strategy"),f(W,"id","autonomous-strategy"),f(Ae,"href","#model-development"),f(j,"id","model-development"),f(ke,"href","#model-selection"),f(N,"id","model-selection"),f(Ee,"href","#reward-structure"),f(Y,"id","reward-structure"),f(Ve,"href","#curriculum-learning"),f(Q,"id","curriculum-learning"),f(Me,"href","#localization"),f(X,"id","localization"),f(He,"href","#real-world-testing"),f(K,"id","real-world-testing"),f(qe,"href","#conclusion"),f(ie,"id","conclusion"),f(T,"width","560"),f(T,"height","315"),vi(T.src,yi="https://www.youtube.com/embed/v3Z9sVuU5DQ")||f(T,"src",yi),f(T,"title","YouTube video player"),f(T,"frameborder","0"),f(T,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"),T.allowFullscreen=!0,f(R,"width","560"),f(R,"height","315"),vi(R.src,bi="https://www.youtube.com/embed/UXqgjFvW4HE")||f(R,"src",bi),f(R,"title","YouTube video player"),f(R,"frameborder","0"),f(R,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"),R.allowFullscreen=!0},m(e,i){o(e,p,i),s(p,_),s(_,S),o(e,E,i),o(e,h,i),s(h,k),o(e,U,i),o(e,ne,i),s(ne,A),o(e,Ge,i),o(e,P,i),s(P,re),s(re,Ft),o(e,Qe,i),o(e,se,i),s(se,Ot),o(e,Ze,i),o(e,le,i),g(V,le,null),o(e,Xe,i),o(e,x,i),s(x,ue),s(ue,Ht),o(e,Be,i),o(e,fe,i),s(fe,Wt),o(e,Je,i),o(e,me,i),g(L,me,null),o(e,Ke,i),o(e,M,i),s(M,de),s(de,Ct),o(e,et,i),o(e,pe,i),s(pe,jt),o(e,tt,i),o(e,z,i),s(z,he),s(he,qt),o(e,it,i),o(e,ce,i),s(ce,Nt),o(e,ot,i),o(e,ve,i),g(D,ve,null),o(e,at,i),o(e,F,i),s(F,ge),s(ge,Yt),o(e,nt,i),o(e,we,i),s(we,Gt),o(e,rt,i),o(e,O,i),s(O,Qt),g(H,O,null),o(e,st,i),o(e,W,i),s(W,ye),s(ye,Zt),o(e,lt,i),o(e,be,i),s(be,Xt),o(e,ut,i),o(e,_e,i),g(C,_e,null),o(e,ft,i),o(e,$e,i),s($e,Bt),o(e,mt,i),o(e,j,i),s(j,Ae),s(Ae,Jt),o(e,dt,i),o(e,Te,i),s(Te,Kt),o(e,pt,i),o(e,Re,i),g(q,Re,null),o(e,ht,i),o(e,N,i),s(N,ke),s(ke,ei),o(e,ct,i),o(e,Se,i),s(Se,ti),o(e,vt,i),o(e,Y,i),s(Y,Ee),s(Ee,ii),o(e,gt,i),o(e,Ie,i),s(Ie,oi),o(e,wt,i),o(e,Ue,i),g(G,Ue,null),o(e,yt,i),o(e,Pe,i),s(Pe,ai),o(e,bt,i),o(e,Q,i),s(Q,Ve),s(Ve,ni),o(e,_t,i),o(e,xe,i),s(xe,ri),o(e,$t,i),o(e,Le,i),g(Z,Le,null),o(e,At,i),o(e,X,i),s(X,Me),s(Me,si),o(e,Tt,i),o(e,ze,i),s(ze,li),o(e,Rt,i),o(e,De,i),g(B,De,null),o(e,kt,i),o(e,Fe,i),s(Fe,ui),o(e,St,i),o(e,Oe,i),g(J,Oe,null),o(e,Et,i),o(e,K,i),s(K,He),s(He,fi),o(e,It,i),o(e,We,i),s(We,mi),o(e,Ut,i),o(e,Ce,i),g(ee,Ce,null),o(e,Pt,i),o(e,je,i),g(te,je,null),o(e,Vt,i),o(e,ie,i),s(ie,qe),s(qe,di),o(e,xt,i),o(e,Ne,i),s(Ne,pi),o(e,Lt,i),o(e,Ye,i),g(oe,Ye,null),o(e,Mt,i),o(e,I,i),s(I,T),s(I,hi),s(I,R),zt=!0},p:Vo,i(e){zt||(w(V.$$.fragment,e),w(L.$$.fragment,e),w(D.$$.fragment,e),w(H.$$.fragment,e),w(C.$$.fragment,e),w(q.$$.fragment,e),w(G.$$.fragment,e),w(Z.$$.fragment,e),w(B.$$.fragment,e),w(J.$$.fragment,e),w(ee.$$.fragment,e),w(te.$$.fragment,e),w(oe.$$.fragment,e),zt=!0)},o(e){y(V.$$.fragment,e),y(L.$$.fragment,e),y(D.$$.fragment,e),y(H.$$.fragment,e),y(C.$$.fragment,e),y(q.$$.fragment,e),y(G.$$.fragment,e),y(Z.$$.fragment,e),y(B.$$.fragment,e),y(J.$$.fragment,e),y(ee.$$.fragment,e),y(te.$$.fragment,e),y(oe.$$.fragment,e),zt=!1},d(e){e&&t(p),e&&t(E),e&&t(h),e&&t(U),e&&t(ne),e&&t(Ge),e&&t(P),e&&t(Qe),e&&t(se),e&&t(Ze),e&&t(le),b(V),e&&t(Xe),e&&t(x),e&&t(Be),e&&t(fe),e&&t(Je),e&&t(me),b(L),e&&t(Ke),e&&t(M),e&&t(et),e&&t(pe),e&&t(tt),e&&t(z),e&&t(it),e&&t(ce),e&&t(ot),e&&t(ve),b(D),e&&t(at),e&&t(F),e&&t(nt),e&&t(we),e&&t(rt),e&&t(O),b(H),e&&t(st),e&&t(W),e&&t(lt),e&&t(be),e&&t(ut),e&&t(_e),b(C),e&&t(ft),e&&t($e),e&&t(mt),e&&t(j),e&&t(dt),e&&t(Te),e&&t(pt),e&&t(Re),b(q),e&&t(ht),e&&t(N),e&&t(ct),e&&t(Se),e&&t(vt),e&&t(Y),e&&t(gt),e&&t(Ie),e&&t(wt),e&&t(Ue),b(G),e&&t(yt),e&&t(Pe),e&&t(bt),e&&t(Q),e&&t(_t),e&&t(xe),e&&t($t),e&&t(Le),b(Z),e&&t(At),e&&t(X),e&&t(Tt),e&&t(ze),e&&t(Rt),e&&t(De),b(B),e&&t(kt),e&&t(Fe),e&&t(St),e&&t(Oe),b(J),e&&t(Et),e&&t(K),e&&t(It),e&&t(We),e&&t(Ut),e&&t(Ce),b(ee),e&&t(Pt),e&&t(je),b(te),e&&t(Vt),e&&t(ie),e&&t(xt),e&&t(Ne),e&&t(Lt),e&&t(Ye),b(oe),e&&t(Mt),e&&t(I)}}}function Mo(ae){let p,_;const S=[ae[0],So];let E={$$slots:{default:[Lo]},$$scope:{ctx:ae}};for(let h=0;h<S.length;h+=1)E=gi(E,S[h]);return p=new xo({props:E}),{c(){c(p.$$.fragment)},l(h){v(p.$$.fragment,h)},m(h,k){g(p,h,k),_=!0},p(h,[k]){const U=k&1?Po(S,[k&1&&Ro(h[0]),k&0&&Ro(So)]):{};k&2&&(U.$$scope={dirty:k,ctx:h}),p.$set(U)},i(h){_||(w(p.$$.fragment,h),_=!0)},o(h){y(p.$$.fragment,h),_=!1},d(h){b(p,h)}}}const So={title:"Subsea Resident Autonomous Underwater Vehicle",image:"/srauv/team.jpg",alt:"SRAUV",created:"2020-05-02T00:00:00.000Z",tags:["Computer Vision","RL"],updated:"2023-06-14T20:55:04.816Z",images:[],slug:"/srauv/+page.svelte.md",path:"/srauv",toc:[{depth:1,title:"Navigating the Depths with RL",slug:"navigating-the-depths-with-rl"},{depth:2,title:"Introduction",slug:"introduction"},{depth:2,title:"Vehicle",slug:"vehicle"},{depth:2,title:"Software Overview",slug:"software-overview"},{depth:2,title:"Simulator",slug:"simulator"},{depth:2,title:"Model Training",slug:"model-training"},{depth:2,title:"Autonomous Strategy",slug:"autonomous-strategy"},{depth:2,title:"Model Development",slug:"model-development"},{depth:3,title:"Model Selection",slug:"model-selection"},{depth:3,title:"Reward Structure",slug:"reward-structure"},{depth:3,title:"Curriculum Learning",slug:"curriculum-learning"},{depth:2,title:"Localization",slug:"localization"},{depth:2,title:"Real World Testing",slug:"real-world-testing"},{depth:2,title:"Conclusion",slug:"conclusion"}]};function zo(ae,p,_){return ae.$$set=S=>{_(0,p=gi(gi({},p),ko(S)))},p=ko(p),[p]}class Ho extends Eo{constructor(p){super(),Io(this,p,zo,Mo,Uo,{})}}export{Ho as component};
